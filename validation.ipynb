{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.embeddings.utils import resolve_embed_model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import Settings\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  6 11:25:54 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 22%   22C    P8    14W / 215W |     15MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1411      G   /usr/bin/gnome-shell                3MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = SimpleDirectoryReader(\"data/docs\").load_data()\n",
    "\n",
    "# embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, show_progress=True, service_context=service_context\n",
    "# )\n",
    "\n",
    "# index.storage_context.persist(persist_dir=\"data/persist_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080, compute capability 7.5, VMM: yes\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3917.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# llm = Ollama(model=\"mistral\", request_timeout=180.0) # , base_url=\"http://62.80.172.138:11434\"\n",
    "llm = Llama('models/mistral-7b-instruct-v0.2.Q4_0.gguf', n_ctx=2048, verbose=True) # mistral-7b-instruct-v0.2.Q4_0.gguf mistral-7b-instruct-v0.2.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1672.22 ms\n",
      "llama_print_timings:      sample time =      23.68 ms /    60 runs   (    0.39 ms per token,  2534.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1671.87 ms /    14 tokens (  119.42 ms per token,     8.37 tokens per second)\n",
      "llama_print_timings:        eval time =    9611.85 ms /    59 runs   (  162.91 ms per token,     6.14 tokens per second)\n",
      "llama_print_timings:       total time =   11417.82 ms /    73 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-31c12d74-cd69-4d46-9d32-5256a85685eb',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1709717165,\n",
       " 'model': 'models/mistral-7b-instruct-v0.2.Q4_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \" The color of the sky appears blue due to a process called Rayleigh scattering. As sunlight reaches Earth's atmosphere, it interacts with different gases and particles in the air. Blue light has a shorter wavelength and gets scattered more easily than other colors because it travels in smaller, shorter\"},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 14, 'completion_tokens': 60, 'total_tokens': 74}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(messages=[{'role': 'user', 'content': 'Why is sky blue?'}], max_tokens=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"data/persist_dir\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, show_progress=True) # , service_context=service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_scheme_prompt = {\n",
    "    \"method\": {\n",
    "        \"type\": \"string\"\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"type\": \"object\"\n",
    "    }\n",
    "}\n",
    "\n",
    "example_1_json = {\n",
    "  \"method\":\"Cover.Open\",\n",
    "  \"params\":\n",
    "  {\n",
    "    \"id\":2\n",
    "  }\n",
    "}\n",
    "\n",
    "example_2_json = {\n",
    "  \"method\":\"Cover.GetStatus\",\n",
    "  \"params\":\n",
    "  {\n",
    "    \"id\":0\n",
    "  }\n",
    "}\n",
    "\n",
    "with open('data/prompts/validation/instruction.md') as f:\n",
    "  instruction = f.read()\n",
    "\n",
    "variables = {\n",
    "    \"instruction\": instruction,\n",
    "    \"json_scheme\": \"The output JSON should follow the next scheme: \" + json.dumps(json_scheme_prompt),\n",
    "    \"devices\": \"\"\"Cover id=1\"\"\",\n",
    "    \"example_1\": \"\"\"Devices: Cover id=2\n",
    "Methods:\n",
    "API method 1: Cover.Open\n",
    "Description: Preconditions: Cover will not accept the command if: An overvoltage error is set at the time of the request. An undervoltage error is set at the time of the request. An overtemp error is set at the time of the request. An engaged safety_switch prohibits movement in the requested direction. Cover calibration is running at the time of the request. Properties: [{'name': 'id', 'type': 'number', 'description': 'The numeric ID of the Cover component instance'}, {'name': 'duration', 'type': 'number', 'description': 'If duration is not provided, Cover will fully open, unless it times out because of maxtime_open first. If duration (seconds) is provided, Cover will move in the open direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional'}] Response: null on success; error if the request can not be executed or failed \n",
    "Command: Open the cover.\n",
    "JSON: \"\"\" + json.dumps(example_1_json),\n",
    "    \"example_2\": \"\"\"Devices: Cover id=0\n",
    "Methods: \n",
    "API method 1: Cover.GetStatus\n",
    "Description: Properties: [{'name': 'id', 'type': 'number', 'description': 'The numeric ID of the Cover component instance'}] Find more about the status properties in status section \n",
    "Command: Get cover status.\n",
    "JSON: \"\"\" + json.dumps(example_2_json),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI Assistant that controls the devices in a house. For a given user command create a corresponding JSON object. Don't add properties with null value in output JSON object. Output must be strictly in JSON format.\n",
      "The output JSON should follow the next scheme: {\"method\": {\"type\": \"string\"}, \"params\": {\"type\": \"object\"}}\n",
      "\n",
      "Devices: Cover id=2\n",
      "Methods:\n",
      "API method 1: Cover.Open\n",
      "Description: Preconditions: Cover will not accept the command if: An overvoltage error is set at the time of the request. An undervoltage error is set at the time of the request. An overtemp error is set at the time of the request. An engaged safety_switch prohibits movement in the requested direction. Cover calibration is running at the time of the request. Properties: [{'name': 'id', 'type': 'number', 'description': 'The numeric ID of the Cover component instance'}, {'name': 'duration', 'type': 'number', 'description': 'If duration is not provided, Cover will fully open, unless it times out because of maxtime_open first. If duration (seconds) is provided, Cover will move in the open direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional'}] Response: null on success; error if the request can not be executed or failed \n",
      "Command: Open the cover.\n",
      "JSON: {\"method\": \"Cover.Open\", \"params\": {\"id\": 2}}\n",
      "\n",
      "Devices: Cover id=0\n",
      "Methods: \n",
      "API method 1: Cover.GetStatus\n",
      "Description: Properties: [{'name': 'id', 'type': 'number', 'description': 'The numeric ID of the Cover component instance'}] Find more about the status properties in status section \n",
      "Command: Get cover status.\n",
      "JSON: {\"method\": \"Cover.GetStatus\", \"params\": {\"id\": 0}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_prompt_template = \"\"\"\n",
    "{instruction}\n",
    "{json_scheme}\n",
    "\n",
    "{example_1}\n",
    "\n",
    "{example_2}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_template = \"\"\"Devices: {env}\n",
    "Methods:\n",
    "{methods_description}\n",
    "Command: {user_cmd}\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "base_prompt = base_prompt_template.format(**variables)\n",
    "\n",
    "print(base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "# logger.addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger Q4_K_M model generates more complex output with unnecessary parameters. Possible fix: parameters tuning. \\\n",
    "Whereas Q4_0 model tries to escape `_` character with `\\` inside object property. Possible fix: grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import LlamaGrammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_scheme_general = {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"method\": {\n",
    "#             \"type\": \"string\"\n",
    "#         },\n",
    "#         \"params\": {\n",
    "#             \"type\": \"object\",\n",
    "#             # doesn't work without properties defined.\n",
    "#             # however, json schema is valid even without 'properties' field\n",
    "#             # https://stackoverflow.com/questions/42977208/json-schema-without-properties-keyword\n",
    "#             # \"properties\": {\n",
    "#             #     \"config\": {\n",
    "#             #         \"type\": \"string\"\n",
    "#             #     }\n",
    "#             # }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# s = str(json_scheme_general).replace(\"'\", '\"')\n",
    "# print(s)\n",
    "# gr = LlamaGrammar.from_json_schema(s)\n",
    "# print(gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root   ::= object\n",
      "value  ::= object | array | string | number | (\"true\" | \"false\" | \"null\") ws\n",
      "\n",
      "object ::=\n",
      "  \"{\" ws (\n",
      "            string \":\" ws value\n",
      "    (\",\" ws string \":\" ws value)*\n",
      "  )? \"}\" ws\n",
      "\n",
      "array  ::=\n",
      "  \"[\" ws (\n",
      "            value\n",
      "    (\",\" ws value)*\n",
      "  )? \"]\" ws\n",
      "\n",
      "string ::=\n",
      "  \"\\\"\" (\n",
      "    [^\"\\\\\\x7F\\x00-\\x1F] |\n",
      "    \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n",
      "  )* \"\\\"\" ws\n",
      "\n",
      "number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n",
      "\n",
      "# Optional space: by convention, applied in this grammar after literal chars when allowed\n",
      "ws ::= ([ \\t\\n])?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/grammars/json.gbnf') as f:\n",
    "    grammar_str = f.read()\n",
    "print(grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_cpp.llama_grammar.LlamaGrammar object at 0x7fc1d433a590>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] \n",
      "ws_31 ::= ws_30 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grammar = LlamaGrammar.from_string(grammar_str)\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1672.22 ms\n",
      "llama_print_timings:      sample time =     155.13 ms /    22 runs   (    7.05 ms per token,   141.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3305.44 ms /   758 tokens (    4.36 ms per token,   229.32 tokens per second)\n",
      "llama_print_timings:        eval time =    3592.75 ms /    21 runs   (  171.08 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =    7117.97 ms /   779 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Temperature.GetConfig\", \"params\": {\"id\": 444}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1672.22 ms\n",
      "llama_print_timings:      sample time =     155.11 ms /    22 runs   (    7.05 ms per token,   141.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1565.01 ms /   310 tokens (    5.05 ms per token,   198.08 tokens per second)\n",
      "llama_print_timings:        eval time =    3576.11 ms /    21 runs   (  170.29 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =    5359.23 ms /   331 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Temperature.GetConfig\", \"params\": {\"id\": 444}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1672.22 ms\n",
      "llama_print_timings:      sample time =     195.13 ms /    28 runs   (    6.97 ms per token,   143.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1570.17 ms /   312 tokens (    5.03 ms per token,   198.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4602.49 ms /    27 runs   (  170.46 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =    6447.82 ms /   339 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Temperature.GetConfig\", \"params\": {\"id\": 444, \"name\": null}}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1672.22 ms\n",
      "llama_print_timings:      sample time =     155.00 ms /    22 runs   (    7.05 ms per token,   141.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1564.55 ms /   310 tokens (    5.05 ms per token,   198.14 tokens per second)\n",
      "llama_print_timings:        eval time =    3575.61 ms /    21 runs   (  170.27 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =    5357.95 ms /   331 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Temperature.GetConfig\", \"params\": {\"id\": 444}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1672.22 ms\n",
      "llama_print_timings:      sample time =     216.96 ms /    31 runs   (    7.00 ms per token,   142.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1574.84 ms /   313 tokens (    5.03 ms per token,   198.75 tokens per second)\n",
      "llama_print_timings:        eval time =    5106.93 ms /    30 runs   (  170.23 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =    6986.94 ms /   343 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Temperature.GetConfig\", \"params\": {\"id\": 444, \"name\": \"bedroom\"}}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1672.22 ms\n",
      "llama_print_timings:      sample time =     217.65 ms /    31 runs   (    7.02 ms per token,   142.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1540.43 ms /   304 tokens (    5.07 ms per token,   197.35 tokens per second)\n",
      "llama_print_timings:        eval time =    5123.99 ms /    30 runs   (  170.80 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =    6971.68 ms /   334 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Temperature.GetConfig\", \"params\": {\"id\": 444, \"name\": \"kitchen\"}}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1672.22 ms\n",
      "llama_print_timings:      sample time =     155.68 ms /    22 runs   (    7.08 ms per token,   141.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1624.65 ms /   328 tokens (    4.95 ms per token,   201.89 tokens per second)\n",
      "llama_print_timings:        eval time =    3586.26 ms /    21 runs   (  170.77 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =    5430.24 ms /   349 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Temperature.GetConfig\", \"params\": {\"id\": 444}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1672.22 ms\n",
      "llama_print_timings:      sample time =     194.99 ms /    28 runs   (    6.96 ms per token,   143.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1555.11 ms /   309 tokens (    5.03 ms per token,   198.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4583.96 ms /    27 runs   (  169.78 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =    6414.99 ms /   336 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Temperature.GetConfig\", \"params\": {\"id\": 444, \"name\": null}}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1672.22 ms\n",
      "llama_print_timings:      sample time =     153.73 ms /    22 runs   (    6.99 ms per token,   143.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3928.69 ms /   754 tokens (    5.21 ms per token,   191.92 tokens per second)\n",
      "llama_print_timings:        eval time =    3662.15 ms /    21 runs   (  174.39 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =    7809.75 ms /   775 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Input.GetStatus\", \"params\": {\"id\": 444}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit_rows = -1 # 20\n",
    "df = pd.read_csv('data/datasets/dataset_v0.csv')\n",
    "devices = list(df['device'].unique())\n",
    "output_df = pd.DataFrame(columns=['device', 'user_cmd', 'mtd', 'json_cmd'])\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # if i < 8 or i > 16:\n",
    "    #     continue\n",
    "    user_cmd = row['user_cmd']\n",
    "\n",
    "    device = row['device']\n",
    "    sample_devices = devices.copy()\n",
    "    sample_devices.remove(device)\n",
    "    sample_devices = random.sample(sample_devices, k=2)\n",
    "    env = f'{sample_devices[0]} id=1, {sample_devices[1]} id=2, {device} id=444'\n",
    "\n",
    "    retrieved_nodes = retriever.retrieve(user_cmd)\n",
    "    methods_description = f'API method 1:\\n{retrieved_nodes[0].text}'\n",
    "    method = retrieved_nodes[0].metadata['file_name'].replace('.md', '')\n",
    "\n",
    "    user_prompt = user_prompt_template.format(**{'env': env, \n",
    "                                                 'methods_description': methods_description, \n",
    "                                                 'user_cmd': user_cmd})\n",
    "    query = base_prompt + '\\n\\n' + user_prompt\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': query}\n",
    "        ],\n",
    "        grammar=grammar\n",
    "        # response_format={\"type\": \"json_object\",\n",
    "        #                 \"schema\": json_scheme_general} # \n",
    "    )\n",
    "    response_text = response['choices'][0]['message']['content']\n",
    "    response_text = response_text.replace('\\_', '_')\n",
    "    print(response_text)\n",
    "    json_cmd = json.dumps(json.loads(response_text))\n",
    "\n",
    "    output_df.loc[len(output_df)] = pd.Series({'device': row['device'], 'user_cmd': user_cmd, 'mtd': method, 'json_cmd': json_cmd})\n",
    "\n",
    "    if limit_rows > 0 and i == limit_rows - 1:\n",
    "        break\n",
    "output_df.to_csv('output/output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device</th>\n",
       "      <th>user_cmd</th>\n",
       "      <th>mtd</th>\n",
       "      <th>json_cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the dining room switch to flip mode.</td>\n",
       "      <td>Switch.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the living room switch to cycle mode.</td>\n",
       "      <td>Switch.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Enable Automatic OFF for the kitchen switch wi...</td>\n",
       "      <td>Switch.GetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the bedroom switch name to 'Bed Lamp'.</td>\n",
       "      <td>Light.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the hallway switch power limit to 50 Watts.</td>\n",
       "      <td>Switch.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the patio switch voltage limit to 220 Volts.</td>\n",
       "      <td>Switch.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Enable autorecover voltage errors for the gara...</td>\n",
       "      <td>Switch.GetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the office switch current limit to 8 Amperes.</td>\n",
       "      <td>Switch.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>Please provide me with the current temperature...</td>\n",
       "      <td>Temperature.SetConfig</td>\n",
       "      <td>{\"method\": \"Temperature.GetConfig\", \"params\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>Get the temperature report threshold value for...</td>\n",
       "      <td>Temperature.GetConfig</td>\n",
       "      <td>{\"method\": \"Temperature.GetConfig\", \"params\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>I would like to know the offset value set for ...</td>\n",
       "      <td>Temperature.GetConfig</td>\n",
       "      <td>{\"method\": \"Temperature.GetConfig\", \"params\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>What is the current temperature configuration ...</td>\n",
       "      <td>Temperature.GetConfig</td>\n",
       "      <td>{\"method\": \"Temperature.GetConfig\", \"params\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>Could you please give me the temperature repor...</td>\n",
       "      <td>Temperature.GetConfig</td>\n",
       "      <td>{\"method\": \"Temperature.GetConfig\", \"params\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>Check the offset value set for the kitchen tem...</td>\n",
       "      <td>Temperature.GetConfig</td>\n",
       "      <td>{\"method\": \"Temperature.GetConfig\", \"params\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>Provide me with the current temperature config...</td>\n",
       "      <td>Temperature.SetConfig</td>\n",
       "      <td>{\"method\": \"Temperature.GetConfig\", \"params\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>What is the offset value for the external temp...</td>\n",
       "      <td>Temperature.GetConfig</td>\n",
       "      <td>{\"method\": \"Temperature.GetConfig\", \"params\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Input</td>\n",
       "      <td>Check the status of the input for the living r...</td>\n",
       "      <td>Input.GetStatus</td>\n",
       "      <td>{\"method\": \"Input.GetStatus\", \"params\": {\"id\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Input</td>\n",
       "      <td>Get details of the status for the input in the...</td>\n",
       "      <td>Input.GetStatus</td>\n",
       "      <td>{\"method\": \"Input.GetStatus\", \"params\": {\"id\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Input</td>\n",
       "      <td>I need to know the current status of the input...</td>\n",
       "      <td>Input.GetStatus</td>\n",
       "      <td>{\"method\": \"Input.GetStatus\", \"params\": {\"id\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Input</td>\n",
       "      <td>Find out the status of the input for the bedro...</td>\n",
       "      <td>Input.GetStatus</td>\n",
       "      <td>{\"method\": \"Input.GetStatus\", \"params\": {\"id\":...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         device                                           user_cmd  \\\n",
       "0        Switch           Set the dining room switch to flip mode.   \n",
       "1        Switch          Set the living room switch to cycle mode.   \n",
       "2        Switch  Enable Automatic OFF for the kitchen switch wi...   \n",
       "3        Switch         Set the bedroom switch name to 'Bed Lamp'.   \n",
       "4        Switch    Set the hallway switch power limit to 50 Watts.   \n",
       "5        Switch   Set the patio switch voltage limit to 220 Volts.   \n",
       "6        Switch  Enable autorecover voltage errors for the gara...   \n",
       "7        Switch  Set the office switch current limit to 8 Amperes.   \n",
       "8   Temperature  Please provide me with the current temperature...   \n",
       "9   Temperature  Get the temperature report threshold value for...   \n",
       "10  Temperature  I would like to know the offset value set for ...   \n",
       "11  Temperature  What is the current temperature configuration ...   \n",
       "12  Temperature  Could you please give me the temperature repor...   \n",
       "13  Temperature  Check the offset value set for the kitchen tem...   \n",
       "14  Temperature  Provide me with the current temperature config...   \n",
       "15  Temperature  What is the offset value for the external temp...   \n",
       "16        Input  Check the status of the input for the living r...   \n",
       "17        Input  Get details of the status for the input in the...   \n",
       "18        Input  I need to know the current status of the input...   \n",
       "19        Input  Find out the status of the input for the bedro...   \n",
       "\n",
       "                      mtd                                           json_cmd  \n",
       "0        Switch.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "1        Switch.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "2        Switch.GetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "3         Light.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "4        Switch.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "5        Switch.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "6        Switch.GetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "7        Switch.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "8   Temperature.SetConfig  {\"method\": \"Temperature.GetConfig\", \"params\": ...  \n",
       "9   Temperature.GetConfig  {\"method\": \"Temperature.GetConfig\", \"params\": ...  \n",
       "10  Temperature.GetConfig  {\"method\": \"Temperature.GetConfig\", \"params\": ...  \n",
       "11  Temperature.GetConfig  {\"method\": \"Temperature.GetConfig\", \"params\": ...  \n",
       "12  Temperature.GetConfig  {\"method\": \"Temperature.GetConfig\", \"params\": ...  \n",
       "13  Temperature.GetConfig  {\"method\": \"Temperature.GetConfig\", \"params\": ...  \n",
       "14  Temperature.SetConfig  {\"method\": \"Temperature.GetConfig\", \"params\": ...  \n",
       "15  Temperature.GetConfig  {\"method\": \"Temperature.GetConfig\", \"params\": ...  \n",
       "16        Input.GetStatus  {\"method\": \"Input.GetStatus\", \"params\": {\"id\":...  \n",
       "17        Input.GetStatus  {\"method\": \"Input.GetStatus\", \"params\": {\"id\":...  \n",
       "18        Input.GetStatus  {\"method\": \"Input.GetStatus\", \"params\": {\"id\":...  \n",
       "19        Input.GetStatus  {\"method\": \"Input.GetStatus\", \"params\": {\"id\":...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "output_df = pd.read_csv('output/output.csv')\n",
    "incorrect_output_df = pd.DataFrame(columns=['device', 'user_cmd', 'gt_mtd', 'output_mtd', 'gt_json_cmd', 'output_json_cmd'])\n",
    "\n",
    "correct_methods = 0\n",
    "correct_json_cmds = 0\n",
    "for i in range(len(output_df)):\n",
    "    gt = df.iloc[i]\n",
    "    output = output_df.iloc[i]\n",
    "\n",
    "    correct_ouput = True\n",
    "    if gt['mtd'] == output['mtd']:\n",
    "        correct_methods += 1\n",
    "    else:\n",
    "        correct_ouput = False\n",
    "    try:\n",
    "        if ast.literal_eval(gt['json_cmd'].replace(\"'\", '\"')) == json.loads(output['json_cmd']):\n",
    "            correct_json_cmds += 1\n",
    "        else:\n",
    "            correct_ouput = False\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(gt['json_cmd'].replace(\"'\", '\"'))\n",
    "        print(i, output['json_cmd'])\n",
    "    \n",
    "    if not correct_ouput:\n",
    "        incorrect_output_df.loc[len(incorrect_output_df)] = pd.Series({'device': gt['device'], 'user_cmd': gt['user_cmd'], 'gt_mtd': gt['mtd'],\n",
    "                                                              'output_mtd': output['mtd'], 'gt_json_cmd': gt['json_cmd'], 'output_json_cmd': output['json_cmd']})\n",
    "acc_methods = round(correct_methods / len(output_df), 2)\n",
    "acc_json_cmds = round(correct_json_cmds / len(output_df), 2)\n",
    "\n",
    "incorrect_output_df.to_csv('output/incorrect_output.csv', index=False)\n",
    "\n",
    "with open('output/results', 'a') as f:\n",
    "    f.write(f'Acc of methods: {acc_methods}\\n'\n",
    "            f'Acc of json cmds: {acc_json_cmds}\\n\\n'\n",
    "            f'-----------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080, compute capability 7.5, VMM: yes\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    72.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "llm = Llama('models/mistral-7b-instruct-v0.2.Q4_K_M.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     572.01 ms\n",
      "llama_print_timings:      sample time =       5.60 ms /    16 runs   (    0.35 ms per token,  2857.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     571.62 ms /     6 tokens (   95.27 ms per token,    10.50 tokens per second)\n",
      "llama_print_timings:        eval time =    2425.41 ms /    15 runs   (  161.69 ms per token,     6.18 tokens per second)\n",
      "llama_print_timings:       total time =    3036.28 ms /    21 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-3836340b-ae7d-4091-b36b-3cc92bd2af80', 'object': 'text_completion', 'created': 1709562983, 'model': 'models/mistral-7b-instruct-v0.2.Q4_K_M.gguf', 'choices': [{'text': ' Is it because of pollution, or the presence of certain gases in the atmosphere', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 6, 'completion_tokens': 16, 'total_tokens': 22}}\n"
     ]
    }
   ],
   "source": [
    "output = llm('Why is sky blue?')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "space ::= space_1 \n",
      "space_1 ::= [ ] | \n",
      "number ::= number_3 number_9 number_13 space \n",
      "number_3 ::= number_4 number_5 \n",
      "number_4 ::= [-] | \n",
      "number_5 ::= [0-9] | [1-9] number_6 \n",
      "number_6 ::= [0-9] number_6 | \n",
      "number_7 ::= [.] number_8 \n",
      "number_8 ::= [0-9] number_8 | [0-9] \n",
      "number_9 ::= number_7 | \n",
      "number_10 ::= [eE] number_11 number_12 \n",
      "number_11 ::= [-+] | \n",
      "number_12 ::= [0-9] number_12 | [0-9] \n",
      "number_13 ::= number_10 | \n",
      "numbers ::= [[] space numbers_16 numbers_18 []] space \n",
      "numbers_15 ::= number \n",
      "numbers_16 ::= numbers_15 | \n",
      "numbers_17 ::= [,] space number \n",
      "numbers_18 ::= numbers_17 numbers_18 | \n",
      "root ::= [{] space [\"] [n] [u] [m] [b] [e] [r] [s] [\"] space [:] space numbers [}] space \n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     572.01 ms\n",
      "llama_print_timings:      sample time =    5160.62 ms /    41 runs   (  125.87 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    6667.23 ms /    41 runs   (  162.62 ms per token,     6.15 tokens per second)\n",
      "llama_print_timings:       total time =   11932.42 ms /    42 tokens\n"
     ]
    }
   ],
   "source": [
    "# for i in range(10):\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Count to 10. Respond in JSON.'}\n",
    "    ],\n",
    "    response_format={'type': 'json_object',\n",
    "                        \"schema\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\"numbers\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}\n",
    "                        }}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"numbers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] }\n"
     ]
    }
   ],
   "source": [
    "print(output['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
