{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.embeddings.utils import resolve_embed_model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser.text.sentence import SentenceSplitter\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp import LlamaGrammar\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_METHODS_DIR = Path('data/docs/manual')\n",
    "METHODS_DIR = Path('data/docs/methods')\n",
    "PROMPT_SEEDS_DIR = Path('data/prompts/generation/components')\n",
    "PROMPT_COMPONENTS_DIR = Path('data/prompts/generation/components')\n",
    "VAL_PROMPT_COMPONENTS_DIR = Path('data/prompts/validation/components')\n",
    "GEN_PROMPTS_DIR = Path('data/prompts/generation/output')\n",
    "VAL_PROMPTS_DIR = Path('data/prompts/validation/output')\n",
    "PERSIST_DIR = Path(\"data/persist_dir\")\n",
    "OUTPUT_DIR = Path(\"output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index generation & loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default tokenizer is for gpt-3.5\n",
    "# llama_index.core.global_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = SimpleDirectoryReader(METHODS_DIR).load_data()\n",
    "\n",
    "# embed_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"BAAI/bge-base-en-v1.5\"\n",
    "# )\n",
    "# Settings.text_splitter = SentenceSplitter(chunk_size=678, tokenizer=embed_tokenizer)\n",
    "\n",
    "# Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, show_progress=True\n",
    "# )\n",
    "\n",
    "# index.storage_context.persist(persist_dir=PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have an instance of TreeIndex named tree_index\n",
    "ref_doc_info = index.ref_doc_info\n",
    "\n",
    "# Now you can iterate over the ref_doc_info to view each node's details\n",
    "with open('temp/index.txt', 'w') as f:\n",
    "    for node_id, node_info in ref_doc_info.items():\n",
    "        f.write(f\"Node ID: {node_id}\\n\")\n",
    "        f.write(f\"Node Info: {node_info}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = retriever.retrieve('Get status of a cover.')\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080, compute capability 7.5, VMM: yes\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   102.54 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  5563.55 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3000\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   375.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  375.00 MiB, K (f16):  187.50 MiB, V (f16):  187.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    14.89 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   225.36 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "llm = Llama('models/mistral-7b-instruct-v0.2.Q6_K.gguf', n_ctx=3000, verbose=True, n_gpu_layers=-1) # mistral-7b-instruct-v0.2.Q4_0.gguf mistral-7b-instruct-v0.2.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =      37.16 ms /   100 runs   (    0.37 ms per token,  2691.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     106.00 ms /    43 tokens (    2.47 ms per token,   405.68 tokens per second)\n",
      "llama_print_timings:        eval time =    1698.39 ms /    99 runs   (   17.16 ms per token,    58.29 tokens per second)\n",
      "llama_print_timings:       total time =    2013.33 ms /   142 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-f786b778-41ef-48f4-8f05-57fb3ed3ef4a',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1710750205,\n",
       " 'model': 'models/mistral-7b-instruct-v0.2.Q6_K.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': ' The user wants to control the living room blinds to close them gradually over a period of 20 seconds. In a smart home system, this function can typically be achieved by sending a command to the blinds controller or automation hub to close the blinds and specifying a slow speed or duration setting. This could be implemented using various protocols such as Z-Wave, Zigbee, Wi-Fi, or Bluetooth, depending on the specific smart home system being used.'},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 43, 'completion_tokens': 100, 'total_tokens': 143}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Command: Please, close the living room blinds slowly for 20 seconds.\n",
    "\n",
    "What is the function user want to call in smart home?\n",
    "\"\"\"\n",
    "llm.create_chat_completion(messages=[{'role': 'user', 'content': prompt}], max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 18 10:23:29 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 22%   26C    P2    51W / 215W |   7670MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1411      G   /usr/bin/gnome-shell                3MiB |\n",
      "|    0   N/A  N/A   3957377      C   ...envs/assistant/bin/python     7652MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_scheme_prompt = {\n",
    "    \"method\": {\n",
    "        \"type\": \"string\"\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"type\": \"object\"\n",
    "    }\n",
    "}\n",
    "\n",
    "example_1_json = {\n",
    "  \"method\":\"Cover.Open\",\n",
    "  \"params\":\n",
    "  {\n",
    "    \"id\":2\n",
    "  }\n",
    "}\n",
    "\n",
    "example_2_json = {\n",
    "  \"method\":\"Cover.Close\",\n",
    "  \"params\":\n",
    "  {\n",
    "    \"id\":0,\n",
    "    \"duration\":5,\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(VAL_PROMPT_COMPONENTS_DIR / 'instruction.md') as f:\n",
    "  instruction = f.read()\n",
    "\n",
    "variables = {\n",
    "    \"instruction\": instruction,\n",
    "    \"json_scheme\": \"The output JSON should follow the next scheme: \" + json.dumps(json_scheme_prompt),\n",
    "    \"devices\": \"\"\"Cover id=1\"\"\",\n",
    "    \"example_1\": \"\"\"Devices: Cover id=2\n",
    "Methods:\n",
    "API method 1:\n",
    "Method name: Cover.Open\n",
    "Method description: Preconditions:\n",
    "Cover will not accept the command if:\n",
    "An  overvoltage  error is set at the time of the request.\n",
    "An  undervoltage  error is set at the time of the request.\n",
    "An  overtemp  error is set at the time of the request.\n",
    "An engaged  safety_switch  prohibits movement in the requested direction.\n",
    "Cover  calibration is running at the time of the request\n",
    "Properties:\n",
    "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully open, unless it times out because of maxtime_open first. If duration (seconds) is provided, Cover will move in the open direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
    "Response:\n",
    "null on success; error if the request can not be executed or failed\n",
    "\n",
    "Command: Open the cover.\n",
    "JSON: \"\"\" + json.dumps(example_1_json),\n",
    "\n",
    "    \"example_2\": \"\"\"Devices: Cover id=0\n",
    "Methods: \n",
    "API method 1:\n",
    "Method name: Cover.Close\n",
    "Method description: Preconditions:\n",
    "Cover will not accept the command if:\n",
    "An  overvoltage  error is set at the time of the request.\n",
    "An  undervoltage  error is set at the time of the request.\n",
    "An  overtemp  error is set at the time of the request.\n",
    "An engaged  safety_switch  prohibits movement in the requested direction.\n",
    "Cover  calibration is running at the time of the request\n",
    "Properties:\n",
    "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully close, unless it times out because of maxtime_close first. If duration (seconds) is provided, Cover will move in the close direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
    "Response:\n",
    "null on success; error if the request can not be executed or failed\n",
    "\n",
    "Command: Close the kitchen cover quickly (for 5 seconds).\n",
    "JSON: \"\"\" + json.dumps(example_2_json),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI Assistant that controls the devices in a house. For a given user command create a corresponding JSON object. Don't add properties with null value in output JSON object. Output must be strictly in JSON format.\n",
      "The output JSON should follow the next scheme: {\"method\": {\"type\": \"string\"}, \"params\": {\"type\": \"object\"}}\n",
      "\n",
      "Devices: Cover id=2\n",
      "Methods:\n",
      "API method 1:\n",
      "Method name: Cover.Open\n",
      "Method description: Preconditions:\n",
      "Cover will not accept the command if:\n",
      "An  overvoltage  error is set at the time of the request.\n",
      "An  undervoltage  error is set at the time of the request.\n",
      "An  overtemp  error is set at the time of the request.\n",
      "An engaged  safety_switch  prohibits movement in the requested direction.\n",
      "Cover  calibration is running at the time of the request\n",
      "Properties:\n",
      "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully open, unless it times out because of maxtime_open first. If duration (seconds) is provided, Cover will move in the open direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
      "Response:\n",
      "null on success; error if the request can not be executed or failed\n",
      "\n",
      "Command: Open the cover.\n",
      "JSON: {\"method\": \"Cover.Open\", \"params\": {\"id\": 2}}\n",
      "\n",
      "Devices: Cover id=0\n",
      "Methods: \n",
      "API method 1:\n",
      "Method name: Cover.Close\n",
      "Method description: Preconditions:\n",
      "Cover will not accept the command if:\n",
      "An  overvoltage  error is set at the time of the request.\n",
      "An  undervoltage  error is set at the time of the request.\n",
      "An  overtemp  error is set at the time of the request.\n",
      "An engaged  safety_switch  prohibits movement in the requested direction.\n",
      "Cover  calibration is running at the time of the request\n",
      "Properties:\n",
      "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully close, unless it times out because of maxtime_close first. If duration (seconds) is provided, Cover will move in the close direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
      "Response:\n",
      "null on success; error if the request can not be executed or failed\n",
      "\n",
      "Command: Close the kitchen cover quickly (for 5 seconds).\n",
      "JSON: {\"method\": \"Cover.Close\", \"params\": {\"id\": 0, \"duration\": 5}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_prompt_template = \"\"\"\n",
    "{instruction}\n",
    "{json_scheme}\n",
    "\n",
    "{example_1}\n",
    "\n",
    "{example_2}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_template = \"\"\"Devices: {env}\n",
    "Methods:\n",
    "{methods_description}\n",
    "Command: {user_cmd}\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "base_prompt = base_prompt_template.format(**variables)\n",
    "\n",
    "print(base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "# logger.addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_cpp.llama_grammar.LlamaGrammar object at 0x7f8ad49679a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] \n",
      "ws_31 ::= ws_30 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/grammars/json.gbnf') as f:\n",
    "    grammar_str = f.read()\n",
    "llama_grammar = LlamaGrammar.from_string(grammar_str)\n",
    "print(llama_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     231.05 ms /    34 runs   (    6.80 ms per token,   147.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     483.96 ms /   857 tokens (    0.56 ms per token,  1770.82 tokens per second)\n",
      "llama_print_timings:        eval time =     605.48 ms /    33 runs   (   18.35 ms per token,    54.50 tokens per second)\n",
      "llama_print_timings:       total time =    1412.47 ms /   890 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     230.47 ms /    34 runs   (    6.78 ms per token,   147.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.45 ms /   215 tokens (    0.76 ms per token,  1315.40 tokens per second)\n",
      "llama_print_timings:        eval time =     603.69 ms /    33 runs   (   18.29 ms per token,    54.66 tokens per second)\n",
      "llama_print_timings:       total time =    1086.46 ms /   248 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     336.81 ms /    51 runs   (    6.60 ms per token,   151.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     547.13 ms /   964 tokens (    0.57 ms per token,  1761.91 tokens per second)\n",
      "llama_print_timings:        eval time =     971.80 ms /    50 runs   (   19.44 ms per token,    51.45 tokens per second)\n",
      "llama_print_timings:       total time =    1997.67 ms /  1014 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     238.18 ms /    35 runs   (    6.81 ms per token,   146.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     771.65 ms /  1055 tokens (    0.73 ms per token,  1367.20 tokens per second)\n",
      "llama_print_timings:        eval time =     664.14 ms /    34 runs   (   19.53 ms per token,    51.19 tokens per second)\n",
      "llama_print_timings:       total time =    1774.54 ms /  1089 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     371.82 ms /    55 runs   (    6.76 ms per token,   147.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.33 ms /   253 tokens (    0.65 ms per token,  1530.30 tokens per second)\n",
      "llama_print_timings:        eval time =     994.69 ms /    54 runs   (   18.42 ms per token,    54.29 tokens per second)\n",
      "llama_print_timings:       total time =    1679.79 ms /   307 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     245.69 ms /    36 runs   (    6.82 ms per token,   146.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     547.79 ms /   967 tokens (    0.57 ms per token,  1765.29 tokens per second)\n",
      "llama_print_timings:        eval time =     679.98 ms /    35 runs   (   19.43 ms per token,    51.47 tokens per second)\n",
      "llama_print_timings:       total time =    1574.39 ms /  1002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     225.48 ms /    33 runs   (    6.83 ms per token,   146.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     405.38 ms /   631 tokens (    0.64 ms per token,  1556.56 tokens per second)\n",
      "llama_print_timings:        eval time =     606.64 ms /    32 runs   (   18.96 ms per token,    52.75 tokens per second)\n",
      "llama_print_timings:       total time =    1330.02 ms /   663 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     267.06 ms /    40 runs   (    6.68 ms per token,   149.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     548.09 ms /   962 tokens (    0.57 ms per token,  1755.18 tokens per second)\n",
      "llama_print_timings:        eval time =     758.39 ms /    39 runs   (   19.45 ms per token,    51.42 tokens per second)\n",
      "llama_print_timings:       total time =    1685.95 ms /  1001 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     170.94 ms /    25 runs   (    6.84 ms per token,   146.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.58 ms /   130 tokens (    1.07 ms per token,   938.11 tokens per second)\n",
      "llama_print_timings:        eval time =     437.81 ms /    24 runs   (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:       total time =     814.66 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.15 ms /    22 runs   (    6.92 ms per token,   144.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     211.50 ms /   312 tokens (    0.68 ms per token,  1475.21 tokens per second)\n",
      "llama_print_timings:        eval time =     388.18 ms /    21 runs   (   18.48 ms per token,    54.10 tokens per second)\n",
      "llama_print_timings:       total time =     812.38 ms /   333 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     151.92 ms /    22 runs   (    6.91 ms per token,   144.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.96 ms /   329 tokens (    0.66 ms per token,  1523.42 tokens per second)\n",
      "llama_print_timings:        eval time =     388.59 ms /    21 runs   (   18.50 ms per token,    54.04 tokens per second)\n",
      "llama_print_timings:       total time =     817.87 ms /   350 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     156.20 ms /    23 runs   (    6.79 ms per token,   147.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.75 ms /   327 tokens (    0.66 ms per token,  1522.70 tokens per second)\n",
      "llama_print_timings:        eval time =     406.56 ms /    22 runs   (   18.48 ms per token,    54.11 tokens per second)\n",
      "llama_print_timings:       total time =     839.34 ms /   349 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     150.91 ms /    22 runs   (    6.86 ms per token,   145.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     211.66 ms /   314 tokens (    0.67 ms per token,  1483.48 tokens per second)\n",
      "llama_print_timings:        eval time =     388.10 ms /    21 runs   (   18.48 ms per token,    54.11 tokens per second)\n",
      "llama_print_timings:       total time =     809.93 ms /   335 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     211.08 ms /    31 runs   (    6.81 ms per token,   146.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     211.08 ms /   311 tokens (    0.68 ms per token,  1473.35 tokens per second)\n",
      "llama_print_timings:        eval time =     554.73 ms /    30 runs   (   18.49 ms per token,    54.08 tokens per second)\n",
      "llama_print_timings:       total time =    1060.74 ms /   341 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     151.18 ms /    22 runs   (    6.87 ms per token,   145.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     211.92 ms /   315 tokens (    0.67 ms per token,  1486.41 tokens per second)\n",
      "llama_print_timings:        eval time =     388.01 ms /    21 runs   (   18.48 ms per token,    54.12 tokens per second)\n",
      "llama_print_timings:       total time =     811.42 ms /   336 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     190.58 ms /    28 runs   (    6.81 ms per token,   146.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     211.60 ms /   311 tokens (    0.68 ms per token,  1469.75 tokens per second)\n",
      "llama_print_timings:        eval time =     499.25 ms /    27 runs   (   18.49 ms per token,    54.08 tokens per second)\n",
      "llama_print_timings:       total time =     977.17 ms /   338 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     130.75 ms /    19 runs   (    6.88 ms per token,   145.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     446.75 ms /   749 tokens (    0.60 ms per token,  1676.55 tokens per second)\n",
      "llama_print_timings:        eval time =     344.89 ms /    18 runs   (   19.16 ms per token,    52.19 tokens per second)\n",
      "llama_print_timings:       total time =     976.78 ms /   767 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.42 ms /    21 runs   (    6.83 ms per token,   146.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     446.70 ms /   750 tokens (    0.60 ms per token,  1678.99 tokens per second)\n",
      "llama_print_timings:        eval time =     383.12 ms /    20 runs   (   19.16 ms per token,    52.20 tokens per second)\n",
      "llama_print_timings:       total time =    1033.24 ms /   770 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.25 ms /    21 runs   (    6.82 ms per token,   146.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     449.52 ms /   755 tokens (    0.60 ms per token,  1679.57 tokens per second)\n",
      "llama_print_timings:        eval time =     383.36 ms /    20 runs   (   19.17 ms per token,    52.17 tokens per second)\n",
      "llama_print_timings:       total time =    1035.52 ms /   775 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     145.44 ms /    21 runs   (    6.93 ms per token,   144.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     448.57 ms /   756 tokens (    0.59 ms per token,  1685.34 tokens per second)\n",
      "llama_print_timings:        eval time =     383.66 ms /    20 runs   (   19.18 ms per token,    52.13 tokens per second)\n",
      "llama_print_timings:       total time =    1038.96 ms /   776 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.26 ms /    21 runs   (    6.82 ms per token,   146.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     448.51 ms /   757 tokens (    0.59 ms per token,  1687.81 tokens per second)\n",
      "llama_print_timings:        eval time =     383.28 ms /    20 runs   (   19.16 ms per token,    52.18 tokens per second)\n",
      "llama_print_timings:       total time =    1035.34 ms /   777 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.52 ms /    21 runs   (    6.83 ms per token,   146.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     449.19 ms /   754 tokens (    0.60 ms per token,  1678.59 tokens per second)\n",
      "llama_print_timings:        eval time =     383.42 ms /    20 runs   (   19.17 ms per token,    52.16 tokens per second)\n",
      "llama_print_timings:       total time =    1035.45 ms /   774 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.12 ms /    21 runs   (    6.82 ms per token,   146.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     448.31 ms /   755 tokens (    0.59 ms per token,  1684.12 tokens per second)\n",
      "llama_print_timings:        eval time =     383.04 ms /    20 runs   (   19.15 ms per token,    52.21 tokens per second)\n",
      "llama_print_timings:       total time =    1034.53 ms /   775 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.85 ms /    21 runs   (    6.85 ms per token,   145.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     449.03 ms /   758 tokens (    0.59 ms per token,  1688.10 tokens per second)\n",
      "llama_print_timings:        eval time =     383.68 ms /    20 runs   (   19.18 ms per token,    52.13 tokens per second)\n",
      "llama_print_timings:       total time =    1037.05 ms /   778 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     173.21 ms /    25 runs   (    6.93 ms per token,   144.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.62 ms /   124 tokens (    1.01 ms per token,   987.07 tokens per second)\n",
      "llama_print_timings:        eval time =     437.46 ms /    24 runs   (   18.23 ms per token,    54.86 tokens per second)\n",
      "llama_print_timings:       total time =     805.30 ms /   148 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     164.46 ms /    24 runs   (    6.85 ms per token,   145.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.15 ms /   117 tokens (    1.06 ms per token,   942.42 tokens per second)\n",
      "llama_print_timings:        eval time =     419.26 ms /    23 runs   (   18.23 ms per token,    54.86 tokens per second)\n",
      "llama_print_timings:       total time =     773.32 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     164.34 ms /    24 runs   (    6.85 ms per token,   146.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.20 ms /   122 tokens (    1.03 ms per token,   974.48 tokens per second)\n",
      "llama_print_timings:        eval time =     419.15 ms /    23 runs   (   18.22 ms per token,    54.87 tokens per second)\n",
      "llama_print_timings:       total time =     774.18 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     164.09 ms /    24 runs   (    6.84 ms per token,   146.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.56 ms /   119 tokens (    1.05 ms per token,   955.33 tokens per second)\n",
      "llama_print_timings:        eval time =     418.88 ms /    23 runs   (   18.21 ms per token,    54.91 tokens per second)\n",
      "llama_print_timings:       total time =     773.86 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     237.83 ms /    35 runs   (    6.80 ms per token,   147.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     548.77 ms /   966 tokens (    0.57 ms per token,  1760.30 tokens per second)\n",
      "llama_print_timings:        eval time =     661.02 ms /    34 runs   (   19.44 ms per token,    51.44 tokens per second)\n",
      "llama_print_timings:       total time =    1547.78 ms /  1000 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     204.52 ms /    30 runs   (    6.82 ms per token,   146.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     800.39 ms /  1337 tokens (    0.60 ms per token,  1670.43 tokens per second)\n",
      "llama_print_timings:        eval time =     578.76 ms /    29 runs   (   19.96 ms per token,    50.11 tokens per second)\n",
      "llama_print_timings:       total time =    1674.13 ms /  1366 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     204.40 ms /    29 runs   (    7.05 ms per token,   141.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     801.33 ms /  1338 tokens (    0.60 ms per token,  1669.73 tokens per second)\n",
      "llama_print_timings:        eval time =     558.77 ms /    28 runs   (   19.96 ms per token,    50.11 tokens per second)\n",
      "llama_print_timings:       total time =    1650.12 ms /  1366 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     217.70 ms /    32 runs   (    6.80 ms per token,   146.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     802.50 ms /  1339 tokens (    0.60 ms per token,  1668.54 tokens per second)\n",
      "llama_print_timings:        eval time =     619.61 ms /    31 runs   (   19.99 ms per token,    50.03 tokens per second)\n",
      "llama_print_timings:       total time =    1735.32 ms /  1370 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     238.97 ms /    33 runs   (    7.24 ms per token,   138.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     802.98 ms /  1339 tokens (    0.60 ms per token,  1667.54 tokens per second)\n",
      "llama_print_timings:        eval time =     639.47 ms /    32 runs   (   19.98 ms per token,    50.04 tokens per second)\n",
      "llama_print_timings:       total time =    1779.38 ms /  1371 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     200.99 ms /    28 runs   (    7.18 ms per token,   139.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.01 ms /   325 tokens (    0.66 ms per token,  1511.57 tokens per second)\n",
      "llama_print_timings:        eval time =     499.66 ms /    27 runs   (   18.51 ms per token,    54.04 tokens per second)\n",
      "llama_print_timings:       total time =     993.41 ms /   352 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     218.13 ms /    32 runs   (    6.82 ms per token,   146.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     794.43 ms /  1308 tokens (    0.61 ms per token,  1646.47 tokens per second)\n",
      "llama_print_timings:        eval time =     619.51 ms /    31 runs   (   19.98 ms per token,    50.04 tokens per second)\n",
      "llama_print_timings:       total time =    1727.19 ms /  1339 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     343.87 ms /    50 runs   (    6.88 ms per token,   145.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     452.93 ms /   763 tokens (    0.59 ms per token,  1684.59 tokens per second)\n",
      "llama_print_timings:        eval time =     941.21 ms /    49 runs   (   19.21 ms per token,    52.06 tokens per second)\n",
      "llama_print_timings:       total time =    1881.36 ms /   812 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     150.39 ms /    22 runs   (    6.84 ms per token,   146.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     790.86 ms /  1300 tokens (    0.61 ms per token,  1643.77 tokens per second)\n",
      "llama_print_timings:        eval time =     419.10 ms /    21 runs   (   19.96 ms per token,    50.11 tokens per second)\n",
      "llama_print_timings:       total time =    1427.36 ms /  1321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     149.44 ms /    22 runs   (    6.79 ms per token,   147.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     791.30 ms /  1300 tokens (    0.61 ms per token,  1642.87 tokens per second)\n",
      "llama_print_timings:        eval time =     418.77 ms /    21 runs   (   19.94 ms per token,    50.15 tokens per second)\n",
      "llama_print_timings:       total time =    1426.73 ms /  1321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     202.18 ms /    29 runs   (    6.97 ms per token,   143.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     447.19 ms /   745 tokens (    0.60 ms per token,  1665.97 tokens per second)\n",
      "llama_print_timings:        eval time =     536.85 ms /    28 runs   (   19.17 ms per token,    52.16 tokens per second)\n",
      "llama_print_timings:       total time =    1269.96 ms /   773 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     150.07 ms /    22 runs   (    6.82 ms per token,   146.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     790.71 ms /  1299 tokens (    0.61 ms per token,  1642.83 tokens per second)\n",
      "llama_print_timings:        eval time =     418.92 ms /    21 runs   (   19.95 ms per token,    50.13 tokens per second)\n",
      "llama_print_timings:       total time =    1426.95 ms /  1320 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     148.58 ms /    22 runs   (    6.75 ms per token,   148.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     789.18 ms /  1294 tokens (    0.61 ms per token,  1639.68 tokens per second)\n",
      "llama_print_timings:        eval time =     418.71 ms /    21 runs   (   19.94 ms per token,    50.15 tokens per second)\n",
      "llama_print_timings:       total time =    1423.02 ms /  1315 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     153.04 ms /    22 runs   (    6.96 ms per token,   143.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     791.13 ms /  1301 tokens (    0.61 ms per token,  1644.47 tokens per second)\n",
      "llama_print_timings:        eval time =     419.24 ms /    21 runs   (   19.96 ms per token,    50.09 tokens per second)\n",
      "llama_print_timings:       total time =    1433.16 ms /  1322 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.13 ms /    22 runs   (    6.92 ms per token,   144.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     748.58 ms /  1267 tokens (    0.59 ms per token,  1692.55 tokens per second)\n",
      "llama_print_timings:        eval time =     418.30 ms /    21 runs   (   19.92 ms per token,    50.20 tokens per second)\n",
      "llama_print_timings:       total time =    1387.50 ms /  1288 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     149.77 ms /    22 runs   (    6.81 ms per token,   146.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     791.03 ms /  1300 tokens (    0.61 ms per token,  1643.43 tokens per second)\n",
      "llama_print_timings:        eval time =     418.82 ms /    21 runs   (   19.94 ms per token,    50.14 tokens per second)\n",
      "llama_print_timings:       total time =    1426.51 ms /  1321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.41 ms /    21 runs   (    6.83 ms per token,   146.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     791.03 ms /  1300 tokens (    0.61 ms per token,  1643.42 tokens per second)\n",
      "llama_print_timings:        eval time =     398.87 ms /    20 runs   (   19.94 ms per token,    50.14 tokens per second)\n",
      "llama_print_timings:       total time =    1397.51 ms /  1320 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     144.46 ms /    21 runs   (    6.88 ms per token,   145.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     791.44 ms /  1300 tokens (    0.61 ms per token,  1642.58 tokens per second)\n",
      "llama_print_timings:        eval time =     399.12 ms /    20 runs   (   19.96 ms per token,    50.11 tokens per second)\n",
      "llama_print_timings:       total time =    1399.81 ms /  1320 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     361.68 ms /    53 runs   (    6.82 ms per token,   146.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     679.38 ms /  1057 tokens (    0.64 ms per token,  1555.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1017.45 ms /    52 runs   (   19.57 ms per token,    51.11 tokens per second)\n",
      "llama_print_timings:       total time =    2212.92 ms /  1109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     202.45 ms /    29 runs   (    6.98 ms per token,   143.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     594.20 ms /  1028 tokens (    0.58 ms per token,  1730.06 tokens per second)\n",
      "llama_print_timings:        eval time =     547.80 ms /    28 runs   (   19.56 ms per token,    51.11 tokens per second)\n",
      "llama_print_timings:       total time =    1433.01 ms /  1056 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     228.32 ms /    33 runs   (    6.92 ms per token,   144.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     790.30 ms /  1297 tokens (    0.61 ms per token,  1641.16 tokens per second)\n",
      "llama_print_timings:        eval time =     638.55 ms /    32 runs   (   19.95 ms per token,    50.11 tokens per second)\n",
      "llama_print_timings:       total time =    1758.50 ms /  1329 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     199.87 ms /    29 runs   (    6.89 ms per token,   145.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     591.72 ms /  1027 tokens (    0.58 ms per token,  1735.61 tokens per second)\n",
      "llama_print_timings:        eval time =     547.40 ms /    28 runs   (   19.55 ms per token,    51.15 tokens per second)\n",
      "llama_print_timings:       total time =    1426.86 ms /  1055 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     208.49 ms /    30 runs   (    6.95 ms per token,   143.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     776.41 ms /  1056 tokens (    0.74 ms per token,  1360.11 tokens per second)\n",
      "llama_print_timings:        eval time =     567.75 ms /    29 runs   (   19.58 ms per token,    51.08 tokens per second)\n",
      "llama_print_timings:       total time =    1643.94 ms /  1085 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     227.15 ms /    33 runs   (    6.88 ms per token,   145.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     792.24 ms /  1300 tokens (    0.61 ms per token,  1640.93 tokens per second)\n",
      "llama_print_timings:        eval time =     638.87 ms /    32 runs   (   19.96 ms per token,    50.09 tokens per second)\n",
      "llama_print_timings:       total time =    1757.10 ms /  1332 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     199.66 ms /    29 runs   (    6.88 ms per token,   145.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.95 ms /   223 tokens (    0.74 ms per token,  1343.80 tokens per second)\n",
      "llama_print_timings:        eval time =     513.55 ms /    28 runs   (   18.34 ms per token,    54.52 tokens per second)\n",
      "llama_print_timings:       total time =     959.60 ms /   251 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     151.67 ms /    22 runs   (    6.89 ms per token,   145.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     164.37 ms /   215 tokens (    0.76 ms per token,  1308.06 tokens per second)\n",
      "llama_print_timings:        eval time =     384.79 ms /    21 runs   (   18.32 ms per token,    54.58 tokens per second)\n",
      "llama_print_timings:       total time =     761.78 ms /   236 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     207.02 ms /    30 runs   (    6.90 ms per token,   144.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     155.07 ms /    22 tokens (    7.05 ms per token,   141.87 tokens per second)\n",
      "llama_print_timings:        eval time =     531.87 ms /    29 runs   (   18.34 ms per token,    54.52 tokens per second)\n",
      "llama_print_timings:       total time =     978.98 ms /    51 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     328.58 ms /    48 runs   (    6.85 ms per token,   146.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.79 ms /   262 tokens (    0.76 ms per token,  1311.40 tokens per second)\n",
      "llama_print_timings:        eval time =     867.76 ms /    47 runs   (   18.46 ms per token,    54.16 tokens per second)\n",
      "llama_print_timings:       total time =    1532.83 ms /   309 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     205.07 ms /    29 runs   (    7.07 ms per token,   141.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.91 ms /   254 tokens (    0.65 ms per token,  1530.95 tokens per second)\n",
      "llama_print_timings:        eval time =     517.32 ms /    28 runs   (   18.48 ms per token,    54.13 tokens per second)\n",
      "llama_print_timings:       total time =     972.21 ms /   282 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     407.60 ms /    61 runs   (    6.68 ms per token,   149.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     680.15 ms /  1063 tokens (    0.64 ms per token,  1562.89 tokens per second)\n",
      "llama_print_timings:        eval time =    1177.20 ms /    60 runs   (   19.62 ms per token,    50.97 tokens per second)\n",
      "llama_print_timings:       total time =    2443.86 ms /  1123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.86 ms /    21 runs   (    6.85 ms per token,   145.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     568.94 ms /  1024 tokens (    0.56 ms per token,  1799.83 tokens per second)\n",
      "llama_print_timings:        eval time =     391.44 ms /    20 runs   (   19.57 ms per token,    51.09 tokens per second)\n",
      "llama_print_timings:       total time =    1166.97 ms /  1044 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     144.79 ms /    21 runs   (    6.89 ms per token,   145.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     593.64 ms /  1027 tokens (    0.58 ms per token,  1730.01 tokens per second)\n",
      "llama_print_timings:        eval time =     391.68 ms /    20 runs   (   19.58 ms per token,    51.06 tokens per second)\n",
      "llama_print_timings:       total time =    1194.90 ms /  1047 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     259.46 ms /    38 runs   (    6.83 ms per token,   146.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     593.23 ms /  1027 tokens (    0.58 ms per token,  1731.19 tokens per second)\n",
      "llama_print_timings:        eval time =     724.34 ms /    37 runs   (   19.58 ms per token,    51.08 tokens per second)\n",
      "llama_print_timings:       total time =    1688.86 ms /  1064 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     149.24 ms /    22 runs   (    6.78 ms per token,   147.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     601.64 ms /  1030 tokens (    0.58 ms per token,  1711.99 tokens per second)\n",
      "llama_print_timings:        eval time =     411.23 ms /    21 runs   (   19.58 ms per token,    51.07 tokens per second)\n",
      "llama_print_timings:       total time =    1228.60 ms /  1051 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     349.82 ms /    51 runs   (    6.86 ms per token,   145.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     793.19 ms /  1298 tokens (    0.61 ms per token,  1636.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1000.13 ms /    50 runs   (   20.00 ms per token,    49.99 tokens per second)\n",
      "llama_print_timings:       total time =    2295.24 ms /  1348 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.26 ms /    22 runs   (    6.92 ms per token,   144.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.39 ms /   267 tokens (    0.75 ms per token,  1325.77 tokens per second)\n",
      "llama_print_timings:        eval time =     388.63 ms /    21 runs   (   18.51 ms per token,    54.04 tokens per second)\n",
      "llama_print_timings:       total time =     804.53 ms /   288 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     148.97 ms /    22 runs   (    6.77 ms per token,   147.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     593.36 ms /  1027 tokens (    0.58 ms per token,  1730.82 tokens per second)\n",
      "llama_print_timings:        eval time =     411.36 ms /    21 runs   (   19.59 ms per token,    51.05 tokens per second)\n",
      "llama_print_timings:       total time =    1220.90 ms /  1048 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     190.22 ms /    27 runs   (    7.05 ms per token,   141.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     544.31 ms /   944 tokens (    0.58 ms per token,  1734.31 tokens per second)\n",
      "llama_print_timings:        eval time =     506.91 ms /    26 runs   (   19.50 ms per token,    51.29 tokens per second)\n",
      "llama_print_timings:       total time =    1321.60 ms /   970 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     207.59 ms /    30 runs   (    6.92 ms per token,   144.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.47 ms /   131 tokens (    1.06 ms per token,   939.26 tokens per second)\n",
      "llama_print_timings:        eval time =     530.13 ms /    29 runs   (   18.28 ms per token,    54.70 tokens per second)\n",
      "llama_print_timings:       total time =     961.83 ms /   160 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     276.42 ms /    41 runs   (    6.74 ms per token,   148.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.63 ms /   329 tokens (    0.66 ms per token,  1518.70 tokens per second)\n",
      "llama_print_timings:        eval time =     742.34 ms /    40 runs   (   18.56 ms per token,    53.88 tokens per second)\n",
      "llama_print_timings:       total time =    1351.78 ms /   369 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     310.46 ms /    45 runs   (    6.90 ms per token,   144.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     217.01 ms /   331 tokens (    0.66 ms per token,  1525.30 tokens per second)\n",
      "llama_print_timings:        eval time =     816.18 ms /    44 runs   (   18.55 ms per token,    53.91 tokens per second)\n",
      "llama_print_timings:       total time =    1470.96 ms /   375 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     201.81 ms /    29 runs   (    6.96 ms per token,   143.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.56 ms /   131 tokens (    1.07 ms per token,   938.65 tokens per second)\n",
      "llama_print_timings:        eval time =     511.79 ms /    28 runs   (   18.28 ms per token,    54.71 tokens per second)\n",
      "llama_print_timings:       total time =     935.15 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     311.41 ms /    46 runs   (    6.77 ms per token,   147.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     217.45 ms /   334 tokens (    0.65 ms per token,  1536.01 tokens per second)\n",
      "llama_print_timings:        eval time =     835.86 ms /    45 runs   (   18.57 ms per token,    53.84 tokens per second)\n",
      "llama_print_timings:       total time =    1494.88 ms /   379 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     206.11 ms /    30 runs   (    6.87 ms per token,   145.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.19 ms /   129 tokens (    1.08 ms per token,   926.78 tokens per second)\n",
      "llama_print_timings:        eval time =     530.26 ms /    29 runs   (   18.28 ms per token,    54.69 tokens per second)\n",
      "llama_print_timings:       total time =     960.17 ms /   158 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     176.65 ms /    26 runs   (    6.79 ms per token,   147.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.40 ms /   241 tokens (    0.68 ms per token,  1474.94 tokens per second)\n",
      "llama_print_timings:        eval time =     462.50 ms /    25 runs   (   18.50 ms per token,    54.05 tokens per second)\n",
      "llama_print_timings:       total time =     875.67 ms /   266 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     217.84 ms /    32 runs   (    6.81 ms per token,   146.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     754.35 ms /  1052 tokens (    0.72 ms per token,  1394.57 tokens per second)\n",
      "llama_print_timings:        eval time =     608.52 ms /    31 runs   (   19.63 ms per token,    50.94 tokens per second)\n",
      "llama_print_timings:       total time =    1677.21 ms /  1083 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     138.56 ms /    20 runs   (    6.93 ms per token,   144.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.49 ms /   123 tokens (    1.02 ms per token,   980.14 tokens per second)\n",
      "llama_print_timings:        eval time =     347.49 ms /    19 runs   (   18.29 ms per token,    54.68 tokens per second)\n",
      "llama_print_timings:       total time =     668.34 ms /   142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     159.21 ms /    23 runs   (    6.92 ms per token,   144.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     144.98 ms /   153 tokens (    0.95 ms per token,  1055.30 tokens per second)\n",
      "llama_print_timings:        eval time =     403.17 ms /    22 runs   (   18.33 ms per token,    54.57 tokens per second)\n",
      "llama_print_timings:       total time =     773.60 ms /   175 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     156.82 ms /    23 runs   (    6.82 ms per token,   146.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.72 ms /   156 tokens (    0.93 ms per token,  1070.51 tokens per second)\n",
      "llama_print_timings:        eval time =     402.84 ms /    22 runs   (   18.31 ms per token,    54.61 tokens per second)\n",
      "llama_print_timings:       total time =     769.44 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     156.75 ms /    23 runs   (    6.82 ms per token,   146.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.09 ms /   154 tokens (    0.94 ms per token,  1061.43 tokens per second)\n",
      "llama_print_timings:        eval time =     402.98 ms /    22 runs   (   18.32 ms per token,    54.59 tokens per second)\n",
      "llama_print_timings:       total time =     769.04 ms /   176 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     224.21 ms /    32 runs   (    7.01 ms per token,   142.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.52 ms /  1049 tokens (    0.72 ms per token,  1392.13 tokens per second)\n",
      "llama_print_timings:        eval time =     608.15 ms /    31 runs   (   19.62 ms per token,    50.97 tokens per second)\n",
      "llama_print_timings:       total time =    1681.56 ms /  1080 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     319.21 ms /    45 runs   (    7.09 ms per token,   140.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.01 ms /  1049 tokens (    0.72 ms per token,  1393.08 tokens per second)\n",
      "llama_print_timings:        eval time =     863.84 ms /    44 runs   (   19.63 ms per token,    50.94 tokens per second)\n",
      "llama_print_timings:       total time =    2069.57 ms /  1093 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     138.09 ms /    20 runs   (    6.90 ms per token,   144.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     147.28 ms /   160 tokens (    0.92 ms per token,  1086.34 tokens per second)\n",
      "llama_print_timings:        eval time =     347.66 ms /    19 runs   (   18.30 ms per token,    54.65 tokens per second)\n",
      "llama_print_timings:       total time =     690.03 ms /   179 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     198.80 ms /    29 runs   (    6.86 ms per token,   145.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     207.42 ms /   292 tokens (    0.71 ms per token,  1407.76 tokens per second)\n",
      "llama_print_timings:        eval time =     519.42 ms /    28 runs   (   18.55 ms per token,    53.91 tokens per second)\n",
      "llama_print_timings:       total time =    1007.42 ms /   320 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     151.01 ms /    22 runs   (    6.86 ms per token,   145.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     207.72 ms /   290 tokens (    0.72 ms per token,  1396.10 tokens per second)\n",
      "llama_print_timings:        eval time =     389.30 ms /    21 runs   (   18.54 ms per token,    53.94 tokens per second)\n",
      "llama_print_timings:       total time =     811.05 ms /   311 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     199.38 ms /    29 runs   (    6.88 ms per token,   145.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     207.55 ms /   292 tokens (    0.71 ms per token,  1406.92 tokens per second)\n",
      "llama_print_timings:        eval time =     519.13 ms /    28 runs   (   18.54 ms per token,    53.94 tokens per second)\n",
      "llama_print_timings:       total time =    1008.22 ms /   320 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.56 ms /    21 runs   (    6.84 ms per token,   146.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     540.75 ms /   931 tokens (    0.58 ms per token,  1721.67 tokens per second)\n",
      "llama_print_timings:        eval time =     390.39 ms /    20 runs   (   19.52 ms per token,    51.23 tokens per second)\n",
      "llama_print_timings:       total time =    1137.95 ms /   951 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     144.03 ms /    21 runs   (    6.86 ms per token,   145.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     543.22 ms /   940 tokens (    0.58 ms per token,  1730.43 tokens per second)\n",
      "llama_print_timings:        eval time =     390.39 ms /    20 runs   (   19.52 ms per token,    51.23 tokens per second)\n",
      "llama_print_timings:       total time =    1140.45 ms /   960 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     218.94 ms /    32 runs   (    6.84 ms per token,   146.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     542.04 ms /   934 tokens (    0.58 ms per token,  1723.13 tokens per second)\n",
      "llama_print_timings:        eval time =     606.56 ms /    31 runs   (   19.57 ms per token,    51.11 tokens per second)\n",
      "llama_print_timings:       total time =    1462.42 ms /   965 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     214.00 ms /    31 runs   (    6.90 ms per token,   144.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     543.43 ms /   939 tokens (    0.58 ms per token,  1727.92 tokens per second)\n",
      "llama_print_timings:        eval time =     585.69 ms /    30 runs   (   19.52 ms per token,    51.22 tokens per second)\n",
      "llama_print_timings:       total time =    1435.66 ms /   969 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     367.91 ms /    55 runs   (    6.69 ms per token,   149.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     789.64 ms /  1282 tokens (    0.62 ms per token,  1623.51 tokens per second)\n",
      "llama_print_timings:        eval time =    1079.65 ms /    54 runs   (   19.99 ms per token,    50.02 tokens per second)\n",
      "llama_print_timings:       total time =    2402.90 ms /  1336 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     710.13 ms /   104 runs   (    6.83 ms per token,   146.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     205.29 ms /   282 tokens (    0.73 ms per token,  1373.68 tokens per second)\n",
      "llama_print_timings:        eval time =    1910.66 ms /   103 runs   (   18.55 ms per token,    53.91 tokens per second)\n",
      "llama_print_timings:       total time =    3125.13 ms /   385 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     150.37 ms /    22 runs   (    6.83 ms per token,   146.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     202.87 ms /   273 tokens (    0.74 ms per token,  1345.68 tokens per second)\n",
      "llama_print_timings:        eval time =     388.51 ms /    21 runs   (   18.50 ms per token,    54.05 tokens per second)\n",
      "llama_print_timings:       total time =     805.11 ms /   294 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     193.77 ms /    28 runs   (    6.92 ms per token,   144.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     204.02 ms /   278 tokens (    0.73 ms per token,  1362.64 tokens per second)\n",
      "llama_print_timings:        eval time =     499.53 ms /    27 runs   (   18.50 ms per token,    54.05 tokens per second)\n",
      "llama_print_timings:       total time =     977.30 ms /   305 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     149.89 ms /    22 runs   (    6.81 ms per token,   146.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     202.12 ms /   270 tokens (    0.75 ms per token,  1335.81 tokens per second)\n",
      "llama_print_timings:        eval time =     388.26 ms /    21 runs   (   18.49 ms per token,    54.09 tokens per second)\n",
      "llama_print_timings:       total time =     803.13 ms /   291 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     150.73 ms /    22 runs   (    6.85 ms per token,   145.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.82 ms /   268 tokens (    0.75 ms per token,  1327.90 tokens per second)\n",
      "llama_print_timings:        eval time =     388.34 ms /    21 runs   (   18.49 ms per token,    54.08 tokens per second)\n",
      "llama_print_timings:       total time =     804.32 ms /   289 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     199.16 ms /    29 runs   (    6.87 ms per token,   145.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     202.50 ms /   272 tokens (    0.74 ms per token,  1343.23 tokens per second)\n",
      "llama_print_timings:        eval time =     517.71 ms /    28 runs   (   18.49 ms per token,    54.08 tokens per second)\n",
      "llama_print_timings:       total time =    1001.25 ms /   300 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     151.20 ms /    22 runs   (    6.87 ms per token,   145.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     203.29 ms /   275 tokens (    0.74 ms per token,  1352.75 tokens per second)\n",
      "llama_print_timings:        eval time =     388.48 ms /    21 runs   (   18.50 ms per token,    54.06 tokens per second)\n",
      "llama_print_timings:       total time =     806.83 ms /   296 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     157.75 ms /    23 runs   (    6.86 ms per token,   145.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.66 ms /   211 tokens (    0.78 ms per token,  1289.26 tokens per second)\n",
      "llama_print_timings:        eval time =     403.64 ms /    22 runs   (   18.35 ms per token,    54.50 tokens per second)\n",
      "llama_print_timings:       total time =     791.23 ms /   233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     157.21 ms /    23 runs   (    6.84 ms per token,   146.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.88 ms /   212 tokens (    0.77 ms per token,  1293.66 tokens per second)\n",
      "llama_print_timings:        eval time =     403.65 ms /    22 runs   (   18.35 ms per token,    54.50 tokens per second)\n",
      "llama_print_timings:       total time =     790.42 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     157.51 ms /    23 runs   (    6.85 ms per token,   146.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.45 ms /   211 tokens (    0.77 ms per token,  1290.92 tokens per second)\n",
      "llama_print_timings:        eval time =     403.40 ms /    22 runs   (   18.34 ms per token,    54.54 tokens per second)\n",
      "llama_print_timings:       total time =     789.65 ms /   233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     145.94 ms /    21 runs   (    6.95 ms per token,   143.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.35 ms /    98 tokens (    1.23 ms per token,   814.27 tokens per second)\n",
      "llama_print_timings:        eval time =     364.30 ms /    20 runs   (   18.22 ms per token,    54.90 tokens per second)\n",
      "llama_print_timings:       total time =     689.93 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     145.83 ms /    21 runs   (    6.94 ms per token,   144.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.95 ms /   101 tokens (    1.20 ms per token,   835.04 tokens per second)\n",
      "llama_print_timings:        eval time =     364.64 ms /    20 runs   (   18.23 ms per token,    54.85 tokens per second)\n",
      "llama_print_timings:       total time =     691.04 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     146.25 ms /    21 runs   (    6.96 ms per token,   143.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.91 ms /   101 tokens (    1.20 ms per token,   835.35 tokens per second)\n",
      "llama_print_timings:        eval time =     364.65 ms /    20 runs   (   18.23 ms per token,    54.85 tokens per second)\n",
      "llama_print_timings:       total time =     691.95 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     159.10 ms /    23 runs   (    6.92 ms per token,   144.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.04 ms /   190 tokens (    0.81 ms per token,  1233.47 tokens per second)\n",
      "llama_print_timings:        eval time =     403.05 ms /    22 runs   (   18.32 ms per token,    54.58 tokens per second)\n",
      "llama_print_timings:       total time =     782.04 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     159.19 ms /    23 runs   (    6.92 ms per token,   144.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.10 ms /   191 tokens (    0.81 ms per token,  1239.45 tokens per second)\n",
      "llama_print_timings:        eval time =     403.17 ms /    22 runs   (   18.33 ms per token,    54.57 tokens per second)\n",
      "llama_print_timings:       total time =     781.63 ms /   213 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     164.48 ms /    24 runs   (    6.85 ms per token,   145.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     146.30 ms /   159 tokens (    0.92 ms per token,  1086.84 tokens per second)\n",
      "llama_print_timings:        eval time =     420.75 ms /    23 runs   (   18.29 ms per token,    54.66 tokens per second)\n",
      "llama_print_timings:       total time =     799.86 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.23 ms /    21 runs   (    6.82 ms per token,   146.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     800.52 ms /  1321 tokens (    0.61 ms per token,  1650.18 tokens per second)\n",
      "llama_print_timings:        eval time =     400.94 ms /    20 runs   (   20.05 ms per token,    49.88 tokens per second)\n",
      "llama_print_timings:       total time =    1410.12 ms /  1341 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     144.18 ms /    21 runs   (    6.87 ms per token,   145.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     803.41 ms /  1327 tokens (    0.61 ms per token,  1651.71 tokens per second)\n",
      "llama_print_timings:        eval time =     401.05 ms /    20 runs   (   20.05 ms per token,    49.87 tokens per second)\n",
      "llama_print_timings:       total time =    1414.61 ms /  1347 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.70 ms /    21 runs   (    6.84 ms per token,   146.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     802.58 ms /  1325 tokens (    0.61 ms per token,  1650.93 tokens per second)\n",
      "llama_print_timings:        eval time =     400.79 ms /    20 runs   (   20.04 ms per token,    49.90 tokens per second)\n",
      "llama_print_timings:       total time =    1413.06 ms /  1345 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     182.09 ms /    27 runs   (    6.74 ms per token,   148.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     806.20 ms /  1336 tokens (    0.60 ms per token,  1657.17 tokens per second)\n",
      "llama_print_timings:        eval time =     520.88 ms /    26 runs   (   20.03 ms per token,    49.92 tokens per second)\n",
      "llama_print_timings:       total time =    1592.57 ms /  1362 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     144.52 ms /    21 runs   (    6.88 ms per token,   145.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     803.40 ms /  1326 tokens (    0.61 ms per token,  1650.48 tokens per second)\n",
      "llama_print_timings:        eval time =     400.80 ms /    20 runs   (   20.04 ms per token,    49.90 tokens per second)\n",
      "llama_print_timings:       total time =    1415.00 ms /  1346 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.74 ms /    21 runs   (    6.84 ms per token,   146.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     804.48 ms /  1329 tokens (    0.61 ms per token,  1652.00 tokens per second)\n",
      "llama_print_timings:        eval time =     400.57 ms /    20 runs   (   20.03 ms per token,    49.93 tokens per second)\n",
      "llama_print_timings:       total time =    1414.89 ms /  1349 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.46 ms /    21 runs   (    6.83 ms per token,   146.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     803.99 ms /  1328 tokens (    0.61 ms per token,  1651.77 tokens per second)\n",
      "llama_print_timings:        eval time =     400.77 ms /    20 runs   (   20.04 ms per token,    49.90 tokens per second)\n",
      "llama_print_timings:       total time =    1414.03 ms /  1348 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.72 ms /    21 runs   (    6.84 ms per token,   146.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     803.65 ms /  1327 tokens (    0.61 ms per token,  1651.22 tokens per second)\n",
      "llama_print_timings:        eval time =     401.13 ms /    20 runs   (   20.06 ms per token,    49.86 tokens per second)\n",
      "llama_print_timings:       total time =    1414.86 ms /  1347 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     214.50 ms /    31 runs   (    6.92 ms per token,   144.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.83 ms /   315 tokens (    0.68 ms per token,  1473.17 tokens per second)\n",
      "llama_print_timings:        eval time =     556.76 ms /    30 runs   (   18.56 ms per token,    53.88 tokens per second)\n",
      "llama_print_timings:       total time =    1073.36 ms /   345 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     210.96 ms /    31 runs   (    6.81 ms per token,   146.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.70 ms /   315 tokens (    0.68 ms per token,  1474.06 tokens per second)\n",
      "llama_print_timings:        eval time =     556.49 ms /    30 runs   (   18.55 ms per token,    53.91 tokens per second)\n",
      "llama_print_timings:       total time =    1068.51 ms /   345 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =    1505.58 ms /   218 runs   (    6.91 ms per token,   144.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     752.87 ms /  1268 tokens (    0.59 ms per token,  1684.23 tokens per second)\n",
      "llama_print_timings:        eval time =    4356.98 ms /   217 runs   (   20.08 ms per token,    49.81 tokens per second)\n",
      "llama_print_timings:       total time =    7277.08 ms /  1485 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     164.56 ms /    24 runs   (    6.86 ms per token,   145.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.61 ms /   206 tokens (    0.79 ms per token,  1266.85 tokens per second)\n",
      "llama_print_timings:        eval time =     422.24 ms /    23 runs   (   18.36 ms per token,    54.47 tokens per second)\n",
      "llama_print_timings:       total time =     817.99 ms /   229 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     207.77 ms /    30 runs   (    6.93 ms per token,   144.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.28 ms /   216 tokens (    0.77 ms per token,  1306.85 tokens per second)\n",
      "llama_print_timings:        eval time =     532.79 ms /    29 runs   (   18.37 ms per token,    54.43 tokens per second)\n",
      "llama_print_timings:       total time =     991.99 ms /   245 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.81 ms /    22 runs   (    6.95 ms per token,   143.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.51 ms /   218 tokens (    0.76 ms per token,  1317.15 tokens per second)\n",
      "llama_print_timings:        eval time =     385.73 ms /    21 runs   (   18.37 ms per token,    54.44 tokens per second)\n",
      "llama_print_timings:       total time =     766.44 ms /   239 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     176.94 ms /    26 runs   (    6.81 ms per token,   146.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.78 ms /   250 tokens (    0.66 ms per token,  1508.02 tokens per second)\n",
      "llama_print_timings:        eval time =     462.34 ms /    25 runs   (   18.49 ms per token,    54.07 tokens per second)\n",
      "llama_print_timings:       total time =     879.23 ms /   275 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     272.24 ms /    41 runs   (    6.64 ms per token,   150.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     782.38 ms /  1055 tokens (    0.74 ms per token,  1348.46 tokens per second)\n",
      "llama_print_timings:        eval time =     785.12 ms /    40 runs   (   19.63 ms per token,    50.95 tokens per second)\n",
      "llama_print_timings:       total time =    1963.14 ms /  1095 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     214.72 ms /    31 runs   (    6.93 ms per token,   144.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     756.95 ms /  1050 tokens (    0.72 ms per token,  1387.14 tokens per second)\n",
      "llama_print_timings:        eval time =     588.85 ms /    30 runs   (   19.63 ms per token,    50.95 tokens per second)\n",
      "llama_print_timings:       total time =    1655.61 ms /  1080 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     201.45 ms /    29 runs   (    6.95 ms per token,   143.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.94 ms /   255 tokens (    0.65 ms per token,  1527.53 tokens per second)\n",
      "llama_print_timings:        eval time =     518.05 ms /    28 runs   (   18.50 ms per token,    54.05 tokens per second)\n",
      "llama_print_timings:       total time =     970.01 ms /   283 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     276.36 ms /    40 runs   (    6.91 ms per token,   144.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.09 ms /   251 tokens (    0.66 ms per token,  1511.22 tokens per second)\n",
      "llama_print_timings:        eval time =     721.51 ms /    39 runs   (   18.50 ms per token,    54.05 tokens per second)\n",
      "llama_print_timings:       total time =    1277.22 ms /   290 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     375.25 ms /    55 runs   (    6.82 ms per token,   146.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.64 ms /   257 tokens (    0.78 ms per token,  1287.32 tokens per second)\n",
      "llama_print_timings:        eval time =     998.79 ms /    54 runs   (   18.50 ms per token,    54.07 tokens per second)\n",
      "llama_print_timings:       total time =    1729.34 ms /   311 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     220.63 ms /    32 runs   (    6.89 ms per token,   145.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     782.01 ms /  1056 tokens (    0.74 ms per token,  1350.37 tokens per second)\n",
      "llama_print_timings:        eval time =     608.23 ms /    31 runs   (   19.62 ms per token,    50.97 tokens per second)\n",
      "llama_print_timings:       total time =    1706.83 ms /  1087 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     217.69 ms /    33 runs   (    6.60 ms per token,   151.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     706.72 ms /  1043 tokens (    0.68 ms per token,  1475.83 tokens per second)\n",
      "llama_print_timings:        eval time =     627.58 ms /    32 runs   (   19.61 ms per token,    50.99 tokens per second)\n",
      "llama_print_timings:       total time =    1650.94 ms /  1075 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     144.00 ms /    21 runs   (    6.86 ms per token,   145.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.94 ms /   630 tokens (    0.65 ms per token,  1544.34 tokens per second)\n",
      "llama_print_timings:        eval time =     380.93 ms /    20 runs   (   19.05 ms per token,    52.50 tokens per second)\n",
      "llama_print_timings:       total time =     994.67 ms /   650 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     144.04 ms /    21 runs   (    6.86 ms per token,   145.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     408.74 ms /   631 tokens (    0.65 ms per token,  1543.77 tokens per second)\n",
      "llama_print_timings:        eval time =     380.69 ms /    20 runs   (   19.03 ms per token,    52.54 tokens per second)\n",
      "llama_print_timings:       total time =     995.38 ms /   651 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     144.13 ms /    21 runs   (    6.86 ms per token,   145.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.43 ms /   629 tokens (    0.65 ms per token,  1543.81 tokens per second)\n",
      "llama_print_timings:        eval time =     380.87 ms /    20 runs   (   19.04 ms per token,    52.51 tokens per second)\n",
      "llama_print_timings:       total time =     994.24 ms /   649 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.82 ms /    21 runs   (    6.85 ms per token,   146.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     544.91 ms /   937 tokens (    0.58 ms per token,  1719.55 tokens per second)\n",
      "llama_print_timings:        eval time =     389.98 ms /    20 runs   (   19.50 ms per token,    51.28 tokens per second)\n",
      "llama_print_timings:       total time =    1141.17 ms /   957 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.80 ms /    21 runs   (    6.85 ms per token,   146.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.95 ms /   630 tokens (    0.65 ms per token,  1544.29 tokens per second)\n",
      "llama_print_timings:        eval time =     380.68 ms /    20 runs   (   19.03 ms per token,    52.54 tokens per second)\n",
      "llama_print_timings:       total time =     994.85 ms /   650 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     143.75 ms /    21 runs   (    6.85 ms per token,   146.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     543.71 ms /   934 tokens (    0.58 ms per token,  1717.83 tokens per second)\n",
      "llama_print_timings:        eval time =     389.83 ms /    20 runs   (   19.49 ms per token,    51.30 tokens per second)\n",
      "llama_print_timings:       total time =    1140.34 ms /   954 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     145.40 ms /    21 runs   (    6.92 ms per token,   144.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.35 ms /   628 tokens (    0.65 ms per token,  1541.69 tokens per second)\n",
      "llama_print_timings:        eval time =     380.53 ms /    20 runs   (   19.03 ms per token,    52.56 tokens per second)\n",
      "llama_print_timings:       total time =     996.07 ms /   648 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     144.19 ms /    21 runs   (    6.87 ms per token,   145.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.23 ms /   628 tokens (    0.65 ms per token,  1542.14 tokens per second)\n",
      "llama_print_timings:        eval time =     380.61 ms /    20 runs   (   19.03 ms per token,    52.55 tokens per second)\n",
      "llama_print_timings:       total time =     993.95 ms /   648 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     150.69 ms /    22 runs   (    6.85 ms per token,   145.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     451.79 ms /   753 tokens (    0.60 ms per token,  1666.71 tokens per second)\n",
      "llama_print_timings:        eval time =     403.40 ms /    21 runs   (   19.21 ms per token,    52.06 tokens per second)\n",
      "llama_print_timings:       total time =    1071.26 ms /   774 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     156.64 ms /    23 runs   (    6.81 ms per token,   146.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     203.69 ms /   275 tokens (    0.74 ms per token,  1350.08 tokens per second)\n",
      "llama_print_timings:        eval time =     407.36 ms /    22 runs   (   18.52 ms per token,    54.01 tokens per second)\n",
      "llama_print_timings:       total time =     833.50 ms /   297 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     205.11 ms /    30 runs   (    6.84 ms per token,   146.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     452.63 ms /   755 tokens (    0.60 ms per token,  1668.01 tokens per second)\n",
      "llama_print_timings:        eval time =     557.95 ms /    29 runs   (   19.24 ms per token,    51.98 tokens per second)\n",
      "llama_print_timings:       total time =    1304.36 ms /   784 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     206.03 ms /    30 runs   (    6.87 ms per token,   145.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     452.26 ms /   754 tokens (    0.60 ms per token,  1667.20 tokens per second)\n",
      "llama_print_timings:        eval time =     557.75 ms /    29 runs   (   19.23 ms per token,    51.99 tokens per second)\n",
      "llama_print_timings:       total time =    1303.62 ms /   783 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     197.77 ms /    29 runs   (    6.82 ms per token,   146.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     451.60 ms /   751 tokens (    0.60 ms per token,  1662.98 tokens per second)\n",
      "llama_print_timings:        eval time =     538.15 ms /    28 runs   (   19.22 ms per token,    52.03 tokens per second)\n",
      "llama_print_timings:       total time =    1271.31 ms /   779 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     153.09 ms /    22 runs   (    6.96 ms per token,   143.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     203.93 ms /   275 tokens (    0.74 ms per token,  1348.54 tokens per second)\n",
      "llama_print_timings:        eval time =     389.09 ms /    21 runs   (   18.53 ms per token,    53.97 tokens per second)\n",
      "llama_print_timings:       total time =     809.26 ms /   296 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     151.12 ms /    22 runs   (    6.87 ms per token,   145.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     595.95 ms /  1027 tokens (    0.58 ms per token,  1723.29 tokens per second)\n",
      "llama_print_timings:        eval time =     411.48 ms /    21 runs   (   19.59 ms per token,    51.04 tokens per second)\n",
      "llama_print_timings:       total time =    1226.64 ms /  1048 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     198.49 ms /    29 runs   (    6.84 ms per token,   146.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     450.56 ms /   747 tokens (    0.60 ms per token,  1657.94 tokens per second)\n",
      "llama_print_timings:        eval time =     538.14 ms /    28 runs   (   19.22 ms per token,    52.03 tokens per second)\n",
      "llama_print_timings:       total time =    1272.43 ms /   775 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.44 ms /    22 runs   (    6.93 ms per token,   144.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.24 ms /   179 tokens (    0.84 ms per token,  1183.57 tokens per second)\n",
      "llama_print_timings:        eval time =     385.07 ms /    21 runs   (   18.34 ms per token,    54.54 tokens per second)\n",
      "llama_print_timings:       total time =     751.88 ms /   200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     157.98 ms /    23 runs   (    6.87 ms per token,   145.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.43 ms /   102 tokens (    1.19 ms per token,   840.01 tokens per second)\n",
      "llama_print_timings:        eval time =     401.54 ms /    22 runs   (   18.25 ms per token,    54.79 tokens per second)\n",
      "llama_print_timings:       total time =     745.85 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     157.64 ms /    23 runs   (    6.85 ms per token,   145.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.77 ms /    94 tokens (    1.27 ms per token,   784.81 tokens per second)\n",
      "llama_print_timings:        eval time =     401.30 ms /    22 runs   (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:       total time =     744.07 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     158.11 ms /    23 runs   (    6.87 ms per token,   145.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.14 ms /   102 tokens (    1.19 ms per token,   842.02 tokens per second)\n",
      "llama_print_timings:        eval time =     401.57 ms /    22 runs   (   18.25 ms per token,    54.78 tokens per second)\n",
      "llama_print_timings:       total time =     746.12 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.27 ms /    22 runs   (    6.92 ms per token,   144.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.22 ms /   199 tokens (    0.81 ms per token,  1234.35 tokens per second)\n",
      "llama_print_timings:        eval time =     385.36 ms /    21 runs   (   18.35 ms per token,    54.49 tokens per second)\n",
      "llama_print_timings:       total time =     761.84 ms /   220 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     153.34 ms /    22 runs   (    6.97 ms per token,   143.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.97 ms /   199 tokens (    0.81 ms per token,  1236.24 tokens per second)\n",
      "llama_print_timings:        eval time =     385.42 ms /    21 runs   (   18.35 ms per token,    54.49 tokens per second)\n",
      "llama_print_timings:       total time =     762.42 ms /   220 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     204.84 ms /    30 runs   (    6.83 ms per token,   146.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.85 ms /    95 tokens (    1.26 ms per token,   792.66 tokens per second)\n",
      "llama_print_timings:        eval time =     529.28 ms /    29 runs   (   18.25 ms per token,    54.79 tokens per second)\n",
      "llama_print_timings:       total time =     938.02 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     157.97 ms /    23 runs   (    6.87 ms per token,   145.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.60 ms /    90 tokens (    1.32 ms per token,   758.83 tokens per second)\n",
      "llama_print_timings:        eval time =     401.16 ms /    22 runs   (   18.23 ms per token,    54.84 tokens per second)\n",
      "llama_print_timings:       total time =     743.66 ms /   112 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     225.46 ms /    33 runs   (    6.83 ms per token,   146.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.94 ms /    96 tokens (    1.25 ms per token,   800.39 tokens per second)\n",
      "llama_print_timings:        eval time =     583.75 ms /    32 runs   (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:       total time =    1022.79 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.63 ms /    22 runs   (    6.94 ms per token,   144.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.09 ms /   200 tokens (    0.81 ms per token,  1241.57 tokens per second)\n",
      "llama_print_timings:        eval time =     385.69 ms /    21 runs   (   18.37 ms per token,    54.45 tokens per second)\n",
      "llama_print_timings:       total time =     761.44 ms /   221 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     157.73 ms /    23 runs   (    6.86 ms per token,   145.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.45 ms /    93 tokens (    1.28 ms per token,   778.55 tokens per second)\n",
      "llama_print_timings:        eval time =     401.21 ms /    22 runs   (   18.24 ms per token,    54.83 tokens per second)\n",
      "llama_print_timings:       total time =     743.19 ms /   115 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     193.37 ms /    28 runs   (    6.91 ms per token,   144.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.91 ms /    96 tokens (    1.25 ms per token,   800.59 tokens per second)\n",
      "llama_print_timings:        eval time =     492.47 ms /    27 runs   (   18.24 ms per token,    54.83 tokens per second)\n",
      "llama_print_timings:       total time =     885.72 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =      86.72 ms /    13 runs   (    6.67 ms per token,   149.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.89 ms /    96 tokens (    1.25 ms per token,   800.73 tokens per second)\n",
      "llama_print_timings:        eval time =     218.80 ms /    12 runs   (   18.23 ms per token,    54.85 tokens per second)\n",
      "llama_print_timings:       total time =     462.39 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     158.19 ms /    23 runs   (    6.88 ms per token,   145.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.30 ms /    93 tokens (    1.28 ms per token,   779.55 tokens per second)\n",
      "llama_print_timings:        eval time =     401.14 ms /    22 runs   (   18.23 ms per token,    54.84 tokens per second)\n",
      "llama_print_timings:       total time =     744.52 ms /   115 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.51 ms /    22 runs   (    6.93 ms per token,   144.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.29 ms /    93 tokens (    1.28 ms per token,   779.62 tokens per second)\n",
      "llama_print_timings:        eval time =     382.74 ms /    21 runs   (   18.23 ms per token,    54.87 tokens per second)\n",
      "llama_print_timings:       total time =     717.47 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.20 ms /    22 runs   (    6.92 ms per token,   144.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.82 ms /    95 tokens (    1.26 ms per token,   792.86 tokens per second)\n",
      "llama_print_timings:        eval time =     383.07 ms /    21 runs   (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:       total time =     717.67 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     153.03 ms /    22 runs   (    6.96 ms per token,   143.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.63 ms /   183 tokens (    0.83 ms per token,  1198.99 tokens per second)\n",
      "llama_print_timings:        eval time =     385.12 ms /    21 runs   (   18.34 ms per token,    54.53 tokens per second)\n",
      "llama_print_timings:       total time =     754.05 ms /   204 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.83 ms /    22 runs   (    6.95 ms per token,   143.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.99 ms /   181 tokens (    0.84 ms per token,  1190.88 tokens per second)\n",
      "llama_print_timings:        eval time =     385.36 ms /    21 runs   (   18.35 ms per token,    54.49 tokens per second)\n",
      "llama_print_timings:       total time =     753.15 ms /   202 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.38 ms /    22 runs   (    6.93 ms per token,   144.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.88 ms /   182 tokens (    0.83 ms per token,  1198.35 tokens per second)\n",
      "llama_print_timings:        eval time =     384.98 ms /    21 runs   (   18.33 ms per token,    54.55 tokens per second)\n",
      "llama_print_timings:       total time =     752.12 ms /   203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     152.08 ms /    22 runs   (    6.91 ms per token,   144.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.41 ms /   201 tokens (    0.80 ms per token,  1245.29 tokens per second)\n",
      "llama_print_timings:        eval time =     385.56 ms /    21 runs   (   18.36 ms per token,    54.47 tokens per second)\n",
      "llama_print_timings:       total time =     762.09 ms /   222 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     235.74 ms /    34 runs   (    6.93 ms per token,   144.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.32 ms /   196 tokens (    0.82 ms per token,  1222.55 tokens per second)\n",
      "llama_print_timings:        eval time =     605.73 ms /    33 runs   (   18.36 ms per token,    54.48 tokens per second)\n",
      "llama_print_timings:       total time =    1097.36 ms /   229 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     157.75 ms /    23 runs   (    6.86 ms per token,   145.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.16 ms /    97 tokens (    1.24 ms per token,   807.24 tokens per second)\n",
      "llama_print_timings:        eval time =     401.24 ms /    22 runs   (   18.24 ms per token,    54.83 tokens per second)\n",
      "llama_print_timings:       total time =     744.74 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     157.84 ms /    23 runs   (    6.86 ms per token,   145.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.36 ms /    89 tokens (    1.33 ms per token,   751.92 tokens per second)\n",
      "llama_print_timings:        eval time =     401.25 ms /    22 runs   (   18.24 ms per token,    54.83 tokens per second)\n",
      "llama_print_timings:       total time =     743.37 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     229.47 ms /    33 runs   (    6.95 ms per token,   143.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.40 ms /   190 tokens (    0.81 ms per token,  1230.59 tokens per second)\n",
      "llama_print_timings:        eval time =     587.31 ms /    32 runs   (   18.35 ms per token,    54.49 tokens per second)\n",
      "llama_print_timings:       total time =    1065.73 ms /   222 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     255.08 ms /    37 runs   (    6.89 ms per token,   145.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.30 ms /   318 tokens (    0.67 ms per token,  1483.87 tokens per second)\n",
      "llama_print_timings:        eval time =     667.98 ms /    36 runs   (   18.55 ms per token,    53.89 tokens per second)\n",
      "llama_print_timings:       total time =    1242.59 ms /   354 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     227.88 ms /    33 runs   (    6.91 ms per token,   144.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     212.97 ms /   312 tokens (    0.68 ms per token,  1464.98 tokens per second)\n",
      "llama_print_timings:        eval time =     593.50 ms /    32 runs   (   18.55 ms per token,    53.92 tokens per second)\n",
      "llama_print_timings:       total time =    1127.86 ms /   344 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     255.06 ms /    37 runs   (    6.89 ms per token,   145.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.96 ms /   320 tokens (    0.67 ms per token,  1488.66 tokens per second)\n",
      "llama_print_timings:        eval time =     668.05 ms /    36 runs   (   18.56 ms per token,    53.89 tokens per second)\n",
      "llama_print_timings:       total time =    1243.73 ms /   356 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     228.68 ms /    33 runs   (    6.93 ms per token,   144.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.40 ms /   319 tokens (    0.67 ms per token,  1487.90 tokens per second)\n",
      "llama_print_timings:        eval time =     593.52 ms /    32 runs   (   18.55 ms per token,    53.92 tokens per second)\n",
      "llama_print_timings:       total time =    1129.65 ms /   351 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     256.87 ms /    37 runs   (    6.94 ms per token,   144.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.86 ms /   321 tokens (    0.67 ms per token,  1487.07 tokens per second)\n",
      "llama_print_timings:        eval time =     668.21 ms /    36 runs   (   18.56 ms per token,    53.88 tokens per second)\n",
      "llama_print_timings:       total time =    1247.00 ms /   357 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     227.10 ms /    33 runs   (    6.88 ms per token,   145.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     212.95 ms /   311 tokens (    0.68 ms per token,  1460.44 tokens per second)\n",
      "llama_print_timings:        eval time =     593.46 ms /    32 runs   (   18.55 ms per token,    53.92 tokens per second)\n",
      "llama_print_timings:       total time =    1126.94 ms /   343 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     249.29 ms /    36 runs   (    6.92 ms per token,   144.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.31 ms /   316 tokens (    0.68 ms per token,  1474.51 tokens per second)\n",
      "llama_print_timings:        eval time =     649.37 ms /    35 runs   (   18.55 ms per token,    53.90 tokens per second)\n",
      "llama_print_timings:       total time =    1214.39 ms /   351 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     228.25 ms /    33 runs   (    6.92 ms per token,   144.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.57 ms /   319 tokens (    0.67 ms per token,  1486.68 tokens per second)\n",
      "llama_print_timings:        eval time =     593.48 ms /    32 runs   (   18.55 ms per token,    53.92 tokens per second)\n",
      "llama_print_timings:       total time =    1129.51 ms /   351 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     160.33 ms /    23 runs   (    6.97 ms per token,   143.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     210.05 ms /   300 tokens (    0.70 ms per token,  1428.23 tokens per second)\n",
      "llama_print_timings:        eval time =     408.17 ms /    22 runs   (   18.55 ms per token,    53.90 tokens per second)\n",
      "llama_print_timings:       total time =     845.25 ms /   322 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     158.68 ms /    23 runs   (    6.90 ms per token,   144.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.79 ms /   301 tokens (    0.70 ms per token,  1434.74 tokens per second)\n",
      "llama_print_timings:        eval time =     407.81 ms /    22 runs   (   18.54 ms per token,    53.95 tokens per second)\n",
      "llama_print_timings:       total time =     841.35 ms /   323 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     185.36 ms /    27 runs   (    6.87 ms per token,   145.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.21 ms /   127 tokens (    0.99 ms per token,  1006.23 tokens per second)\n",
      "llama_print_timings:        eval time =     475.51 ms /    26 runs   (   18.29 ms per token,    54.68 tokens per second)\n",
      "llama_print_timings:       total time =     863.93 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     158.53 ms /    23 runs   (    6.89 ms per token,   145.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.36 ms /   299 tokens (    0.70 ms per token,  1428.16 tokens per second)\n",
      "llama_print_timings:        eval time =     407.78 ms /    22 runs   (   18.54 ms per token,    53.95 tokens per second)\n",
      "llama_print_timings:       total time =     841.76 ms /   321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     185.83 ms /    27 runs   (    6.88 ms per token,   145.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.49 ms /   123 tokens (    1.02 ms per token,   980.17 tokens per second)\n",
      "llama_print_timings:        eval time =     475.42 ms /    26 runs   (   18.29 ms per token,    54.69 tokens per second)\n",
      "llama_print_timings:       total time =     862.82 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     187.38 ms /    27 runs   (    6.94 ms per token,   144.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.75 ms /   124 tokens (    1.01 ms per token,   986.07 tokens per second)\n",
      "llama_print_timings:        eval time =     475.61 ms /    26 runs   (   18.29 ms per token,    54.67 tokens per second)\n",
      "llama_print_timings:       total time =     864.88 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     186.69 ms /    27 runs   (    6.91 ms per token,   144.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.86 ms /   125 tokens (    1.01 ms per token,   993.17 tokens per second)\n",
      "llama_print_timings:        eval time =     475.45 ms /    26 runs   (   18.29 ms per token,    54.68 tokens per second)\n",
      "llama_print_timings:       total time =     864.19 ms /   151 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     159.84 ms /    23 runs   (    6.95 ms per token,   143.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.96 ms /   236 tokens (    0.69 ms per token,  1457.12 tokens per second)\n",
      "llama_print_timings:        eval time =     405.67 ms /    22 runs   (   18.44 ms per token,    54.23 tokens per second)\n",
      "llama_print_timings:       total time =     792.94 ms /   258 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     158.36 ms /    23 runs   (    6.89 ms per token,   145.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.08 ms /   261 tokens (    0.77 ms per token,  1298.02 tokens per second)\n",
      "llama_print_timings:        eval time =     407.87 ms /    22 runs   (   18.54 ms per token,    53.94 tokens per second)\n",
      "llama_print_timings:       total time =     833.24 ms /   283 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     159.24 ms /    23 runs   (    6.92 ms per token,   144.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     207.98 ms /   292 tokens (    0.71 ms per token,  1403.99 tokens per second)\n",
      "llama_print_timings:        eval time =     407.77 ms /    22 runs   (   18.54 ms per token,    53.95 tokens per second)\n",
      "llama_print_timings:       total time =     839.86 ms /   314 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     158.90 ms /    23 runs   (    6.91 ms per token,   144.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     207.80 ms /   292 tokens (    0.71 ms per token,  1405.21 tokens per second)\n",
      "llama_print_timings:        eval time =     407.77 ms /    22 runs   (   18.54 ms per token,    53.95 tokens per second)\n",
      "llama_print_timings:       total time =     839.66 ms /   314 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     159.43 ms /    23 runs   (    6.93 ms per token,   144.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.31 ms /   234 tokens (    0.69 ms per token,  1450.67 tokens per second)\n",
      "llama_print_timings:        eval time =     405.38 ms /    22 runs   (   18.43 ms per token,    54.27 tokens per second)\n",
      "llama_print_timings:       total time =     791.67 ms /   256 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     185.82 ms /    27 runs   (    6.88 ms per token,   145.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.69 ms /   119 tokens (    1.05 ms per token,   954.34 tokens per second)\n",
      "llama_print_timings:        eval time =     475.38 ms /    26 runs   (   18.28 ms per token,    54.69 tokens per second)\n",
      "llama_print_timings:       total time =     862.42 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     185.12 ms /    27 runs   (    6.86 ms per token,   145.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.45 ms /   127 tokens (    1.00 ms per token,  1004.39 tokens per second)\n",
      "llama_print_timings:        eval time =     475.13 ms /    26 runs   (   18.27 ms per token,    54.72 tokens per second)\n",
      "llama_print_timings:       total time =     862.13 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     159.75 ms /    23 runs   (    6.95 ms per token,   143.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.96 ms /   228 tokens (    0.70 ms per token,  1425.33 tokens per second)\n",
      "llama_print_timings:        eval time =     405.75 ms /    22 runs   (   18.44 ms per token,    54.22 tokens per second)\n",
      "llama_print_timings:       total time =     791.61 ms /   250 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     185.84 ms /    27 runs   (    6.88 ms per token,   145.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.77 ms /   124 tokens (    1.01 ms per token,   985.97 tokens per second)\n",
      "llama_print_timings:        eval time =     475.16 ms /    26 runs   (   18.28 ms per token,    54.72 tokens per second)\n",
      "llama_print_timings:       total time =     863.64 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     208.55 ms /    30 runs   (    6.95 ms per token,   143.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.97 ms /   125 tokens (    1.01 ms per token,   992.28 tokens per second)\n",
      "llama_print_timings:        eval time =     530.49 ms /    29 runs   (   18.29 ms per token,    54.67 tokens per second)\n",
      "llama_print_timings:       total time =     950.27 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     203.26 ms /    28 runs   (    7.26 ms per token,   137.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.04 ms /   129 tokens (    1.08 ms per token,   927.78 tokens per second)\n",
      "llama_print_timings:        eval time =     493.79 ms /    27 runs   (   18.29 ms per token,    54.68 tokens per second)\n",
      "llama_print_timings:       total time =     916.42 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     208.25 ms /    30 runs   (    6.94 ms per token,   144.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.24 ms /   127 tokens (    0.99 ms per token,  1006.03 tokens per second)\n",
      "llama_print_timings:        eval time =     530.63 ms /    29 runs   (   18.30 ms per token,    54.65 tokens per second)\n",
      "llama_print_timings:       total time =     950.96 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     228.09 ms /    33 runs   (    6.91 ms per token,   144.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.88 ms /   132 tokens (    1.06 ms per token,   943.63 tokens per second)\n",
      "llama_print_timings:        eval time =     585.50 ms /    32 runs   (   18.30 ms per token,    54.65 tokens per second)\n",
      "llama_print_timings:       total time =    1046.60 ms /   164 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     228.62 ms /    33 runs   (    6.93 ms per token,   144.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.40 ms /   130 tokens (    1.07 ms per token,   932.57 tokens per second)\n",
      "llama_print_timings:        eval time =     585.27 ms /    32 runs   (   18.29 ms per token,    54.68 tokens per second)\n",
      "llama_print_timings:       total time =    1046.14 ms /   162 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.52 ms\n",
      "llama_print_timings:      sample time =     229.30 ms /    33 runs   (    6.95 ms per token,   143.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.13 ms /   133 tokens (    1.05 ms per token,   949.11 tokens per second)\n",
      "llama_print_timings:        eval time =     585.55 ms /    32 runs   (   18.30 ms per token,    54.65 tokens per second)\n",
      "llama_print_timings:       total time =    1047.86 ms /   165 tokens\n"
     ]
    }
   ],
   "source": [
    "limit_rows = -1 # 20\n",
    "selected_devices = None # ['Smoke', 'Humidity']\n",
    "\n",
    "df = pd.read_csv('data/datasets/dataset_v2.csv')\n",
    "devices = list(df['device'].unique())\n",
    "if selected_devices:\n",
    "    df = df[df['device'].isin(selected_devices)].sort_index()\n",
    "    # print(df)\n",
    "\n",
    "output_df = pd.DataFrame(columns=['id', 'device', 'user_cmd', 'mtd', 'json_cmd'])\n",
    "for i, row in df.iterrows():\n",
    "    # if i < 100: # or i > 16:\n",
    "    #     continue\n",
    "    user_cmd = row['user_cmd']\n",
    "\n",
    "    device = row['device']\n",
    "    sample_devices = devices.copy()\n",
    "    sample_devices.remove(device)\n",
    "    sample_devices = random.sample(sample_devices, k=2)\n",
    "    env = f'{sample_devices[0]} id=1, {sample_devices[1]} id=2, {device} id=444'\n",
    "\n",
    "    retrieval_prompt = \"Represent this sentence for searching relevant passages: \" + user_cmd\n",
    "    retrieved_nodes = retriever.retrieve(retrieval_prompt)\n",
    "    methods_description = f'API method 1:\\n{retrieved_nodes[0].text}'\n",
    "    method_name = retrieved_nodes[0].metadata['file_name'].replace('.md', '')\n",
    "\n",
    "    user_prompt = user_prompt_template.format(**{'env': env, \n",
    "                                                 'methods_description': methods_description, \n",
    "                                                 'user_cmd': user_cmd})\n",
    "    llm_prompt = base_prompt + '\\n\\n' + user_prompt\n",
    "\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': llm_prompt}\n",
    "        ],\n",
    "        grammar=llama_grammar\n",
    "    )\n",
    "    response_text = response['choices'][0]['message']['content']\n",
    "    \n",
    "    response_text = response_text.replace('\\_', '_')\n",
    "    try:\n",
    "        json_cmd = json.dumps(json.loads(response_text))\n",
    "    except Exception as ex:\n",
    "        print(response_text)\n",
    "        print(ex)\n",
    "        continue\n",
    "\n",
    "    output_df.loc[len(output_df)] = pd.Series({'id': row['id'], 'device': row['device'], 'user_cmd': user_cmd, 'mtd': method_name, 'json_cmd': json_cmd})\n",
    "\n",
    "    # print(user_cmd)\n",
    "    # print(response_text)\n",
    "\n",
    "    if limit_rows > 0 and i == limit_rows - 1:\n",
    "        break\n",
    "\n",
    "output_num = list(sorted([int(d.name.replace('output', '')) for d in list(OUTPUT_DIR.iterdir())]))[-1] + 1\n",
    "CUR_OUTPUT_DIR = OUTPUT_DIR / f'output{output_num}'\n",
    "CUR_OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "output_df.to_csv(CUR_OUTPUT_DIR / 'output.csv', index=False, header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>device</th>\n",
       "      <th>user_cmd</th>\n",
       "      <th>mtd</th>\n",
       "      <th>json_cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>192</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>Could you lower the temperature to 18 degrees ...</td>\n",
       "      <td>Temperature.SetTemperature</td>\n",
       "      <td>{\"method\": \"Temperature.SetTemperature\", \"para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>193</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>Set the temperature to 20 degrees in the bedroom.</td>\n",
       "      <td>Temperature.SetTemperature</td>\n",
       "      <td>{\"method\": \"Temperature.SetTemperature\", \"para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>194</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>Can you increase the humidity in the living ro...</td>\n",
       "      <td>Humidity.SetHumidity</td>\n",
       "      <td>{\"method\": \"Humidity.SetHumidity\", \"params\": {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>Set the humidity to 50% in the bedroom.</td>\n",
       "      <td>Humidity.SetHumidity</td>\n",
       "      <td>{\"method\": \"Humidity.SetHumidity\", \"params\": {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>Decrease the humidity in the kitchen to 40 per...</td>\n",
       "      <td>Humidity.SetHumidity</td>\n",
       "      <td>{\"method\": \"Humidity.SetHumidity\", \"params\": {...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       device                                           user_cmd  \\\n",
       "192  192  Temperature  Could you lower the temperature to 18 degrees ...   \n",
       "193  193  Temperature  Set the temperature to 20 degrees in the bedroom.   \n",
       "194  194     Humidity  Can you increase the humidity in the living ro...   \n",
       "195  195     Humidity            Set the humidity to 50% in the bedroom.   \n",
       "196  196     Humidity  Decrease the humidity in the kitchen to 40 per...   \n",
       "\n",
       "                            mtd  \\\n",
       "192  Temperature.SetTemperature   \n",
       "193  Temperature.SetTemperature   \n",
       "194        Humidity.SetHumidity   \n",
       "195        Humidity.SetHumidity   \n",
       "196        Humidity.SetHumidity   \n",
       "\n",
       "                                              json_cmd  \n",
       "192  {\"method\": \"Temperature.SetTemperature\", \"para...  \n",
       "193  {\"method\": \"Temperature.SetTemperature\", \"para...  \n",
       "194  {\"method\": \"Humidity.SetHumidity\", \"params\": {...  \n",
       "195  {\"method\": \"Humidity.SetHumidity\", \"params\": {...  \n",
       "196  {\"method\": \"Humidity.SetHumidity\", \"params\": {...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/datasets/dataset_v2.csv')\n",
    "output_df = pd.read_csv(CUR_OUTPUT_DIR / 'output.csv')\n",
    "merged_df = df.merge(output_df, how='inner', on='id', suffixes=(\"_gt\", \"_pred\"))\n",
    "\n",
    "incorrect_output_df = pd.DataFrame(columns=['device', 'user_cmd', 'gt_mtd', 'pred_mtd', 'gt_json_cmd', 'pred_json_cmd'])\n",
    "correct_output_df = pd.DataFrame(columns=['device', 'user_cmd', 'gt_mtd', 'pred_mtd', 'gt_json_cmd', 'pred_json_cmd'])\n",
    "correct_methods = 0\n",
    "correct_json_cmds = 0\n",
    "for _, row in merged_df.iterrows():\n",
    "    correct_ouput = True\n",
    "    if row['mtd_gt'] == row['mtd_pred']:\n",
    "        correct_methods += 1\n",
    "    else:\n",
    "        correct_ouput = False\n",
    "    try:\n",
    "        if json.loads(row['json_cmd_gt']) == json.loads(row['json_cmd_pred']):\n",
    "            correct_json_cmds += 1\n",
    "        else:\n",
    "            correct_ouput = False\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(row['id'])\n",
    "        print(row['json_cmd_gt'])\n",
    "        print(row['json_cmd_pred'])\n",
    "\n",
    "    if not correct_ouput:\n",
    "        incorrect_output_df.loc[len(incorrect_output_df)] = pd.Series({'device': row['device_gt'],\n",
    "        'user_cmd': row['user_cmd_gt'], 'gt_mtd': row['mtd_gt'],\n",
    "        'pred_mtd': row['mtd_pred'], 'gt_json_cmd': row['json_cmd_gt'], 'pred_json_cmd': row['json_cmd_pred']})\n",
    "    else:\n",
    "        correct_output_df.loc[len(correct_output_df)] = pd.Series({'device': row['device_gt'],\n",
    "        'user_cmd': row['user_cmd_gt'], 'gt_mtd': row['mtd_gt'],\n",
    "        'pred_mtd': row['mtd_pred'], 'gt_json_cmd': row['json_cmd_gt'], 'pred_json_cmd': row['json_cmd_pred']})\n",
    "acc_methods = round(correct_methods / len(merged_df), 2)\n",
    "acc_json_cmds = round(correct_json_cmds / len(merged_df), 2)\n",
    "\n",
    "incorrect_output_df.to_csv(CUR_OUTPUT_DIR / 'incorrect_output.csv', index=False)\n",
    "correct_output_df.to_csv(CUR_OUTPUT_DIR / 'correct_output.csv', index=False)\n",
    "\n",
    "with open(CUR_OUTPUT_DIR / 'results.txt', 'a') as f:\n",
    "    f.write(f'Acc of methods: {acc_methods}\\n'\n",
    "            f'Acc of json cmds: {acc_json_cmds}\\n\\n'\n",
    "            f'-----------------------------------\\n\\n')\n",
    "\n",
    "print(len(df))\n",
    "print(len(merged_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
