{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.embeddings.utils import resolve_embed_model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser.text.sentence import SentenceSplitter\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp import LlamaGrammar\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_METHODS_DIR = Path('data/docs/manual')\n",
    "METHODS_DIR = Path('data/docs/methods')\n",
    "PROMPT_SEEDS_DIR = Path('data/prompts/generation/components')\n",
    "PROMPT_COMPONENTS_DIR = Path('data/prompts/generation/components')\n",
    "VAL_PROMPT_COMPONENTS_DIR = Path('data/prompts/validation/components')\n",
    "GEN_PROMPTS_DIR = Path('data/prompts/generation/output')\n",
    "VAL_PROMPTS_DIR = Path('data/prompts/validation/output')\n",
    "PERSIST_DIR = Path(\"data/persist_dir\")\n",
    "OUTPUT_DIR = Path(\"output/\")\n",
    "DATASET_PATH = Path('data/datasets/dataset_v3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index generation & loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default tokenizer is for gpt-3.5\n",
    "# llama_index.core.global_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = SimpleDirectoryReader(METHODS_DIR).load_data()\n",
    "\n",
    "# embed_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"BAAI/bge-base-en-v1.5\"\n",
    "# )\n",
    "# Settings.text_splitter = SentenceSplitter(chunk_size=678, tokenizer=embed_tokenizer)\n",
    "\n",
    "# Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, show_progress=True\n",
    "# )\n",
    "\n",
    "# index.storage_context.persist(persist_dir=PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have an instance of TreeIndex named tree_index\n",
    "ref_doc_info = index.ref_doc_info\n",
    "\n",
    "# Now you can iterate over the ref_doc_info to view each node's details\n",
    "with open('temp/index.txt', 'w') as f:\n",
    "    for node_id, node_info in ref_doc_info.items():\n",
    "        f.write(f\"Node ID: {node_id}\\n\")\n",
    "        f.write(f\"Node Info: {node_info}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = retriever.retrieve('Get status of a cover.')\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080, compute capability 7.5, VMM: yes\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   102.54 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  5563.55 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3000\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   375.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  375.00 MiB, K (f16):  187.50 MiB, V (f16):  187.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    14.89 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   225.36 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "llm = Llama('models/mistral-7b-instruct-v0.2.Q6_K.gguf', n_ctx=3000, verbose=False, n_gpu_layers=-1) # mistral-7b-instruct-v0.2.Q4_0.gguf mistral-7b-instruct-v0.2.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =      30.34 ms /    78 runs   (    0.39 ms per token,  2571.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     106.17 ms /    43 tokens (    2.47 ms per token,   405.00 tokens per second)\n",
      "llama_print_timings:        eval time =    1320.72 ms /    77 runs   (   17.15 ms per token,    58.30 tokens per second)\n",
      "llama_print_timings:       total time =    1592.60 ms /   120 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-ccb579d1-43a9-4a63-9185-9bb4f7581452',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1710754907,\n",
       " 'model': 'models/mistral-7b-instruct-v0.2.Q6_K.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': ' The user likely wants to invoke a \"Close Blinds\" or \"Close Living Room Blinds\" command with an added instruction for the action to be performed slowly and take approximately 20 seconds to complete. In a smart home system, this could be achieved by using voice commands or an app interface that allows users to control their window coverings with adjustable speed settings.'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 43, 'completion_tokens': 77, 'total_tokens': 120}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Command: Please, close the living room blinds slowly for 20 seconds.\n",
    "\n",
    "What is the function user want to call in smart home?\n",
    "\"\"\"\n",
    "llm.create_chat_completion(messages=[{'role': 'user', 'content': prompt}], max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 18 11:41:49 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 22%   28C    P2    61W / 215W |   7670MiB /  8192MiB |     57%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1411      G   /usr/bin/gnome-shell                3MiB |\n",
      "|    0   N/A  N/A   3973912      C   ...envs/assistant/bin/python     7652MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_scheme_prompt = {\n",
    "    \"method\": {\n",
    "        \"type\": \"string\"\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"type\": \"object\"\n",
    "    }\n",
    "}\n",
    "\n",
    "example_1_json = {\n",
    "  \"method\":\"Cover.Open\",\n",
    "  \"params\":\n",
    "  {\n",
    "    \"id\":2\n",
    "  }\n",
    "}\n",
    "\n",
    "example_2_json = {\n",
    "  \"method\":\"Cover.Close\",\n",
    "  \"params\":\n",
    "  {\n",
    "    \"id\":0,\n",
    "    \"duration\":5,\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(VAL_PROMPT_COMPONENTS_DIR / 'instruction.md') as f:\n",
    "  instruction = f.read()\n",
    "\n",
    "variables = {\n",
    "    \"instruction\": instruction,\n",
    "    \"json_scheme\": \"The output JSON should follow the next scheme: \" + json.dumps(json_scheme_prompt),\n",
    "    \"devices\": \"\"\"Cover id=1\"\"\",\n",
    "    \"example_1\": \"\"\"Devices: Cover id=2\n",
    "Methods:\n",
    "API method 1:\n",
    "Method name: Cover.Open\n",
    "Method description:\n",
    "Properties:\n",
    "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully open, unless it times out because of maxtime_open first. If duration (seconds) is provided, Cover will move in the open direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
    "Response:\n",
    "null on success; error if the request can not be executed or failed\n",
    "\n",
    "Command: Open the cover.\n",
    "JSON: \"\"\" + json.dumps(example_1_json),\n",
    "\n",
    "    \"example_2\": \"\"\"Devices: Cover id=0\n",
    "Methods: \n",
    "API method 1:\n",
    "Method name: Cover.Close\n",
    "Method description:\n",
    "Properties:\n",
    "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully close, unless it times out because of maxtime_close first. If duration (seconds) is provided, Cover will move in the close direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
    "Response:\n",
    "null on success; error if the request can not be executed or failed\n",
    "\n",
    "Command: Close the kitchen cover quickly (for 5 seconds).\n",
    "JSON: \"\"\" + json.dumps(example_2_json),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI Assistant that controls the devices in a house. For a given user command create a corresponding JSON object. Don't add properties with null value in output JSON object. Output must be strictly in JSON format.\n",
      "The output JSON should follow the next scheme: {\"method\": {\"type\": \"string\"}, \"params\": {\"type\": \"object\"}}\n",
      "\n",
      "Devices: Cover id=2\n",
      "Methods:\n",
      "API method 1:\n",
      "Method name: Cover.Open\n",
      "Method description: Preconditions:\n",
      "Cover will not accept the command if:\n",
      "An  overvoltage  error is set at the time of the request.\n",
      "An  undervoltage  error is set at the time of the request.\n",
      "An  overtemp  error is set at the time of the request.\n",
      "An engaged  safety_switch  prohibits movement in the requested direction.\n",
      "Cover  calibration is running at the time of the request\n",
      "Properties:\n",
      "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully open, unless it times out because of maxtime_open first. If duration (seconds) is provided, Cover will move in the open direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
      "Response:\n",
      "null on success; error if the request can not be executed or failed\n",
      "\n",
      "Command: Open the cover.\n",
      "JSON: {\"method\": \"Cover.Open\", \"params\": {\"id\": 2}}\n",
      "\n",
      "Devices: Cover id=0\n",
      "Methods: \n",
      "API method 1:\n",
      "Method name: Cover.Close\n",
      "Method description: Preconditions:\n",
      "Cover will not accept the command if:\n",
      "An  overvoltage  error is set at the time of the request.\n",
      "An  undervoltage  error is set at the time of the request.\n",
      "An  overtemp  error is set at the time of the request.\n",
      "An engaged  safety_switch  prohibits movement in the requested direction.\n",
      "Cover  calibration is running at the time of the request\n",
      "Properties:\n",
      "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully close, unless it times out because of maxtime_close first. If duration (seconds) is provided, Cover will move in the close direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
      "Response:\n",
      "null on success; error if the request can not be executed or failed\n",
      "\n",
      "Command: Close the kitchen cover quickly (for 5 seconds).\n",
      "JSON: {\"method\": \"Cover.Close\", \"params\": {\"id\": 0, \"duration\": 5}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_prompt_template = \"\"\"\n",
    "{instruction}\n",
    "{json_scheme}\n",
    "\n",
    "{example_1}\n",
    "\n",
    "{example_2}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_template = \"\"\"Devices: {env}\n",
    "Methods:\n",
    "{methods_description}\n",
    "Command: {user_cmd}\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "base_prompt = base_prompt_template.format(**variables)\n",
    "\n",
    "print(base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "# logger.addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_cpp.llama_grammar.LlamaGrammar object at 0x7fd8361c0820>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] \n",
      "ws_31 ::= ws_30 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/grammars/json.gbnf') as f:\n",
    "    grammar_str = f.read()\n",
    "llama_grammar = LlamaGrammar.from_string(grammar_str)\n",
    "print(llama_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     240.60 ms /    34 runs   (    7.08 ms per token,   141.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     710.24 ms /  1137 tokens (    0.62 ms per token,  1600.87 tokens per second)\n",
      "llama_print_timings:        eval time =     650.41 ms /    33 runs   (   19.71 ms per token,    50.74 tokens per second)\n",
      "llama_print_timings:       total time =    1705.51 ms /  1170 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     229.55 ms /    33 runs   (    6.96 ms per token,   143.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     697.39 ms /  1138 tokens (    0.61 ms per token,  1631.80 tokens per second)\n",
      "llama_print_timings:        eval time =     630.57 ms /    32 runs   (   19.71 ms per token,    50.75 tokens per second)\n",
      "llama_print_timings:       total time =    1654.90 ms /  1170 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     340.62 ms /    51 runs   (    6.68 ms per token,   149.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     701.29 ms /  1147 tokens (    0.61 ms per token,  1635.55 tokens per second)\n",
      "llama_print_timings:        eval time =     987.38 ms /    50 runs   (   19.75 ms per token,    50.64 tokens per second)\n",
      "llama_print_timings:       total time =    2177.36 ms /  1197 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     227.32 ms /    32 runs   (    7.10 ms per token,   140.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     746.80 ms /  1265 tokens (    0.59 ms per token,  1693.89 tokens per second)\n",
      "llama_print_timings:        eval time =     616.92 ms /    31 runs   (   19.90 ms per token,    50.25 tokens per second)\n",
      "llama_print_timings:       total time =    1685.51 ms /  1296 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     209.52 ms /    30 runs   (    6.98 ms per token,   143.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     273.49 ms /   498 tokens (    0.55 ms per token,  1820.93 tokens per second)\n",
      "llama_print_timings:        eval time =     546.14 ms /    29 runs   (   18.83 ms per token,    53.10 tokens per second)\n",
      "llama_print_timings:       total time =    1115.33 ms /   527 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     246.55 ms /    36 runs   (    6.85 ms per token,   146.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1378.50 ms /  2195 tokens (    0.63 ms per token,  1592.31 tokens per second)\n",
      "llama_print_timings:        eval time =     740.52 ms /    35 runs   (   21.16 ms per token,    47.26 tokens per second)\n",
      "llama_print_timings:       total time =    2478.38 ms /  2230 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     248.12 ms /    36 runs   (    6.89 ms per token,   145.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1017.19 ms /  1554 tokens (    0.65 ms per token,  1527.74 tokens per second)\n",
      "llama_print_timings:        eval time =     695.87 ms /    35 runs   (   19.88 ms per token,    50.30 tokens per second)\n",
      "llama_print_timings:       total time =    2070.41 ms /  1589 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     224.77 ms /    32 runs   (    7.02 ms per token,   142.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     735.62 ms /  1213 tokens (    0.61 ms per token,  1648.95 tokens per second)\n",
      "llama_print_timings:        eval time =     613.67 ms /    31 runs   (   19.80 ms per token,    50.52 tokens per second)\n",
      "llama_print_timings:       total time =    1670.42 ms /  1244 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     191.06 ms /    28 runs   (    6.82 ms per token,   146.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     247.62 ms /   412 tokens (    0.60 ms per token,  1663.84 tokens per second)\n",
      "llama_print_timings:        eval time =     505.11 ms /    27 runs   (   18.71 ms per token,    53.45 tokens per second)\n",
      "llama_print_timings:       total time =    1021.69 ms /   439 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     156.73 ms /    23 runs   (    6.81 ms per token,   146.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     390.77 ms /   568 tokens (    0.69 ms per token,  1453.54 tokens per second)\n",
      "llama_print_timings:        eval time =     414.65 ms /    22 runs   (   18.85 ms per token,    53.06 tokens per second)\n",
      "llama_print_timings:       total time =    1028.19 ms /   590 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     152.70 ms /    22 runs   (    6.94 ms per token,   144.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     397.27 ms /   596 tokens (    0.67 ms per token,  1500.23 tokens per second)\n",
      "llama_print_timings:        eval time =     396.60 ms /    21 runs   (   18.89 ms per token,    52.95 tokens per second)\n",
      "llama_print_timings:       total time =    1010.45 ms /   617 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     152.41 ms /    22 runs   (    6.93 ms per token,   144.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     397.31 ms /   595 tokens (    0.67 ms per token,  1497.55 tokens per second)\n",
      "llama_print_timings:        eval time =     396.37 ms /    21 runs   (   18.87 ms per token,    52.98 tokens per second)\n",
      "llama_print_timings:       total time =    1010.14 ms /   616 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     157.02 ms /    23 runs   (    6.83 ms per token,   146.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     397.94 ms /   598 tokens (    0.67 ms per token,  1502.73 tokens per second)\n",
      "llama_print_timings:        eval time =     415.17 ms /    22 runs   (   18.87 ms per token,    52.99 tokens per second)\n",
      "llama_print_timings:       total time =    1035.32 ms /   620 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     156.84 ms /    23 runs   (    6.82 ms per token,   146.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     397.02 ms /   594 tokens (    0.67 ms per token,  1496.15 tokens per second)\n",
      "llama_print_timings:        eval time =     415.22 ms /    22 runs   (   18.87 ms per token,    52.98 tokens per second)\n",
      "llama_print_timings:       total time =    1035.81 ms /   616 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     151.46 ms /    22 runs   (    6.88 ms per token,   145.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     398.32 ms /   598 tokens (    0.67 ms per token,  1501.29 tokens per second)\n",
      "llama_print_timings:        eval time =     396.41 ms /    21 runs   (   18.88 ms per token,    52.98 tokens per second)\n",
      "llama_print_timings:       total time =    1009.95 ms /   619 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     191.38 ms /    28 runs   (    6.83 ms per token,   146.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     396.88 ms /   593 tokens (    0.67 ms per token,  1494.14 tokens per second)\n",
      "llama_print_timings:        eval time =     509.78 ms /    27 runs   (   18.88 ms per token,    52.96 tokens per second)\n",
      "llama_print_timings:       total time =    1178.63 ms /   620 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     148.72 ms /    22 runs   (    6.76 ms per token,   147.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1073.54 ms /  1741 tokens (    0.62 ms per token,  1621.74 tokens per second)\n",
      "llama_print_timings:        eval time =     431.15 ms /    21 runs   (   20.53 ms per token,    48.71 tokens per second)\n",
      "llama_print_timings:       total time =    1722.70 ms /  1762 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     131.42 ms /    19 runs   (    6.92 ms per token,   144.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1022.60 ms /  1655 tokens (    0.62 ms per token,  1618.43 tokens per second)\n",
      "llama_print_timings:        eval time =     368.50 ms /    18 runs   (   20.47 ms per token,    48.85 tokens per second)\n",
      "llama_print_timings:       total time =    1582.69 ms /  1673 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     144.69 ms /    21 runs   (    6.89 ms per token,   145.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1075.36 ms /  1747 tokens (    0.62 ms per token,  1624.58 tokens per second)\n",
      "llama_print_timings:        eval time =     411.53 ms /    20 runs   (   20.58 ms per token,    48.60 tokens per second)\n",
      "llama_print_timings:       total time =    1698.78 ms /  1767 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     136.36 ms /    20 runs   (    6.82 ms per token,   146.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1073.98 ms /  1742 tokens (    0.62 ms per token,  1622.01 tokens per second)\n",
      "llama_print_timings:        eval time =     391.06 ms /    19 runs   (   20.58 ms per token,    48.59 tokens per second)\n",
      "llama_print_timings:       total time =    1664.60 ms /  1761 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     150.97 ms /    22 runs   (    6.86 ms per token,   145.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1076.88 ms /  1744 tokens (    0.62 ms per token,  1619.49 tokens per second)\n",
      "llama_print_timings:        eval time =     432.70 ms /    21 runs   (   20.60 ms per token,    48.53 tokens per second)\n",
      "llama_print_timings:       total time =    1730.20 ms /  1765 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     131.37 ms /    19 runs   (    6.91 ms per token,   144.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1027.04 ms /  1660 tokens (    0.62 ms per token,  1616.29 tokens per second)\n",
      "llama_print_timings:        eval time =     369.00 ms /    18 runs   (   20.50 ms per token,    48.78 tokens per second)\n",
      "llama_print_timings:       total time =    1588.40 ms /  1678 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     150.45 ms /    22 runs   (    6.84 ms per token,   146.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1074.57 ms /  1741 tokens (    0.62 ms per token,  1620.18 tokens per second)\n",
      "llama_print_timings:        eval time =     432.78 ms /    21 runs   (   20.61 ms per token,    48.52 tokens per second)\n",
      "llama_print_timings:       total time =    1727.62 ms /  1762 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     143.95 ms /    21 runs   (    6.85 ms per token,   145.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1077.81 ms /  1746 tokens (    0.62 ms per token,  1619.95 tokens per second)\n",
      "llama_print_timings:        eval time =     411.59 ms /    20 runs   (   20.58 ms per token,    48.59 tokens per second)\n",
      "llama_print_timings:       total time =    1699.85 ms /  1766 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     139.94 ms /    20 runs   (    7.00 ms per token,   142.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     218.37 ms /   351 tokens (    0.62 ms per token,  1607.38 tokens per second)\n",
      "llama_print_timings:        eval time =     353.05 ms /    19 runs   (   18.58 ms per token,    53.82 tokens per second)\n",
      "llama_print_timings:       total time =     767.47 ms /   370 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     157.46 ms /    23 runs   (    6.85 ms per token,   146.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     243.61 ms /   391 tokens (    0.62 ms per token,  1605.05 tokens per second)\n",
      "llama_print_timings:        eval time =     411.76 ms /    22 runs   (   18.72 ms per token,    53.43 tokens per second)\n",
      "llama_print_timings:       total time =     878.28 ms /   413 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     157.29 ms /    23 runs   (    6.84 ms per token,   146.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.53 ms /   355 tokens (    0.62 ms per token,  1617.11 tokens per second)\n",
      "llama_print_timings:        eval time =     408.66 ms /    22 runs   (   18.58 ms per token,    53.83 tokens per second)\n",
      "llama_print_timings:       total time =     850.15 ms /   377 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     157.21 ms /    23 runs   (    6.84 ms per token,   146.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     243.49 ms /   393 tokens (    0.62 ms per token,  1614.05 tokens per second)\n",
      "llama_print_timings:        eval time =     411.82 ms /    22 runs   (   18.72 ms per token,    53.42 tokens per second)\n",
      "llama_print_timings:       total time =     876.73 ms /   415 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     232.66 ms /    33 runs   (    7.05 ms per token,   141.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     702.00 ms /  1142 tokens (    0.61 ms per token,  1626.77 tokens per second)\n",
      "llama_print_timings:        eval time =     633.04 ms /    32 runs   (   19.78 ms per token,    50.55 tokens per second)\n",
      "llama_print_timings:       total time =    1666.09 ms /  1174 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     204.29 ms /    30 runs   (    6.81 ms per token,   146.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1241.55 ms /  2051 tokens (    0.61 ms per token,  1651.97 tokens per second)\n",
      "llama_print_timings:        eval time =     594.00 ms /    29 runs   (   20.48 ms per token,    48.82 tokens per second)\n",
      "llama_print_timings:       total time =    2135.71 ms /  2080 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Input.SetConfig\", \"params\": {\"id\": 444, \"config\": {\"enable\": false}}}\n",
      "\n",
      "Requested tokens (3275) exceed context window of 3000\n",
      "{\"method\": \"Input.SetConfig\", \"params\": {\"id\": 444, \"config\": {\"enable\": false}}}\n",
      "\n",
      "Requested tokens (3273) exceed context window of 3000\n",
      "{\"method\": \"Input.SetConfig\", \"params\": {\"id\": 444, \"config\": {\"enable\": false}}}\n",
      "\n",
      "Requested tokens (3273) exceed context window of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     265.93 ms /    37 runs   (    7.19 ms per token,   139.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1015.71 ms /  1625 tokens (    0.63 ms per token,  1599.86 tokens per second)\n",
      "llama_print_timings:        eval time =     722.96 ms /    36 runs   (   20.08 ms per token,    49.80 tokens per second)\n",
      "llama_print_timings:       total time =    2117.14 ms /  1661 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Input.SetConfig\", \"params\": {\"id\": 444, \"config\": {\"range_map\": [1, 10]}}}\n",
      "Requested tokens (3275) exceed context window of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     377.43 ms /    56 runs   (    6.74 ms per token,   148.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1062.41 ms /  1718 tokens (    0.62 ms per token,  1617.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1123.79 ms /    55 runs   (   20.43 ms per token,    48.94 tokens per second)\n",
      "llama_print_timings:       total time =    2735.18 ms /  1773 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     149.44 ms /    22 runs   (    6.79 ms per token,   147.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1456.08 ms /  2306 tokens (    0.63 ms per token,  1583.71 tokens per second)\n",
      "llama_print_timings:        eval time =     439.31 ms /    21 runs   (   20.92 ms per token,    47.80 tokens per second)\n",
      "llama_print_timings:       total time =    2118.43 ms /  2327 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Light.GetConfig\", \"params\": {\"id\": 444}}\n",
      "\n",
      "Requested tokens (3233) exceed context window of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     151.32 ms /    22 runs   (    6.88 ms per token,   145.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1163.50 ms /  1918 tokens (    0.61 ms per token,  1648.47 tokens per second)\n",
      "llama_print_timings:        eval time =     439.24 ms /    21 runs   (   20.92 ms per token,    47.81 tokens per second)\n",
      "llama_print_timings:       total time =    1825.24 ms /  1939 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     136.44 ms /    20 runs   (    6.82 ms per token,   146.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1459.33 ms /  2313 tokens (    0.63 ms per token,  1584.98 tokens per second)\n",
      "llama_print_timings:        eval time =     397.13 ms /    19 runs   (   20.90 ms per token,    47.84 tokens per second)\n",
      "llama_print_timings:       total time =    2059.53 ms /  2332 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     149.27 ms /    22 runs   (    6.78 ms per token,   147.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1464.75 ms /  2312 tokens (    0.63 ms per token,  1578.42 tokens per second)\n",
      "llama_print_timings:        eval time =     439.95 ms /    21 runs   (   20.95 ms per token,    47.73 tokens per second)\n",
      "llama_print_timings:       total time =    2126.71 ms /  2333 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Light.GetConfig\", \"params\": {\"id\": 444}}\n",
      "\n",
      "Requested tokens (3236) exceed context window of 3000\n",
      "{\"method\": \"Light.GetConfig\", \"params\": {\"id\": 444}}\n",
      "\n",
      "Requested tokens (3083) exceed context window of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     157.01 ms /    23 runs   (    6.83 ms per token,   146.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1152.12 ms /  1884 tokens (    0.61 ms per token,  1635.24 tokens per second)\n",
      "llama_print_timings:        eval time =     447.39 ms /    22 runs   (   20.34 ms per token,    49.17 tokens per second)\n",
      "llama_print_timings:       total time =    1829.79 ms /  1906 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     143.99 ms /    21 runs   (    6.86 ms per token,   145.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1459.31 ms /  2312 tokens (    0.63 ms per token,  1584.31 tokens per second)\n",
      "llama_print_timings:        eval time =     419.36 ms /    20 runs   (   20.97 ms per token,    47.69 tokens per second)\n",
      "llama_print_timings:       total time =    2092.76 ms /  2332 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     150.91 ms /    22 runs   (    6.86 ms per token,   145.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1461.05 ms /  2314 tokens (    0.63 ms per token,  1583.79 tokens per second)\n",
      "llama_print_timings:        eval time =     438.82 ms /    21 runs   (   20.90 ms per token,    47.86 tokens per second)\n",
      "llama_print_timings:       total time =    2125.97 ms /  2335 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     249.86 ms /    36 runs   (    6.94 ms per token,   144.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1462.43 ms /  2314 tokens (    0.63 ms per token,  1582.30 tokens per second)\n",
      "llama_print_timings:        eval time =     732.10 ms /    35 runs   (   20.92 ms per token,    47.81 tokens per second)\n",
      "llama_print_timings:       total time =    2560.31 ms /  2349 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Light.GetConfig\", \"params\": {\"id\": 444, \"properties\": {\"button_presets\": {\"type\": \"object\"}}\n",
      "Expecting ',' delimiter: line 1 column 105 (char 104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     250.09 ms /    36 runs   (    6.95 ms per token,   143.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     736.20 ms /  1206 tokens (    0.61 ms per token,  1638.14 tokens per second)\n",
      "llama_print_timings:        eval time =     694.60 ms /    35 runs   (   19.85 ms per token,    50.39 tokens per second)\n",
      "llama_print_timings:       total time =    1791.23 ms /  1241 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     262.51 ms /    38 runs   (    6.91 ms per token,   144.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     875.02 ms /  1501 tokens (    0.58 ms per token,  1715.40 tokens per second)\n",
      "llama_print_timings:        eval time =     742.27 ms /    37 runs   (   20.06 ms per token,    49.85 tokens per second)\n",
      "llama_print_timings:       total time =    1995.98 ms /  1538 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     206.64 ms /    30 runs   (    6.89 ms per token,   145.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1418.58 ms /  2286 tokens (    0.62 ms per token,  1611.48 tokens per second)\n",
      "llama_print_timings:        eval time =     609.63 ms /    29 runs   (   21.02 ms per token,    47.57 tokens per second)\n",
      "llama_print_timings:       total time =    2332.02 ms /  2315 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     150.36 ms /    22 runs   (    6.83 ms per token,   146.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1462.21 ms /  2307 tokens (    0.63 ms per token,  1577.74 tokens per second)\n",
      "llama_print_timings:        eval time =     439.25 ms /    21 runs   (   20.92 ms per token,    47.81 tokens per second)\n",
      "llama_print_timings:       total time =    2125.37 ms /  2328 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     241.09 ms /    35 runs   (    6.89 ms per token,   145.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1465.53 ms /  2315 tokens (    0.63 ms per token,  1579.63 tokens per second)\n",
      "llama_print_timings:        eval time =     711.64 ms /    34 runs   (   20.93 ms per token,    47.78 tokens per second)\n",
      "llama_print_timings:       total time =    2530.94 ms /  2349 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     203.55 ms /    29 runs   (    7.02 ms per token,   142.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     258.35 ms /   440 tokens (    0.59 ms per token,  1703.12 tokens per second)\n",
      "llama_print_timings:        eval time =     526.93 ms /    28 runs   (   18.82 ms per token,    53.14 tokens per second)\n",
      "llama_print_timings:       total time =    1072.60 ms /   468 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     153.01 ms /    22 runs   (    6.96 ms per token,   143.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.24 ms /   467 tokens (    0.57 ms per token,  1760.68 tokens per second)\n",
      "llama_print_timings:        eval time =     394.86 ms /    21 runs   (   18.80 ms per token,    53.18 tokens per second)\n",
      "llama_print_timings:       total time =     875.41 ms /   488 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     200.94 ms /    29 runs   (    6.93 ms per token,   144.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.50 ms /   472 tokens (    0.56 ms per token,  1771.14 tokens per second)\n",
      "llama_print_timings:        eval time =     526.80 ms /    28 runs   (   18.81 ms per token,    53.15 tokens per second)\n",
      "llama_print_timings:       total time =    1076.16 ms /   500 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     290.67 ms /    42 runs   (    6.92 ms per token,   144.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     754.67 ms /  1247 tokens (    0.61 ms per token,  1652.37 tokens per second)\n",
      "llama_print_timings:        eval time =     817.60 ms /    41 runs   (   19.94 ms per token,    50.15 tokens per second)\n",
      "llama_print_timings:       total time =    1988.99 ms /  1288 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     207.01 ms /    30 runs   (    6.90 ms per token,   144.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     751.85 ms /  1241 tokens (    0.61 ms per token,  1650.59 tokens per second)\n",
      "llama_print_timings:        eval time =     576.69 ms /    29 runs   (   19.89 ms per token,    50.29 tokens per second)\n",
      "llama_print_timings:       total time =    1627.34 ms /  1270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     201.29 ms /    30 runs   (    6.71 ms per token,   149.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1466.66 ms /  2320 tokens (    0.63 ms per token,  1581.82 tokens per second)\n",
      "llama_print_timings:        eval time =     607.15 ms /    29 runs   (   20.94 ms per token,    47.76 tokens per second)\n",
      "llama_print_timings:       total time =    2373.13 ms /  2349 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Light.SetConfig\", \"params\": {\n",
      "\"id\": 444,\n",
      "\"config\": {\n",
      "\"auto\n",
      "Unterminated string starting at: line 4 column 1 (char 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     145.22 ms /    21 runs   (    6.92 ms per token,   144.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1078.21 ms /  1738 tokens (    0.62 ms per token,  1611.93 tokens per second)\n",
      "llama_print_timings:        eval time =     412.26 ms /    20 runs   (   20.61 ms per token,    48.51 tokens per second)\n",
      "llama_print_timings:       total time =    1704.05 ms /  1758 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     145.74 ms /    21 runs   (    6.94 ms per token,   144.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     751.97 ms /  1238 tokens (    0.61 ms per token,  1646.34 tokens per second)\n",
      "llama_print_timings:        eval time =     397.87 ms /    20 runs   (   19.89 ms per token,    50.27 tokens per second)\n",
      "llama_print_timings:       total time =    1361.97 ms /  1258 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     149.91 ms /    22 runs   (    6.81 ms per token,   146.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     736.27 ms /  1204 tokens (    0.61 ms per token,  1635.26 tokens per second)\n",
      "llama_print_timings:        eval time =     416.85 ms /    21 runs   (   19.85 ms per token,    50.38 tokens per second)\n",
      "llama_print_timings:       total time =    1371.20 ms /  1225 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     198.79 ms /    29 runs   (    6.85 ms per token,   145.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1177.81 ms /  1927 tokens (    0.61 ms per token,  1636.09 tokens per second)\n",
      "llama_print_timings:        eval time =     585.86 ms /    28 runs   (   20.92 ms per token,    47.79 tokens per second)\n",
      "llama_print_timings:       total time =    2055.01 ms /  1955 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     259.26 ms /    37 runs   (    7.01 ms per token,   142.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1464.16 ms /  2313 tokens (    0.63 ms per token,  1579.75 tokens per second)\n",
      "llama_print_timings:        eval time =     753.10 ms /    36 runs   (   20.92 ms per token,    47.80 tokens per second)\n",
      "llama_print_timings:       total time =    2595.97 ms /  2349 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Light.GetConfig\", \"params\": {\"id\": 444, \"properties\": {\"errors\": {\"type\": \"object\", \"description\": \"An\n",
      "Unterminated string starting at: line 1 column 112 (char 111)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     254.23 ms /    37 runs   (    6.87 ms per token,   145.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     747.95 ms /  1253 tokens (    0.60 ms per token,  1675.25 tokens per second)\n",
      "llama_print_timings:        eval time =     718.98 ms /    36 runs   (   19.97 ms per token,    50.07 tokens per second)\n",
      "llama_print_timings:       total time =    1833.17 ms /  1289 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     213.24 ms /    31 runs   (    6.88 ms per token,   145.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1420.68 ms /  2286 tokens (    0.62 ms per token,  1609.09 tokens per second)\n",
      "llama_print_timings:        eval time =     630.60 ms /    30 runs   (   21.02 ms per token,    47.57 tokens per second)\n",
      "llama_print_timings:       total time =    2366.07 ms /  2316 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     214.04 ms /    31 runs   (    6.90 ms per token,   144.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1181.10 ms /  1929 tokens (    0.61 ms per token,  1633.23 tokens per second)\n",
      "llama_print_timings:        eval time =     627.54 ms /    30 runs   (   20.92 ms per token,    47.81 tokens per second)\n",
      "llama_print_timings:       total time =    2120.97 ms /  1959 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     301.19 ms /    44 runs   (    6.85 ms per token,   146.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     250.11 ms /   414 tokens (    0.60 ms per token,  1655.27 tokens per second)\n",
      "llama_print_timings:        eval time =     806.99 ms /    43 runs   (   18.77 ms per token,    53.28 tokens per second)\n",
      "llama_print_timings:       total time =    1483.23 ms /   457 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     250.48 ms /    36 runs   (    6.96 ms per token,   143.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     249.89 ms /   412 tokens (    0.61 ms per token,  1648.74 tokens per second)\n",
      "llama_print_timings:        eval time =     656.89 ms /    35 runs   (   18.77 ms per token,    53.28 tokens per second)\n",
      "llama_print_timings:       total time =    1260.12 ms /   447 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     257.00 ms /    37 runs   (    6.95 ms per token,   143.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     400.05 ms /   599 tokens (    0.67 ms per token,  1497.30 tokens per second)\n",
      "llama_print_timings:        eval time =     681.80 ms /    36 runs   (   18.94 ms per token,    52.80 tokens per second)\n",
      "llama_print_timings:       total time =    1446.38 ms /   635 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     292.71 ms /    42 runs   (    6.97 ms per token,   143.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     249.75 ms /   413 tokens (    0.60 ms per token,  1653.65 tokens per second)\n",
      "llama_print_timings:        eval time =     769.21 ms /    41 runs   (   18.76 ms per token,    53.30 tokens per second)\n",
      "llama_print_timings:       total time =    1431.46 ms /   454 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     314.69 ms /    46 runs   (    6.84 ms per token,   146.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     401.17 ms /   602 tokens (    0.67 ms per token,  1500.63 tokens per second)\n",
      "llama_print_timings:        eval time =     854.52 ms /    45 runs   (   18.99 ms per token,    52.66 tokens per second)\n",
      "llama_print_timings:       total time =    1704.18 ms /   647 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     291.11 ms /    42 runs   (    6.93 ms per token,   144.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     249.82 ms /   412 tokens (    0.61 ms per token,  1649.17 tokens per second)\n",
      "llama_print_timings:        eval time =     769.26 ms /    41 runs   (   18.76 ms per token,    53.30 tokens per second)\n",
      "llama_print_timings:       total time =    1429.47 ms /   453 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     176.87 ms /    26 runs   (    6.80 ms per token,   147.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     749.28 ms /  1257 tokens (    0.60 ms per token,  1677.62 tokens per second)\n",
      "llama_print_timings:        eval time =     498.77 ms /    25 runs   (   19.95 ms per token,    50.12 tokens per second)\n",
      "llama_print_timings:       total time =    1505.35 ms /  1282 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     219.46 ms /    32 runs   (    6.86 ms per token,   145.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1464.34 ms /  2308 tokens (    0.63 ms per token,  1576.14 tokens per second)\n",
      "llama_print_timings:        eval time =     648.72 ms /    31 runs   (   20.93 ms per token,    47.79 tokens per second)\n",
      "llama_print_timings:       total time =    2436.82 ms /  2339 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     139.12 ms /    20 runs   (    6.96 ms per token,   143.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.27 ms /   243 tokens (    0.67 ms per token,  1488.35 tokens per second)\n",
      "llama_print_timings:        eval time =     351.28 ms /    19 runs   (   18.49 ms per token,    54.09 tokens per second)\n",
      "llama_print_timings:       total time =     710.83 ms /   262 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     158.23 ms /    23 runs   (    6.88 ms per token,   145.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     221.79 ms /   365 tokens (    0.61 ms per token,  1645.72 tokens per second)\n",
      "llama_print_timings:        eval time =     410.65 ms /    22 runs   (   18.67 ms per token,    53.57 tokens per second)\n",
      "llama_print_timings:       total time =     856.79 ms /   387 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     158.50 ms /    23 runs   (    6.89 ms per token,   145.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     222.24 ms /   367 tokens (    0.61 ms per token,  1651.35 tokens per second)\n",
      "llama_print_timings:        eval time =     411.14 ms /    22 runs   (   18.69 ms per token,    53.51 tokens per second)\n",
      "llama_print_timings:       total time =     857.00 ms /   389 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     158.94 ms /    23 runs   (    6.91 ms per token,   144.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     222.26 ms /   366 tokens (    0.61 ms per token,  1646.71 tokens per second)\n",
      "llama_print_timings:        eval time =     411.03 ms /    22 runs   (   18.68 ms per token,    53.52 tokens per second)\n",
      "llama_print_timings:       total time =     857.80 ms /   388 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     212.74 ms /    30 runs   (    7.09 ms per token,   141.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1464.23 ms /  2308 tokens (    0.63 ms per token,  1576.25 tokens per second)\n",
      "llama_print_timings:        eval time =     606.61 ms /    29 runs   (   20.92 ms per token,    47.81 tokens per second)\n",
      "llama_print_timings:       total time =    2382.09 ms /  2337 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     214.96 ms /    30 runs   (    7.17 ms per token,   139.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1463.86 ms /  2307 tokens (    0.63 ms per token,  1575.97 tokens per second)\n",
      "llama_print_timings:        eval time =     606.61 ms /    29 runs   (   20.92 ms per token,    47.81 tokens per second)\n",
      "llama_print_timings:       total time =    2384.42 ms /  2336 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     145.99 ms /    21 runs   (    6.95 ms per token,   143.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.73 ms /   240 tokens (    0.68 ms per token,  1474.84 tokens per second)\n",
      "llama_print_timings:        eval time =     369.79 ms /    20 runs   (   18.49 ms per token,    54.09 tokens per second)\n",
      "llama_print_timings:       total time =     739.65 ms /   260 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     210.41 ms /    30 runs   (    7.01 ms per token,   142.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.97 ms /   465 tokens (    0.57 ms per token,  1754.90 tokens per second)\n",
      "llama_print_timings:        eval time =     545.17 ms /    29 runs   (   18.80 ms per token,    53.19 tokens per second)\n",
      "llama_print_timings:       total time =    1104.96 ms /   494 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     196.87 ms /    28 runs   (    7.03 ms per token,   142.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.02 ms /   461 tokens (    0.57 ms per token,  1746.05 tokens per second)\n",
      "llama_print_timings:        eval time =     507.82 ms /    27 runs   (   18.81 ms per token,    53.17 tokens per second)\n",
      "llama_print_timings:       total time =    1047.42 ms /   488 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     209.76 ms /    30 runs   (    6.99 ms per token,   143.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.57 ms /   463 tokens (    0.57 ms per token,  1750.00 tokens per second)\n",
      "llama_print_timings:        eval time =     544.83 ms /    29 runs   (   18.79 ms per token,    53.23 tokens per second)\n",
      "llama_print_timings:       total time =    1103.91 ms /   492 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     150.44 ms /    22 runs   (    6.84 ms per token,   146.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1176.97 ms /  1923 tokens (    0.61 ms per token,  1633.85 tokens per second)\n",
      "llama_print_timings:        eval time =     439.11 ms /    21 runs   (   20.91 ms per token,    47.82 tokens per second)\n",
      "llama_print_timings:       total time =    1838.23 ms /  1944 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     144.42 ms /    21 runs   (    6.88 ms per token,   145.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1179.55 ms /  1925 tokens (    0.61 ms per token,  1631.98 tokens per second)\n",
      "llama_print_timings:        eval time =     418.18 ms /    20 runs   (   20.91 ms per token,    47.83 tokens per second)\n",
      "llama_print_timings:       total time =    1810.65 ms /  1945 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     220.53 ms /    32 runs   (    6.89 ms per token,   145.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     888.28 ms /  1529 tokens (    0.58 ms per token,  1721.30 tokens per second)\n",
      "llama_print_timings:        eval time =     617.64 ms /    31 runs   (   19.92 ms per token,    50.19 tokens per second)\n",
      "llama_print_timings:       total time =    1824.92 ms /  1560 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     212.84 ms /    31 runs   (    6.87 ms per token,   145.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     887.06 ms /  1528 tokens (    0.58 ms per token,  1722.54 tokens per second)\n",
      "llama_print_timings:        eval time =     597.49 ms /    30 runs   (   19.92 ms per token,    50.21 tokens per second)\n",
      "llama_print_timings:       total time =    1791.61 ms /  1558 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Switch.GetStatus\", \"params\": {\"id\": 444, \"properties\": [\"pf\", \"freq\"]}}\n",
      "Requested tokens (3099) exceed context window of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     757.84 ms /   109 runs   (    6.95 ms per token,   143.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     366.75 ms /   521 tokens (    0.70 ms per token,  1420.59 tokens per second)\n",
      "llama_print_timings:        eval time =    2040.97 ms /   108 runs   (   18.90 ms per token,    52.92 tokens per second)\n",
      "llama_print_timings:       total time =    3480.11 ms /   629 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     151.72 ms /    22 runs   (    6.90 ms per token,   145.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.49 ms /   356 tokens (    0.62 ms per token,  1621.93 tokens per second)\n",
      "llama_print_timings:        eval time =     390.93 ms /    21 runs   (   18.62 ms per token,    53.72 tokens per second)\n",
      "llama_print_timings:       total time =     825.23 ms /   377 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     194.25 ms /    28 runs   (    6.94 ms per token,   144.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     297.82 ms /   514 tokens (    0.58 ms per token,  1725.90 tokens per second)\n",
      "llama_print_timings:        eval time =     509.42 ms /    27 runs   (   18.87 ms per token,    53.00 tokens per second)\n",
      "llama_print_timings:       total time =    1083.62 ms /   541 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     150.34 ms /    22 runs   (    6.83 ms per token,   146.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     218.18 ms /   335 tokens (    0.65 ms per token,  1535.46 tokens per second)\n",
      "llama_print_timings:        eval time =     390.13 ms /    21 runs   (   18.58 ms per token,    53.83 tokens per second)\n",
      "llama_print_timings:       total time =     821.19 ms /   356 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     150.09 ms /    22 runs   (    6.82 ms per token,   146.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     278.81 ms /   512 tokens (    0.54 ms per token,  1836.40 tokens per second)\n",
      "llama_print_timings:        eval time =     396.52 ms /    21 runs   (   18.88 ms per token,    52.96 tokens per second)\n",
      "llama_print_timings:       total time =     888.27 ms /   533 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     194.07 ms /    28 runs   (    6.93 ms per token,   144.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.10 ms /   511 tokens (    0.55 ms per token,  1830.87 tokens per second)\n",
      "llama_print_timings:        eval time =     509.06 ms /    27 runs   (   18.85 ms per token,    53.04 tokens per second)\n",
      "llama_print_timings:       total time =    1062.38 ms /   538 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     151.79 ms /    22 runs   (    6.90 ms per token,   144.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     220.50 ms /   358 tokens (    0.62 ms per token,  1623.60 tokens per second)\n",
      "llama_print_timings:        eval time =     391.16 ms /    21 runs   (   18.63 ms per token,    53.69 tokens per second)\n",
      "llama_print_timings:       total time =     826.04 ms /   379 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     157.47 ms /    23 runs   (    6.85 ms per token,   146.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     263.80 ms /   460 tokens (    0.57 ms per token,  1743.73 tokens per second)\n",
      "llama_print_timings:        eval time =     413.34 ms /    22 runs   (   18.79 ms per token,    53.23 tokens per second)\n",
      "llama_print_timings:       total time =     899.72 ms /   482 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     144.81 ms /    21 runs   (    6.90 ms per token,   145.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.62 ms /   462 tokens (    0.57 ms per token,  1745.92 tokens per second)\n",
      "llama_print_timings:        eval time =     375.72 ms /    20 runs   (   18.79 ms per token,    53.23 tokens per second)\n",
      "llama_print_timings:       total time =     845.05 ms /   482 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     192.76 ms /    27 runs   (    7.14 ms per token,   140.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.54 ms /   462 tokens (    0.57 ms per token,  1746.46 tokens per second)\n",
      "llama_print_timings:        eval time =     488.64 ms /    26 runs   (   18.79 ms per token,    53.21 tokens per second)\n",
      "llama_print_timings:       total time =    1023.45 ms /   488 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     152.17 ms /    22 runs   (    6.92 ms per token,   144.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.72 ms /   326 tokens (    0.66 ms per token,  1504.26 tokens per second)\n",
      "llama_print_timings:        eval time =     390.22 ms /    21 runs   (   18.58 ms per token,    53.82 tokens per second)\n",
      "llama_print_timings:       total time =     822.16 ms /   347 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     147.31 ms /    21 runs   (    7.01 ms per token,   142.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     217.59 ms /   334 tokens (    0.65 ms per token,  1535.00 tokens per second)\n",
      "llama_print_timings:        eval time =     371.70 ms /    20 runs   (   18.58 ms per token,    53.81 tokens per second)\n",
      "llama_print_timings:       total time =     796.95 ms /   354 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     147.51 ms /    21 runs   (    7.02 ms per token,   142.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     217.82 ms /   337 tokens (    0.65 ms per token,  1547.16 tokens per second)\n",
      "llama_print_timings:        eval time =     371.72 ms /    20 runs   (   18.59 ms per token,    53.80 tokens per second)\n",
      "llama_print_timings:       total time =     798.61 ms /   357 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     165.11 ms /    24 runs   (    6.88 ms per token,   145.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     210.55 ms /   305 tokens (    0.69 ms per token,  1448.57 tokens per second)\n",
      "llama_print_timings:        eval time =     426.93 ms /    23 runs   (   18.56 ms per token,    53.87 tokens per second)\n",
      "llama_print_timings:       total time =     871.27 ms /   328 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     165.10 ms /    24 runs   (    6.88 ms per token,   145.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     211.25 ms /   307 tokens (    0.69 ms per token,  1453.26 tokens per second)\n",
      "llama_print_timings:        eval time =     426.95 ms /    23 runs   (   18.56 ms per token,    53.87 tokens per second)\n",
      "llama_print_timings:       total time =     871.71 ms /   330 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     159.76 ms /    23 runs   (    6.95 ms per token,   143.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     210.92 ms /   307 tokens (    0.69 ms per token,  1455.52 tokens per second)\n",
      "llama_print_timings:        eval time =     408.37 ms /    22 runs   (   18.56 ms per token,    53.87 tokens per second)\n",
      "llama_print_timings:       total time =     844.72 ms /   329 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     146.09 ms /    21 runs   (    6.96 ms per token,   143.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1167.91 ms /  1918 tokens (    0.61 ms per token,  1642.25 tokens per second)\n",
      "llama_print_timings:        eval time =     419.01 ms /    20 runs   (   20.95 ms per token,    47.73 tokens per second)\n",
      "llama_print_timings:       total time =    1802.88 ms /  1938 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Input.GetConfig\", \"params\": {\"id\": 444}}\n",
      "Requested tokens (3277) exceed context window of 3000\n",
      "{\"method\": \"Input.GetConfig\", \"params\": {\"id\": 444}}\n",
      "Requested tokens (3276) exceed context window of 3000\n",
      "{\"method\": \"Input.GetConfig\", \"params\": {\"id\": 444}}\n",
      "Requested tokens (3272) exceed context window of 3000\n",
      "{\"method\": \"Input.GetConfig\", \"params\": {\"id\": 444}}\n",
      "Requested tokens (3276) exceed context window of 3000\n",
      "{\"method\": \"Input.GetConfig\", \"params\": {\"id\": 444}}\n",
      "Requested tokens (3277) exceed context window of 3000\n",
      "{\"method\": \"Input.GetConfig\", \"params\": {\"id\": 444}}\n",
      "Requested tokens (3276) exceed context window of 3000\n",
      "{\"method\": \"Input.GetConfig\", \"params\": {\"id\": 444}}\n",
      "Requested tokens (3276) exceed context window of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     213.50 ms /    31 runs   (    6.89 ms per token,   145.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     487.82 ms /   543 tokens (    0.90 ms per token,  1113.11 tokens per second)\n",
      "llama_print_timings:        eval time =     567.58 ms /    30 runs   (   18.92 ms per token,    52.86 tokens per second)\n",
      "llama_print_timings:       total time =    1360.12 ms /   573 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     214.73 ms /    31 runs   (    6.93 ms per token,   144.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     919.62 ms /  1540 tokens (    0.60 ms per token,  1674.61 tokens per second)\n",
      "llama_print_timings:        eval time =     598.94 ms /    30 runs   (   19.96 ms per token,    50.09 tokens per second)\n",
      "llama_print_timings:       total time =    1831.69 ms /  1570 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     467.42 ms /    69 runs   (    6.77 ms per token,   147.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1420.23 ms /  2281 tokens (    0.62 ms per token,  1606.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1430.90 ms /    68 runs   (   21.04 ms per token,    47.52 tokens per second)\n",
      "llama_print_timings:       total time =    3537.91 ms /  2349 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\": \"Light.SetConfig\", \"params\": {\n",
      "\"id\": 444,\n",
      "\"config\": {\n",
      "\"name\": \"bedroom curtains\",\n",
      "\"in_mode\": \"dim\",\n",
      "\"initial_state\": \"restore_last\",\n",
      "\"min_brightness_on_toggle\":\n",
      "Expecting value: line 7 column 28 (char 171)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     164.94 ms /    24 runs   (    6.87 ms per token,   145.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     233.77 ms /   380 tokens (    0.62 ms per token,  1625.56 tokens per second)\n",
      "llama_print_timings:        eval time =     431.34 ms /    23 runs   (   18.75 ms per token,    53.32 tokens per second)\n",
      "llama_print_timings:       total time =     898.08 ms /   403 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     211.99 ms /    31 runs   (    6.84 ms per token,   146.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.12 ms /   467 tokens (    0.57 ms per token,  1754.83 tokens per second)\n",
      "llama_print_timings:        eval time =     565.16 ms /    30 runs   (   18.84 ms per token,    53.08 tokens per second)\n",
      "llama_print_timings:       total time =    1131.21 ms /   497 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     161.01 ms /    23 runs   (    7.00 ms per token,   142.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     244.66 ms /   392 tokens (    0.62 ms per token,  1602.20 tokens per second)\n",
      "llama_print_timings:        eval time =     413.13 ms /    22 runs   (   18.78 ms per token,    53.25 tokens per second)\n",
      "llama_print_timings:       total time =     884.74 ms /   414 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     227.49 ms /    33 runs   (    6.89 ms per token,   145.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.54 ms /  1264 tokens (    0.60 ms per token,  1677.40 tokens per second)\n",
      "llama_print_timings:        eval time =     640.74 ms /    32 runs   (   20.02 ms per token,    49.94 tokens per second)\n",
      "llama_print_timings:       total time =    1722.31 ms /  1296 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     231.97 ms /    34 runs   (    6.82 ms per token,   146.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1469.03 ms /  2314 tokens (    0.63 ms per token,  1575.19 tokens per second)\n",
      "llama_print_timings:        eval time =     691.67 ms /    33 runs   (   20.96 ms per token,    47.71 tokens per second)\n",
      "llama_print_timings:       total time =    2503.08 ms /  2347 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     191.99 ms /    28 runs   (    6.86 ms per token,   145.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1468.07 ms /  2312 tokens (    0.63 ms per token,  1574.86 tokens per second)\n",
      "llama_print_timings:        eval time =     566.09 ms /    27 runs   (   20.97 ms per token,    47.70 tokens per second)\n",
      "llama_print_timings:       total time =    2319.40 ms /  2339 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     270.49 ms /    39 runs   (    6.94 ms per token,   144.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     755.65 ms /  1269 tokens (    0.60 ms per token,  1679.34 tokens per second)\n",
      "llama_print_timings:        eval time =     761.64 ms /    38 runs   (   20.04 ms per token,    49.89 tokens per second)\n",
      "llama_print_timings:       total time =    1904.94 ms /  1307 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     283.61 ms /    41 runs   (    6.92 ms per token,   144.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     256.12 ms /   435 tokens (    0.59 ms per token,  1698.40 tokens per second)\n",
      "llama_print_timings:        eval time =     752.73 ms /    40 runs   (   18.82 ms per token,    53.14 tokens per second)\n",
      "llama_print_timings:       total time =    1409.67 ms /   475 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     382.58 ms /    55 runs   (    6.96 ms per token,   143.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     255.19 ms /   432 tokens (    0.59 ms per token,  1692.84 tokens per second)\n",
      "llama_print_timings:        eval time =    1016.94 ms /    54 runs   (   18.83 ms per token,    53.10 tokens per second)\n",
      "llama_print_timings:       total time =    1814.09 ms /   486 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     227.41 ms /    33 runs   (    6.89 ms per token,   145.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1469.12 ms /  2313 tokens (    0.64 ms per token,  1574.41 tokens per second)\n",
      "llama_print_timings:        eval time =     671.13 ms /    32 runs   (   20.97 ms per token,    47.68 tokens per second)\n",
      "llama_print_timings:       total time =    2475.51 ms /  2345 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     906.89 ms /   136 runs   (    6.67 ms per token,   149.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     750.11 ms /  1252 tokens (    0.60 ms per token,  1669.10 tokens per second)\n",
      "llama_print_timings:        eval time =    2709.97 ms /   135 runs   (   20.07 ms per token,    49.82 tokens per second)\n",
      "llama_print_timings:       total time =    4774.01 ms /  1387 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     144.95 ms /    21 runs   (    6.90 ms per token,   144.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1162.55 ms /  1882 tokens (    0.62 ms per token,  1618.86 tokens per second)\n",
      "llama_print_timings:        eval time =     406.45 ms /    20 runs   (   20.32 ms per token,    49.21 tokens per second)\n",
      "llama_print_timings:       total time =    1783.30 ms /  1902 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     144.53 ms /    21 runs   (    6.88 ms per token,   145.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1169.30 ms /  1915 tokens (    0.61 ms per token,  1637.73 tokens per second)\n",
      "llama_print_timings:        eval time =     419.80 ms /    20 runs   (   20.99 ms per token,    47.64 tokens per second)\n",
      "llama_print_timings:       total time =    1802.24 ms /  1935 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     150.18 ms /    22 runs   (    6.83 ms per token,   146.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1014.22 ms /  1551 tokens (    0.65 ms per token,  1529.25 tokens per second)\n",
      "llama_print_timings:        eval time =     420.30 ms /    21 runs   (   20.01 ms per token,    49.96 tokens per second)\n",
      "llama_print_timings:       total time =    1655.64 ms /  1572 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     145.87 ms /    21 runs   (    6.95 ms per token,   143.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     889.98 ms /  1527 tokens (    0.58 ms per token,  1715.76 tokens per second)\n",
      "llama_print_timings:        eval time =     400.04 ms /    20 runs   (   20.00 ms per token,    49.99 tokens per second)\n",
      "llama_print_timings:       total time =    1502.47 ms /  1547 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     150.05 ms /    22 runs   (    6.82 ms per token,   146.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1015.15 ms /  1552 tokens (    0.65 ms per token,  1528.84 tokens per second)\n",
      "llama_print_timings:        eval time =     419.78 ms /    21 runs   (   19.99 ms per token,    50.03 tokens per second)\n",
      "llama_print_timings:       total time =    1655.57 ms /  1573 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     144.77 ms /    21 runs   (    6.89 ms per token,   145.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     890.59 ms /  1528 tokens (    0.58 ms per token,  1715.71 tokens per second)\n",
      "llama_print_timings:        eval time =     400.34 ms /    20 runs   (   20.02 ms per token,    49.96 tokens per second)\n",
      "llama_print_timings:       total time =    1501.72 ms /  1548 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     144.48 ms /    21 runs   (    6.88 ms per token,   145.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1168.10 ms /  1915 tokens (    0.61 ms per token,  1639.41 tokens per second)\n",
      "llama_print_timings:        eval time =     419.66 ms /    20 runs   (   20.98 ms per token,    47.66 tokens per second)\n",
      "llama_print_timings:       total time =    1800.88 ms /  1935 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     144.36 ms /    21 runs   (    6.87 ms per token,   145.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1174.67 ms /  1913 tokens (    0.61 ms per token,  1628.54 tokens per second)\n",
      "llama_print_timings:        eval time =     419.66 ms /    20 runs   (   20.98 ms per token,    47.66 tokens per second)\n",
      "llama_print_timings:       total time =    1807.38 ms /  1933 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     151.31 ms /    22 runs   (    6.88 ms per token,   145.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1200.82 ms /  1981 tokens (    0.61 ms per token,  1649.71 tokens per second)\n",
      "llama_print_timings:        eval time =     442.46 ms /    21 runs   (   21.07 ms per token,    47.46 tokens per second)\n",
      "llama_print_timings:       total time =    1866.96 ms /  2002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     201.06 ms /    29 runs   (    6.93 ms per token,   144.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     559.96 ms /   988 tokens (    0.57 ms per token,  1764.42 tokens per second)\n",
      "llama_print_timings:        eval time =     549.45 ms /    28 runs   (   19.62 ms per token,    50.96 tokens per second)\n",
      "llama_print_timings:       total time =    1397.94 ms /  1016 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     219.67 ms /    32 runs   (    6.86 ms per token,   145.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     542.52 ms /   924 tokens (    0.59 ms per token,  1703.15 tokens per second)\n",
      "llama_print_timings:        eval time =     605.42 ms /    31 runs   (   19.53 ms per token,    51.20 tokens per second)\n",
      "llama_print_timings:       total time =    1463.35 ms /   955 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     156.78 ms /    23 runs   (    6.82 ms per token,   146.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     508.37 ms /   831 tokens (    0.61 ms per token,  1634.65 tokens per second)\n",
      "llama_print_timings:        eval time =     425.80 ms /    22 runs   (   19.35 ms per token,    51.67 tokens per second)\n",
      "llama_print_timings:       total time =    1159.70 ms /   853 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     200.03 ms /    29 runs   (    6.90 ms per token,   144.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1170.82 ms /  1918 tokens (    0.61 ms per token,  1638.17 tokens per second)\n",
      "llama_print_timings:        eval time =     586.98 ms /    28 runs   (   20.96 ms per token,    47.70 tokens per second)\n",
      "llama_print_timings:       total time =    2051.65 ms /  1946 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     153.24 ms /    22 runs   (    6.97 ms per token,   143.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     253.57 ms /   426 tokens (    0.60 ms per token,  1679.99 tokens per second)\n",
      "llama_print_timings:        eval time =     394.94 ms /    21 runs   (   18.81 ms per token,    53.17 tokens per second)\n",
      "llama_print_timings:       total time =     865.81 ms /   447 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     151.16 ms /    22 runs   (    6.87 ms per token,   145.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1081.57 ms /  1738 tokens (    0.62 ms per token,  1606.92 tokens per second)\n",
      "llama_print_timings:        eval time =     434.18 ms /    21 runs   (   20.68 ms per token,    48.37 tokens per second)\n",
      "llama_print_timings:       total time =    1738.85 ms /  1759 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     201.63 ms /    29 runs   (    6.95 ms per token,   143.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1200.45 ms /  1980 tokens (    0.61 ms per token,  1649.38 tokens per second)\n",
      "llama_print_timings:        eval time =     588.45 ms /    28 runs   (   21.02 ms per token,    47.58 tokens per second)\n",
      "llama_print_timings:       total time =    2085.51 ms /  2008 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     158.62 ms /    23 runs   (    6.90 ms per token,   145.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     218.66 ms /   336 tokens (    0.65 ms per token,  1536.60 tokens per second)\n",
      "llama_print_timings:        eval time =     409.54 ms /    22 runs   (   18.62 ms per token,    53.72 tokens per second)\n",
      "llama_print_timings:       total time =     853.25 ms /   358 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     270.09 ms /    39 runs   (    6.93 ms per token,   144.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     167.35 ms /   256 tokens (    0.65 ms per token,  1529.72 tokens per second)\n",
      "llama_print_timings:        eval time =     704.73 ms /    38 runs   (   18.55 ms per token,    53.92 tokens per second)\n",
      "llama_print_timings:       total time =    1253.09 ms /   294 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     158.01 ms /    23 runs   (    6.87 ms per token,   145.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.29 ms /   251 tokens (    0.66 ms per token,  1509.40 tokens per second)\n",
      "llama_print_timings:        eval time =     407.67 ms /    22 runs   (   18.53 ms per token,    53.97 tokens per second)\n",
      "llama_print_timings:       total time =     797.31 ms /   273 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     269.53 ms /    39 runs   (    6.91 ms per token,   144.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     200.26 ms /   257 tokens (    0.78 ms per token,  1283.34 tokens per second)\n",
      "llama_print_timings:        eval time =     705.04 ms /    38 runs   (   18.55 ms per token,    53.90 tokens per second)\n",
      "llama_print_timings:       total time =    1285.74 ms /   295 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     158.74 ms /    23 runs   (    6.90 ms per token,   144.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.13 ms /   250 tokens (    0.66 ms per token,  1504.87 tokens per second)\n",
      "llama_print_timings:        eval time =     408.00 ms /    22 runs   (   18.55 ms per token,    53.92 tokens per second)\n",
      "llama_print_timings:       total time =     799.42 ms /   272 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     157.62 ms /    23 runs   (    6.85 ms per token,   145.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     245.25 ms /   392 tokens (    0.63 ms per token,  1598.36 tokens per second)\n",
      "llama_print_timings:        eval time =     413.09 ms /    22 runs   (   18.78 ms per token,    53.26 tokens per second)\n",
      "llama_print_timings:       total time =     881.88 ms /   414 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     160.17 ms /    23 runs   (    6.96 ms per token,   143.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.83 ms /   253 tokens (    0.66 ms per token,  1516.51 tokens per second)\n",
      "llama_print_timings:        eval time =     408.10 ms /    22 runs   (   18.55 ms per token,    53.91 tokens per second)\n",
      "llama_print_timings:       total time =     802.66 ms /   275 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     153.43 ms /    22 runs   (    6.97 ms per token,   143.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.88 ms /   253 tokens (    0.66 ms per token,  1516.07 tokens per second)\n",
      "llama_print_timings:        eval time =     389.48 ms /    21 runs   (   18.55 ms per token,    53.92 tokens per second)\n",
      "llama_print_timings:       total time =     773.28 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     153.57 ms /    22 runs   (    6.98 ms per token,   143.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.78 ms /   253 tokens (    0.66 ms per token,  1516.95 tokens per second)\n",
      "llama_print_timings:        eval time =     389.23 ms /    21 runs   (   18.53 ms per token,    53.95 tokens per second)\n",
      "llama_print_timings:       total time =     773.24 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     158.59 ms /    23 runs   (    6.90 ms per token,   145.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     167.17 ms /   254 tokens (    0.66 ms per token,  1519.45 tokens per second)\n",
      "llama_print_timings:        eval time =     407.78 ms /    22 runs   (   18.54 ms per token,    53.95 tokens per second)\n",
      "llama_print_timings:       total time =     800.33 ms /   276 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     161.96 ms /    23 runs   (    7.04 ms per token,   142.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.28 ms /   217 tokens (    0.77 ms per token,  1304.99 tokens per second)\n",
      "llama_print_timings:        eval time =     408.18 ms /    22 runs   (   18.55 ms per token,    53.90 tokens per second)\n",
      "llama_print_timings:       total time =     804.72 ms /   239 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     159.12 ms /    23 runs   (    6.92 ms per token,   144.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.59 ms /   252 tokens (    0.66 ms per token,  1512.69 tokens per second)\n",
      "llama_print_timings:        eval time =     407.80 ms /    22 runs   (   18.54 ms per token,    53.95 tokens per second)\n",
      "llama_print_timings:       total time =     800.24 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     270.68 ms /    39 runs   (    6.94 ms per token,   144.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.51 ms /   251 tokens (    0.66 ms per token,  1507.43 tokens per second)\n",
      "llama_print_timings:        eval time =     705.00 ms /    38 runs   (   18.55 ms per token,    53.90 tokens per second)\n",
      "llama_print_timings:       total time =    1254.61 ms /   289 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     160.47 ms /    23 runs   (    6.98 ms per token,   143.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.77 ms /   253 tokens (    0.66 ms per token,  1517.07 tokens per second)\n",
      "llama_print_timings:        eval time =     407.99 ms /    22 runs   (   18.54 ms per token,    53.92 tokens per second)\n",
      "llama_print_timings:       total time =     802.82 ms /   275 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     161.19 ms /    23 runs   (    7.01 ms per token,   142.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.75 ms /   252 tokens (    0.66 ms per token,  1511.23 tokens per second)\n",
      "llama_print_timings:        eval time =     408.36 ms /    22 runs   (   18.56 ms per token,    53.87 tokens per second)\n",
      "llama_print_timings:       total time =     805.05 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     158.86 ms /    23 runs   (    6.91 ms per token,   144.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     167.06 ms /   254 tokens (    0.66 ms per token,  1520.40 tokens per second)\n",
      "llama_print_timings:        eval time =     407.91 ms /    22 runs   (   18.54 ms per token,    53.93 tokens per second)\n",
      "llama_print_timings:       total time =     801.01 ms /   276 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     158.31 ms /    23 runs   (    6.88 ms per token,   145.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.08 ms /   339 tokens (    0.65 ms per token,  1547.39 tokens per second)\n",
      "llama_print_timings:        eval time =     409.77 ms /    22 runs   (   18.63 ms per token,    53.69 tokens per second)\n",
      "llama_print_timings:       total time =     853.34 ms /   361 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     161.59 ms /    23 runs   (    7.03 ms per token,   142.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     218.81 ms /   338 tokens (    0.65 ms per token,  1544.69 tokens per second)\n",
      "llama_print_timings:        eval time =     410.03 ms /    22 runs   (   18.64 ms per token,    53.65 tokens per second)\n",
      "llama_print_timings:       total time =     859.10 ms /   360 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     159.44 ms /    23 runs   (    6.93 ms per token,   144.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.23 ms /    10 tokens (    8.72 ms per token,   114.65 tokens per second)\n",
      "llama_print_timings:        eval time =     409.48 ms /    22 runs   (   18.61 ms per token,    53.73 tokens per second)\n",
      "llama_print_timings:       total time =     723.99 ms /    32 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     152.82 ms /    22 runs   (    6.95 ms per token,   143.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.75 ms /   342 tokens (    0.64 ms per token,  1556.31 tokens per second)\n",
      "llama_print_timings:        eval time =     390.82 ms /    21 runs   (   18.61 ms per token,    53.73 tokens per second)\n",
      "llama_print_timings:       total time =     826.43 ms /   363 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     242.11 ms /    35 runs   (    6.92 ms per token,   144.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.73 ms /   252 tokens (    0.66 ms per token,  1511.41 tokens per second)\n",
      "llama_print_timings:        eval time =     630.38 ms /    34 runs   (   18.54 ms per token,    53.94 tokens per second)\n",
      "llama_print_timings:       total time =    1139.68 ms /   286 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     159.53 ms /    23 runs   (    6.94 ms per token,   144.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     167.00 ms /   253 tokens (    0.66 ms per token,  1515.01 tokens per second)\n",
      "llama_print_timings:        eval time =     407.97 ms /    22 runs   (   18.54 ms per token,    53.93 tokens per second)\n",
      "llama_print_timings:       total time =     800.88 ms /   275 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     229.35 ms /    33 runs   (    6.95 ms per token,   143.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     166.15 ms /   249 tokens (    0.67 ms per token,  1498.66 tokens per second)\n",
      "llama_print_timings:        eval time =     593.27 ms /    32 runs   (   18.54 ms per token,    53.94 tokens per second)\n",
      "llama_print_timings:       total time =    1083.19 ms /   281 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     234.64 ms /    34 runs   (    6.90 ms per token,   144.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     164.57 ms /   244 tokens (    0.67 ms per token,  1482.64 tokens per second)\n",
      "llama_print_timings:        eval time =     611.73 ms /    33 runs   (   18.54 ms per token,    53.95 tokens per second)\n",
      "llama_print_timings:       total time =    1108.30 ms /   277 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     254.95 ms /    37 runs   (    6.89 ms per token,   145.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     395.24 ms /   573 tokens (    0.69 ms per token,  1449.74 tokens per second)\n",
      "llama_print_timings:        eval time =     682.08 ms /    36 runs   (   18.95 ms per token,    52.78 tokens per second)\n",
      "llama_print_timings:       total time =    1440.35 ms /   609 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     246.04 ms /    36 runs   (    6.83 ms per token,   146.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     246.35 ms /   398 tokens (    0.62 ms per token,  1615.61 tokens per second)\n",
      "llama_print_timings:        eval time =     657.57 ms /    35 runs   (   18.79 ms per token,    53.23 tokens per second)\n",
      "llama_print_timings:       total time =    1253.13 ms /   433 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     255.44 ms /    37 runs   (    6.90 ms per token,   144.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     396.17 ms /   576 tokens (    0.69 ms per token,  1453.91 tokens per second)\n",
      "llama_print_timings:        eval time =     682.01 ms /    36 runs   (   18.94 ms per token,    52.78 tokens per second)\n",
      "llama_print_timings:       total time =    1441.95 ms /   612 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     229.22 ms /    33 runs   (    6.95 ms per token,   143.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     395.76 ms /   575 tokens (    0.69 ms per token,  1452.89 tokens per second)\n",
      "llama_print_timings:        eval time =     605.97 ms /    32 runs   (   18.94 ms per token,    52.81 tokens per second)\n",
      "llama_print_timings:       total time =    1327.59 ms /   607 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     254.90 ms /    37 runs   (    6.89 ms per token,   145.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     395.85 ms /   576 tokens (    0.69 ms per token,  1455.08 tokens per second)\n",
      "llama_print_timings:        eval time =     681.87 ms /    36 runs   (   18.94 ms per token,    52.80 tokens per second)\n",
      "llama_print_timings:       total time =    1441.06 ms /   612 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     234.23 ms /    34 runs   (    6.89 ms per token,   145.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     395.02 ms /   573 tokens (    0.69 ms per token,  1450.57 tokens per second)\n",
      "llama_print_timings:        eval time =     625.36 ms /    33 runs   (   18.95 ms per token,    52.77 tokens per second)\n",
      "llama_print_timings:       total time =    1354.45 ms /   606 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     255.38 ms /    37 runs   (    6.90 ms per token,   144.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     395.38 ms /   574 tokens (    0.69 ms per token,  1451.78 tokens per second)\n",
      "llama_print_timings:        eval time =     682.02 ms /    36 runs   (   18.94 ms per token,    52.78 tokens per second)\n",
      "llama_print_timings:       total time =    1440.88 ms /   610 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     237.00 ms /    34 runs   (    6.97 ms per token,   143.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     247.40 ms /   402 tokens (    0.62 ms per token,  1624.88 tokens per second)\n",
      "llama_print_timings:        eval time =     619.69 ms /    33 runs   (   18.78 ms per token,    53.25 tokens per second)\n",
      "llama_print_timings:       total time =    1202.23 ms /   435 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     159.80 ms /    23 runs   (    6.95 ms per token,   143.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     393.96 ms /   567 tokens (    0.69 ms per token,  1439.23 tokens per second)\n",
      "llama_print_timings:        eval time =     416.71 ms /    22 runs   (   18.94 ms per token,    52.79 tokens per second)\n",
      "llama_print_timings:       total time =    1039.60 ms /   589 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     159.25 ms /    23 runs   (    6.92 ms per token,   144.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     394.06 ms /   568 tokens (    0.69 ms per token,  1441.42 tokens per second)\n",
      "llama_print_timings:        eval time =     416.55 ms /    22 runs   (   18.93 ms per token,    52.82 tokens per second)\n",
      "llama_print_timings:       total time =    1038.36 ms /   590 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     162.66 ms /    23 runs   (    7.07 ms per token,   141.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     232.75 ms /   377 tokens (    0.62 ms per token,  1619.76 tokens per second)\n",
      "llama_print_timings:        eval time =     412.94 ms /    22 runs   (   18.77 ms per token,    53.28 tokens per second)\n",
      "llama_print_timings:       total time =     876.52 ms /   399 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     160.21 ms /    23 runs   (    6.97 ms per token,   143.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     393.48 ms /   566 tokens (    0.70 ms per token,  1438.45 tokens per second)\n",
      "llama_print_timings:        eval time =     416.95 ms /    22 runs   (   18.95 ms per token,    52.76 tokens per second)\n",
      "llama_print_timings:       total time =    1040.35 ms /   588 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     164.06 ms /    23 runs   (    7.13 ms per token,   140.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.92 ms /   318 tokens (    0.68 ms per token,  1479.65 tokens per second)\n",
      "llama_print_timings:        eval time =     409.33 ms /    22 runs   (   18.61 ms per token,    53.75 tokens per second)\n",
      "llama_print_timings:       total time =     857.66 ms /   340 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     160.67 ms /    23 runs   (    6.99 ms per token,   143.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.27 ms /    18 tokens (    7.35 ms per token,   136.08 tokens per second)\n",
      "llama_print_timings:        eval time =     409.31 ms /    22 runs   (   18.61 ms per token,    53.75 tokens per second)\n",
      "llama_print_timings:       total time =     769.94 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     164.63 ms /    23 runs   (    7.16 ms per token,   139.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.18 ms /   320 tokens (    0.67 ms per token,  1487.13 tokens per second)\n",
      "llama_print_timings:        eval time =     409.68 ms /    22 runs   (   18.62 ms per token,    53.70 tokens per second)\n",
      "llama_print_timings:       total time =     859.05 ms /   342 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     165.45 ms /    24 runs   (    6.89 ms per token,   145.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     273.59 ms /   493 tokens (    0.55 ms per token,  1801.95 tokens per second)\n",
      "llama_print_timings:        eval time =     434.17 ms /    23 runs   (   18.88 ms per token,    52.97 tokens per second)\n",
      "llama_print_timings:       total time =     943.99 ms /   516 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     164.50 ms /    24 runs   (    6.85 ms per token,   145.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     437.93 ms /   533 tokens (    0.82 ms per token,  1217.09 tokens per second)\n",
      "llama_print_timings:        eval time =     435.79 ms /    23 runs   (   18.95 ms per token,    52.78 tokens per second)\n",
      "llama_print_timings:       total time =    1108.99 ms /   556 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     161.47 ms /    23 runs   (    7.02 ms per token,   142.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     394.46 ms /   569 tokens (    0.69 ms per token,  1442.48 tokens per second)\n",
      "llama_print_timings:        eval time =     416.70 ms /    22 runs   (   18.94 ms per token,    52.80 tokens per second)\n",
      "llama_print_timings:       total time =    1041.52 ms /   591 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     166.86 ms /    24 runs   (    6.95 ms per token,   143.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     394.79 ms /   571 tokens (    0.69 ms per token,  1446.34 tokens per second)\n",
      "llama_print_timings:        eval time =     435.84 ms /    23 runs   (   18.95 ms per token,    52.77 tokens per second)\n",
      "llama_print_timings:       total time =    1069.56 ms /   594 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     161.90 ms /    23 runs   (    7.04 ms per token,   142.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     273.15 ms /   491 tokens (    0.56 ms per token,  1797.53 tokens per second)\n",
      "llama_print_timings:        eval time =     415.15 ms /    22 runs   (   18.87 ms per token,    52.99 tokens per second)\n",
      "llama_print_timings:       total time =     917.14 ms /   513 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     160.26 ms /    23 runs   (    6.97 ms per token,   143.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.09 ms /   320 tokens (    0.67 ms per token,  1487.76 tokens per second)\n",
      "llama_print_timings:        eval time =     409.11 ms /    22 runs   (   18.60 ms per token,    53.78 tokens per second)\n",
      "llama_print_timings:       total time =     850.74 ms /   342 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     161.76 ms /    23 runs   (    7.03 ms per token,   142.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.89 ms /   319 tokens (    0.67 ms per token,  1484.48 tokens per second)\n",
      "llama_print_timings:        eval time =     409.22 ms /    22 runs   (   18.60 ms per token,    53.76 tokens per second)\n",
      "llama_print_timings:       total time =     851.91 ms /   341 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     158.94 ms /    23 runs   (    6.91 ms per token,   144.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     272.65 ms /   489 tokens (    0.56 ms per token,  1793.50 tokens per second)\n",
      "llama_print_timings:        eval time =     414.92 ms /    22 runs   (   18.86 ms per token,    53.02 tokens per second)\n",
      "llama_print_timings:       total time =     912.31 ms /   511 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     159.44 ms /    23 runs   (    6.93 ms per token,   144.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.43 ms /   318 tokens (    0.67 ms per token,  1483.04 tokens per second)\n",
      "llama_print_timings:        eval time =     408.85 ms /    22 runs   (   18.58 ms per token,    53.81 tokens per second)\n",
      "llama_print_timings:       total time =     848.12 ms /   340 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     214.66 ms /    31 runs   (    6.92 ms per token,   144.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     249.66 ms /   408 tokens (    0.61 ms per token,  1634.20 tokens per second)\n",
      "llama_print_timings:        eval time =     563.75 ms /    30 runs   (   18.79 ms per token,    53.22 tokens per second)\n",
      "llama_print_timings:       total time =    1117.66 ms /   438 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     213.80 ms /    31 runs   (    6.90 ms per token,   145.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     250.21 ms /   412 tokens (    0.61 ms per token,  1646.62 tokens per second)\n",
      "llama_print_timings:        eval time =     563.65 ms /    30 runs   (   18.79 ms per token,    53.22 tokens per second)\n",
      "llama_print_timings:       total time =    1116.11 ms /   442 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     213.98 ms /    31 runs   (    6.90 ms per token,   144.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     249.75 ms /   409 tokens (    0.61 ms per token,  1637.64 tokens per second)\n",
      "llama_print_timings:        eval time =     563.87 ms /    30 runs   (   18.80 ms per token,    53.20 tokens per second)\n",
      "llama_print_timings:       total time =    1116.18 ms /   439 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     229.84 ms /    33 runs   (    6.96 ms per token,   143.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     248.63 ms /   406 tokens (    0.61 ms per token,  1632.93 tokens per second)\n",
      "llama_print_timings:        eval time =     601.35 ms /    32 runs   (   18.79 ms per token,    53.21 tokens per second)\n",
      "llama_print_timings:       total time =    1174.85 ms /   438 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     235.35 ms /    34 runs   (    6.92 ms per token,   144.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     247.18 ms /   401 tokens (    0.62 ms per token,  1622.33 tokens per second)\n",
      "llama_print_timings:        eval time =     620.12 ms /    33 runs   (   18.79 ms per token,    53.22 tokens per second)\n",
      "llama_print_timings:       total time =    1199.59 ms /   434 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     106.43 ms\n",
      "llama_print_timings:      sample time =     230.29 ms /    33 runs   (    6.98 ms per token,   143.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     246.34 ms /   398 tokens (    0.62 ms per token,  1615.63 tokens per second)\n",
      "llama_print_timings:        eval time =     601.44 ms /    32 runs   (   18.79 ms per token,    53.21 tokens per second)\n",
      "llama_print_timings:       total time =    1173.45 ms /   430 tokens\n"
     ]
    }
   ],
   "source": [
    "limit_rows = -1 # 20\n",
    "selected_devices = None # ['Smoke', 'Humidity']\n",
    "\n",
    "df = pd.read_csv('data/datasets/dataset_v2.csv')\n",
    "devices = list(df['device'].unique())\n",
    "if selected_devices:\n",
    "    df = df[df['device'].isin(selected_devices)].sort_index()\n",
    "    # print(df)\n",
    "\n",
    "output_df = pd.DataFrame(columns=['id', 'device', 'user_cmd', 'mtd', 'json_cmd'])\n",
    "for i, row in df.iterrows():\n",
    "    # if i > 50: # or i > 16:\n",
    "    #     continue\n",
    "    user_cmd = row['user_cmd']\n",
    "\n",
    "    device = row['device']\n",
    "    sample_devices = devices.copy()\n",
    "    sample_devices.remove(device)\n",
    "    sample_devices = random.sample(sample_devices, k=2)\n",
    "    env = f'{sample_devices[0]} id=1, {sample_devices[1]} id=2, {device} id=444'\n",
    "\n",
    "    retrieval_prompt = \"Represent this sentence for searching relevant passages: \" + user_cmd\n",
    "    retrieved_nodes = retriever.retrieve(retrieval_prompt)\n",
    "    retrieved_nodes = retrieved_nodes[:2]\n",
    "    methods_names = []\n",
    "    methods_description = ''\n",
    "    for k, node in enumerate(retrieved_nodes):\n",
    "        methods_description += f'API method {k}:\\n{node.text}\\n\\n'\n",
    "        method_name = node.metadata['file_name'].replace('.md', '')\n",
    "        methods_names.append(method_name)\n",
    "    methods_names = ','.join(methods_names)\n",
    "    methods_description = methods_description.strip('\\n')\n",
    "    \n",
    "\n",
    "    user_prompt = user_prompt_template.format(**{'env': env, \n",
    "                                                 'methods_description': methods_description, \n",
    "                                                 'user_cmd': user_cmd})\n",
    "    llm_prompt = base_prompt + '\\n\\n' + user_prompt\n",
    "\n",
    "    try:\n",
    "        response = llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': llm_prompt}\n",
    "            ],\n",
    "            grammar=llama_grammar\n",
    "        )\n",
    "        \n",
    "        response_text = response['choices'][0]['message']['content']\n",
    "        response_text = response_text.replace('\\_', '_')\n",
    "        json_cmd = json.dumps(json.loads(response_text))\n",
    "    except Exception as ex:\n",
    "        print(response_text)\n",
    "        print(ex)\n",
    "        continue\n",
    "\n",
    "    output_df.loc[len(output_df)] = pd.Series({'id': row['id'], 'device': row['device'], 'user_cmd': user_cmd, 'mtd': methods_names, 'json_cmd': json_cmd})\n",
    "\n",
    "    # print(user_cmd)\n",
    "    # print(response_text)\n",
    "\n",
    "    if limit_rows > 0 and i == limit_rows - 1:\n",
    "        break\n",
    "\n",
    "output_num = list(sorted([int(d.name.replace('output', '')) for d in list(OUTPUT_DIR.iterdir())]))[-1] + 1\n",
    "CUR_OUTPUT_DIR = OUTPUT_DIR / f'output{output_num}'\n",
    "CUR_OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "output_df.to_csv(CUR_OUTPUT_DIR / 'output.csv', index=False, header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>device</th>\n",
       "      <th>user_cmd</th>\n",
       "      <th>mtd</th>\n",
       "      <th>json_cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>192</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>Could you lower the temperature to 18 degrees ...</td>\n",
       "      <td>Temperature.SetTemperature</td>\n",
       "      <td>{\"method\": \"Temperature.SetTemperature\", \"para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>193</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>Set the temperature to 20 degrees in the bedroom.</td>\n",
       "      <td>Temperature.SetTemperature</td>\n",
       "      <td>{\"method\": \"Temperature.SetTemperature\", \"para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>194</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>Can you increase the humidity in the living ro...</td>\n",
       "      <td>Humidity.SetHumidity</td>\n",
       "      <td>{\"method\": \"Humidity.SetHumidity\", \"params\": {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>Set the humidity to 50% in the bedroom.</td>\n",
       "      <td>Humidity.SetHumidity</td>\n",
       "      <td>{\"method\": \"Humidity.SetHumidity\", \"params\": {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>Decrease the humidity in the kitchen to 40 per...</td>\n",
       "      <td>Humidity.SetHumidity</td>\n",
       "      <td>{\"method\": \"Humidity.SetHumidity\", \"params\": {...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       device                                           user_cmd  \\\n",
       "192  192  Temperature  Could you lower the temperature to 18 degrees ...   \n",
       "193  193  Temperature  Set the temperature to 20 degrees in the bedroom.   \n",
       "194  194     Humidity  Can you increase the humidity in the living ro...   \n",
       "195  195     Humidity            Set the humidity to 50% in the bedroom.   \n",
       "196  196     Humidity  Decrease the humidity in the kitchen to 40 per...   \n",
       "\n",
       "                            mtd  \\\n",
       "192  Temperature.SetTemperature   \n",
       "193  Temperature.SetTemperature   \n",
       "194        Humidity.SetHumidity   \n",
       "195        Humidity.SetHumidity   \n",
       "196        Humidity.SetHumidity   \n",
       "\n",
       "                                              json_cmd  \n",
       "192  {\"method\": \"Temperature.SetTemperature\", \"para...  \n",
       "193  {\"method\": \"Temperature.SetTemperature\", \"para...  \n",
       "194  {\"method\": \"Humidity.SetHumidity\", \"params\": {...  \n",
       "195  {\"method\": \"Humidity.SetHumidity\", \"params\": {...  \n",
       "196  {\"method\": \"Humidity.SetHumidity\", \"params\": {...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_additional_parameters(gt_json, pred_json):\n",
    "    if len(set(pred_json.keys()) - set(gt_json.keys())) > 0:\n",
    "        return True\n",
    "    for key, val in gt_json.items():\n",
    "        if key not in pred_json:\n",
    "            continue\n",
    "        if type(val) == dict:\n",
    "            if check_additional_parameters(val, pred_json[key]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def check_incorrect_parameters(gt_json, pred_json):\n",
    "    for key, val in gt_json.items():\n",
    "        if key not in pred_json:\n",
    "            return True\n",
    "        if type(val) == dict:\n",
    "            if check_incorrect_parameters(val, pred_json[key]):\n",
    "                return True\n",
    "        elif pred_json[key] != val:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "CUR_OUTPUT_DIR = Path('output/output4')\n",
    "output_df = pd.read_csv(CUR_OUTPUT_DIR / 'output.csv')\n",
    "merged_df = df.merge(output_df, how='inner', on='id', suffixes=(\"_gt\", \"_pred\"))\n",
    "\n",
    "compared_output_df = pd.DataFrame(columns=['device', 'user_cmd', 'gt_mtd', 'pred_mtd', 'gt_json_cmd', 'pred_json_cmd', 'method', 'add_params', 'inc_params'])\n",
    "for _, row in merged_df.iterrows():\n",
    "    correct_ouput = True\n",
    "    methods_names_pred = row['mtd_pred'].split(',')\n",
    "    try:\n",
    "        gt_json = json.loads(row['json_cmd_gt'])\n",
    "        pred_json = json.loads(row['json_cmd_pred'])\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(row['id'])\n",
    "        print(row['json_cmd_gt'])\n",
    "        print(row['json_cmd_pred'])\n",
    "        continue\n",
    "    compared_dict = {'device': row['device_gt'],\n",
    "        'user_cmd': row['user_cmd_gt'], 'gt_mtd': row['mtd_gt'],\n",
    "        'pred_mtd': row['mtd_pred'], 'gt_json_cmd': row['json_cmd_gt'], 'pred_json_cmd': row['json_cmd_pred']}\n",
    "    \n",
    "    compared_dict['method'] = row['mtd_gt'] in methods_names_pred\n",
    "    compared_dict['add_params'] = check_additional_parameters(gt_json, pred_json)\n",
    "    compared_dict['inc_params'] = check_incorrect_parameters(gt_json, pred_json)\n",
    "\n",
    "    compared_output_df.loc[len(compared_output_df)] = pd.Series(compared_dict)\n",
    "\n",
    "compared_output_df.to_csv(CUR_OUTPUT_DIR / 'compared_output.csv', index=False)\n",
    "\n",
    "print(len(df))\n",
    "print(len(merged_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For incorrect retrieved method we mostly have incorrect parameters. Don't need to inspect them further until we fix incorrect parameters.\n",
    "\n",
    "Excluding samples with incorrectly retrieved method, part of samples with additional parameters is relatively small and higher for methods with incorrect parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 590.861x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 596.986x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = compared_output_df\n",
    "\n",
    "sns.catplot(\n",
    "    data=df, y=\"method\", hue=\"inc_params\", kind=\"count\",\n",
    "    palette=\"pastel\", edgecolor=\".6\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "sns.catplot(\n",
    "    data=df[df['method'] == True], y=\"inc_params\", hue=\"add_params\", kind=\"count\",\n",
    "    palette=\"pastel\", edgecolor=\".6\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
