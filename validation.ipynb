{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.embeddings.utils import resolve_embed_model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import Settings\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp import LlamaGrammar\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_METHODS_DIR = Path('data/docs/manual')\n",
    "METHODS_DIR = Path('data/docs/methods')\n",
    "PROMPT_SEEDS_DIR = Path('data/prompts/generation/components')\n",
    "PROMPT_COMPONENTS_DIR = Path('data/prompts/generation/components')\n",
    "VAL_PROMPT_COMPONENTS_DIR = Path('data/prompts/validation/components')\n",
    "GEN_PROMPTS_DIR = Path('data/prompts/generation/output')\n",
    "VAL_PROMPTS_DIR = Path('data/prompts/validation/output')\n",
    "PERSIST_DIR = \"data/persist_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = SimpleDirectoryReader(METHODS_DIR).load_data()\n",
    "\n",
    "# embed_model = resolve_embed_model(\"local:BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, show_progress=True, service_context=service_context\n",
    "# )\n",
    "\n",
    "# index.storage_context.persist(persist_dir=PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080, compute capability 7.5, VMM: yes\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   102.54 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  5563.55 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# llm = Ollama(model=\"mistral\", request_timeout=180.0) # , base_url=\"http://62.80.172.138:11434\"\n",
    "llm = Llama('models/mistral-7b-instruct-v0.2.Q6_K.gguf', n_ctx=2048, verbose=True, n_gpu_layers=-1) # mistral-7b-instruct-v0.2.Q4_0.gguf mistral-7b-instruct-v0.2.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     152.90 ms\n",
      "llama_print_timings:      sample time =      38.31 ms /   100 runs   (    0.38 ms per token,  2610.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.83 ms /    17 tokens (    8.99 ms per token,   111.24 tokens per second)\n",
      "llama_print_timings:        eval time =    1698.72 ms /    99 runs   (   17.16 ms per token,    58.28 tokens per second)\n",
      "llama_print_timings:       total time =    2060.35 ms /   116 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-ec3d3e90-400d-4f88-907f-b229d53e201a',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1710604169,\n",
       " 'model': 'models/mistral-7b-instruct-v0.2.Q6_K.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \" The color of the sky appears blue due to a process called Rayleigh scattering. When sunlight, which is made up of different colors, enters the Earth's atmosphere, it interacts with molecules and particles in the air, such as nitrogen and oxygen. Blue light is scattered more easily than other colors because it travels in smaller, shorter waves. This scattered blue light is what we see when we look up at the sky. Additionally, other factors such as the presence of dust, water\"},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 17, 'completion_tokens': 100, 'total_tokens': 117}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Why is sky blue?\n",
    "\"\"\"\n",
    "llm.create_chat_completion(messages=[{'role': 'user', 'content': prompt}], max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 16 17:52:30 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 23%   30C    P2    52W / 215W |   7236MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1411      G   /usr/bin/gnome-shell                3MiB |\n",
      "|    0   N/A  N/A   3597888      C   ...envs/assistant/bin/python     7218MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_scheme_prompt = {\n",
    "    \"method\": {\n",
    "        \"type\": \"string\"\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"type\": \"object\"\n",
    "    }\n",
    "}\n",
    "\n",
    "example_1_json = {\n",
    "  \"method\":\"Cover.Open\",\n",
    "  \"params\":\n",
    "  {\n",
    "    \"id\":2\n",
    "  }\n",
    "}\n",
    "\n",
    "example_2_json = {\n",
    "  \"method\":\"Cover.Close\",\n",
    "  \"params\":\n",
    "  {\n",
    "    \"id\":0,\n",
    "    \"duration\":5,\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(VAL_PROMPT_COMPONENTS_DIR / 'instruction.md') as f:\n",
    "  instruction = f.read()\n",
    "\n",
    "variables = {\n",
    "    \"instruction\": instruction,\n",
    "    \"json_scheme\": \"The output JSON should follow the next scheme: \" + json.dumps(json_scheme_prompt),\n",
    "    \"devices\": \"\"\"Cover id=1\"\"\",\n",
    "    \"example_1\": \"\"\"Devices: Cover id=2\n",
    "Methods:\n",
    "API method 1:\n",
    "Method name: Cover.Open\n",
    "Method description: Preconditions:\n",
    "Cover will not accept the command if:\n",
    "An  overvoltage  error is set at the time of the request.\n",
    "An  undervoltage  error is set at the time of the request.\n",
    "An  overtemp  error is set at the time of the request.\n",
    "An engaged  safety_switch  prohibits movement in the requested direction.\n",
    "Cover  calibration is running at the time of the request\n",
    "Properties:\n",
    "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully open, unless it times out because of maxtime_open first. If duration (seconds) is provided, Cover will move in the open direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
    "Response:\n",
    "null on success; error if the request can not be executed or failed\n",
    "\n",
    "Command: Open the cover.\n",
    "JSON: \"\"\" + json.dumps(example_1_json),\n",
    "\n",
    "    \"example_2\": \"\"\"Devices: Cover id=0\n",
    "Methods: \n",
    "API method 1:\n",
    "Method name: Cover.Close\n",
    "Method description: Preconditions:\n",
    "Cover will not accept the command if:\n",
    "An  overvoltage  error is set at the time of the request.\n",
    "An  undervoltage  error is set at the time of the request.\n",
    "An  overtemp  error is set at the time of the request.\n",
    "An engaged  safety_switch  prohibits movement in the requested direction.\n",
    "Cover  calibration is running at the time of the request\n",
    "Properties:\n",
    "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully close, unless it times out because of maxtime_close first. If duration (seconds) is provided, Cover will move in the close direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
    "Response:\n",
    "null on success; error if the request can not be executed or failed\n",
    "\n",
    "Command: Close the kitchen cover quickly (for 5 seconds).\n",
    "JSON: \"\"\" + json.dumps(example_2_json),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI Assistant that controls the devices in a house. For a given user command create a corresponding JSON object. Don't add properties with null value in output JSON object. Output must be strictly in JSON format.\n",
      "The output JSON should follow the next scheme: {\"method\": {\"type\": \"string\"}, \"params\": {\"type\": \"object\"}}\n",
      "\n",
      "Devices: Cover id=2\n",
      "Methods:\n",
      "API method 1:\n",
      "Method name: Cover.Open\n",
      "Method description: Preconditions:\n",
      "Cover will not accept the command if:\n",
      "An  overvoltage  error is set at the time of the request.\n",
      "An  undervoltage  error is set at the time of the request.\n",
      "An  overtemp  error is set at the time of the request.\n",
      "An engaged  safety_switch  prohibits movement in the requested direction.\n",
      "Cover  calibration is running at the time of the request\n",
      "Properties:\n",
      "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully open, unless it times out because of maxtime_open first. If duration (seconds) is provided, Cover will move in the open direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
      "Response:\n",
      "null on success; error if the request can not be executed or failed\n",
      "\n",
      "Command: Open the cover.\n",
      "JSON: {\"method\": \"Cover.Open\", \"params\": {\"id\": 2}}\n",
      "\n",
      "Devices: Cover id=0\n",
      "Methods: \n",
      "API method 1:\n",
      "Method name: Cover.Close\n",
      "Method description: Preconditions:\n",
      "Cover will not accept the command if:\n",
      "An  overvoltage  error is set at the time of the request.\n",
      "An  undervoltage  error is set at the time of the request.\n",
      "An  overtemp  error is set at the time of the request.\n",
      "An engaged  safety_switch  prohibits movement in the requested direction.\n",
      "Cover  calibration is running at the time of the request\n",
      "Properties:\n",
      "{\"id\": {\"type\": \"number\", \"description\": \"The numeric ID of the Cover component instance\"}, \"duration\": {\"type\": \"number\", \"description\": \"If duration is not provided, Cover will fully close, unless it times out because of maxtime_close first. If duration (seconds) is provided, Cover will move in the close direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional\"}}\n",
      "Response:\n",
      "null on success; error if the request can not be executed or failed\n",
      "\n",
      "Command: Close the kitchen cover quickly (for 5 seconds).\n",
      "JSON: {\"method\": \"Cover.Close\", \"params\": {\"id\": 0, \"duration\": 5}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_prompt_template = \"\"\"\n",
    "{instruction}\n",
    "{json_scheme}\n",
    "\n",
    "{example_1}\n",
    "\n",
    "{example_2}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_template = \"\"\"Devices: {env}\n",
    "Methods:\n",
    "{methods_description}\n",
    "Command: {user_cmd}\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "base_prompt = base_prompt_template.format(**variables)\n",
    "\n",
    "print(base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "# logger.addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_cpp.llama_grammar.LlamaGrammar object at 0x7f12c07b6620>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] \n",
      "ws_31 ::= ws_30 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/grammars/json.gbnf') as f:\n",
    "    grammar_str = f.read()\n",
    "llama_grammar = LlamaGrammar.from_string(grammar_str)\n",
    "print(llama_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger Q4_K_M model generates more complex output with unnecessary parameters. Possible fix: parameters tuning. \\\n",
    "Whereas Q4_0 model tries to escape `_` character with `\\` inside object property. Possible fix: grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     198.42 ms /    29 runs   (    6.84 ms per token,   146.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     223.24 ms /   303 tokens (    0.74 ms per token,  1357.27 tokens per second)\n",
      "llama_print_timings:        eval time =     515.70 ms /    28 runs   (   18.42 ms per token,    54.30 tokens per second)\n",
      "llama_print_timings:       total time =    1017.60 ms /   331 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     285.99 ms /    42 runs   (    6.81 ms per token,   146.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     547.93 ms /   961 tokens (    0.57 ms per token,  1753.86 tokens per second)\n",
      "llama_print_timings:        eval time =     793.64 ms /    41 runs   (   19.36 ms per token,    51.66 tokens per second)\n",
      "llama_print_timings:       total time =    1754.24 ms /  1002 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     337.20 ms /    51 runs   (    6.61 ms per token,   151.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     548.00 ms /   969 tokens (    0.57 ms per token,  1768.24 tokens per second)\n",
      "llama_print_timings:        eval time =     968.20 ms /    50 runs   (   19.36 ms per token,    51.64 tokens per second)\n",
      "llama_print_timings:       total time =    2000.38 ms /  1019 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     235.39 ms /    33 runs   (    7.13 ms per token,   140.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     695.47 ms /  1138 tokens (    0.61 ms per token,  1636.31 tokens per second)\n",
      "llama_print_timings:        eval time =     627.33 ms /    32 runs   (   19.60 ms per token,    51.01 tokens per second)\n",
      "llama_print_timings:       total time =    1662.06 ms /  1170 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     239.72 ms /    35 runs   (    6.85 ms per token,   146.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     728.49 ms /  1203 tokens (    0.61 ms per token,  1651.36 tokens per second)\n",
      "llama_print_timings:        eval time =     668.47 ms /    34 runs   (   19.66 ms per token,    50.86 tokens per second)\n",
      "llama_print_timings:       total time =    1744.13 ms /  1237 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     251.99 ms /    37 runs   (    6.81 ms per token,   146.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     728.93 ms /  1205 tokens (    0.60 ms per token,  1653.12 tokens per second)\n",
      "llama_print_timings:        eval time =     707.80 ms /    36 runs   (   19.66 ms per token,    50.86 tokens per second)\n",
      "llama_print_timings:       total time =    1797.57 ms /  1241 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     268.14 ms /    39 runs   (    6.88 ms per token,   145.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     742.75 ms /  1265 tokens (    0.59 ms per token,  1703.13 tokens per second)\n",
      "llama_print_timings:        eval time =     753.23 ms /    38 runs   (   19.82 ms per token,    50.45 tokens per second)\n",
      "llama_print_timings:       total time =    1896.94 ms /  1303 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     222.93 ms /    32 runs   (    6.97 ms per token,   143.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     547.98 ms /   965 tokens (    0.57 ms per token,  1761.02 tokens per second)\n",
      "llama_print_timings:        eval time =     601.35 ms /    31 runs   (   19.40 ms per token,    51.55 tokens per second)\n",
      "llama_print_timings:       total time =    1465.59 ms /   996 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     172.84 ms /    25 runs   (    6.91 ms per token,   144.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.65 ms /   129 tokens (    1.07 ms per token,   930.42 tokens per second)\n",
      "llama_print_timings:        eval time =     437.10 ms /    24 runs   (   18.21 ms per token,    54.91 tokens per second)\n",
      "llama_print_timings:       total time =     819.14 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     162.72 ms /    22 runs   (    7.40 ms per token,   135.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.87 ms /   310 tokens (    0.68 ms per token,  1477.08 tokens per second)\n",
      "llama_print_timings:        eval time =     388.14 ms /    21 runs   (   18.48 ms per token,    54.10 tokens per second)\n",
      "llama_print_timings:       total time =     828.14 ms /   331 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     151.02 ms /    22 runs   (    6.86 ms per token,   145.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.74 ms /   328 tokens (    0.65 ms per token,  1527.41 tokens per second)\n",
      "llama_print_timings:        eval time =     387.58 ms /    21 runs   (   18.46 ms per token,    54.18 tokens per second)\n",
      "llama_print_timings:       total time =     814.79 ms /   349 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     160.25 ms /    23 runs   (    6.97 ms per token,   143.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.56 ms /   326 tokens (    0.66 ms per token,  1519.37 tokens per second)\n",
      "llama_print_timings:        eval time =     406.70 ms /    22 runs   (   18.49 ms per token,    54.09 tokens per second)\n",
      "llama_print_timings:       total time =     846.74 ms /   348 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     156.78 ms /    23 runs   (    6.82 ms per token,   146.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     730.88 ms /  1206 tokens (    0.61 ms per token,  1650.05 tokens per second)\n",
      "llama_print_timings:        eval time =     433.08 ms /    22 runs   (   19.69 ms per token,    50.80 tokens per second)\n",
      "llama_print_timings:       total time =    1389.81 ms /  1228 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     378.92 ms /    55 runs   (    6.89 ms per token,   145.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     728.76 ms /  1198 tokens (    0.61 ms per token,  1643.89 tokens per second)\n",
      "llama_print_timings:        eval time =    1064.23 ms /    54 runs   (   19.71 ms per token,    50.74 tokens per second)\n",
      "llama_print_timings:       total time =    2336.19 ms /  1252 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     156.28 ms /    23 runs   (    6.79 ms per token,   147.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.10 ms /   328 tokens (    0.66 ms per token,  1524.89 tokens per second)\n",
      "llama_print_timings:        eval time =     406.03 ms /    22 runs   (   18.46 ms per token,    54.18 tokens per second)\n",
      "llama_print_timings:       total time =     840.79 ms /   350 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     190.52 ms /    28 runs   (    6.80 ms per token,   146.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.69 ms /   309 tokens (    0.68 ms per token,  1473.64 tokens per second)\n",
      "llama_print_timings:        eval time =     498.32 ms /    27 runs   (   18.46 ms per token,    54.18 tokens per second)\n",
      "llama_print_timings:       total time =     976.70 ms /   336 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     150.38 ms /    22 runs   (    6.84 ms per token,   146.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     449.10 ms /   756 tokens (    0.59 ms per token,  1683.36 tokens per second)\n",
      "llama_print_timings:        eval time =     402.01 ms /    21 runs   (   19.14 ms per token,    52.24 tokens per second)\n",
      "llama_print_timings:       total time =    1070.02 ms /   777 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     144.97 ms /    21 runs   (    6.90 ms per token,   144.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     448.47 ms /   756 tokens (    0.59 ms per token,  1685.72 tokens per second)\n",
      "llama_print_timings:        eval time =     382.58 ms /    20 runs   (   19.13 ms per token,    52.28 tokens per second)\n",
      "llama_print_timings:       total time =    1037.82 ms /   776 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     146.44 ms /    21 runs   (    6.97 ms per token,   143.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     450.19 ms /   755 tokens (    0.60 ms per token,  1677.07 tokens per second)\n",
      "llama_print_timings:        eval time =     382.92 ms /    20 runs   (   19.15 ms per token,    52.23 tokens per second)\n",
      "llama_print_timings:       total time =    1041.92 ms /   775 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     144.27 ms /    21 runs   (    6.87 ms per token,   145.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     449.04 ms /   756 tokens (    0.59 ms per token,  1683.58 tokens per second)\n",
      "llama_print_timings:        eval time =     382.65 ms /    20 runs   (   19.13 ms per token,    52.27 tokens per second)\n",
      "llama_print_timings:       total time =    1037.77 ms /   776 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     143.85 ms /    21 runs   (    6.85 ms per token,   145.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     449.05 ms /   756 tokens (    0.59 ms per token,  1683.57 tokens per second)\n",
      "llama_print_timings:        eval time =     382.38 ms /    20 runs   (   19.12 ms per token,    52.30 tokens per second)\n",
      "llama_print_timings:       total time =    1036.30 ms /   776 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     143.73 ms /    21 runs   (    6.84 ms per token,   146.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     451.24 ms /   759 tokens (    0.59 ms per token,  1682.03 tokens per second)\n",
      "llama_print_timings:        eval time =     382.78 ms /    20 runs   (   19.14 ms per token,    52.25 tokens per second)\n",
      "llama_print_timings:       total time =    1038.91 ms /   779 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     144.10 ms /    21 runs   (    6.86 ms per token,   145.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     448.69 ms /   756 tokens (    0.59 ms per token,  1684.89 tokens per second)\n",
      "llama_print_timings:        eval time =     382.60 ms /    20 runs   (   19.13 ms per token,    52.27 tokens per second)\n",
      "llama_print_timings:       total time =    1036.66 ms /   776 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     143.53 ms /    21 runs   (    6.83 ms per token,   146.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     451.28 ms /   759 tokens (    0.59 ms per token,  1681.88 tokens per second)\n",
      "llama_print_timings:        eval time =     382.61 ms /    20 runs   (   19.13 ms per token,    52.27 tokens per second)\n",
      "llama_print_timings:       total time =    1038.70 ms /   779 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     156.29 ms /    23 runs   (    6.80 ms per token,   147.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.38 ms /   272 tokens (    0.74 ms per token,  1350.71 tokens per second)\n",
      "llama_print_timings:        eval time =     405.46 ms /    22 runs   (   18.43 ms per token,    54.26 tokens per second)\n",
      "llama_print_timings:       total time =     829.20 ms /   294 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     165.11 ms /    24 runs   (    6.88 ms per token,   145.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.24 ms /   123 tokens (    1.02 ms per token,   982.11 tokens per second)\n",
      "llama_print_timings:        eval time =     418.92 ms /    23 runs   (   18.21 ms per token,    54.90 tokens per second)\n",
      "llama_print_timings:       total time =     775.67 ms /   146 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     164.40 ms /    24 runs   (    6.85 ms per token,   145.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.73 ms /   121 tokens (    1.03 ms per token,   970.11 tokens per second)\n",
      "llama_print_timings:        eval time =     418.93 ms /    23 runs   (   18.21 ms per token,    54.90 tokens per second)\n",
      "llama_print_timings:       total time =     774.88 ms /   144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     159.61 ms /    23 runs   (    6.94 ms per token,   144.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.28 ms /   124 tokens (    1.01 ms per token,   989.76 tokens per second)\n",
      "llama_print_timings:        eval time =     400.94 ms /    22 runs   (   18.22 ms per token,    54.87 tokens per second)\n",
      "llama_print_timings:       total time =     750.13 ms /   146 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     249.77 ms /    33 runs   (    7.57 ms per token,   132.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     548.98 ms /   965 tokens (    0.57 ms per token,  1757.82 tokens per second)\n",
      "llama_print_timings:        eval time =     622.90 ms /    32 runs   (   19.47 ms per token,    51.37 tokens per second)\n",
      "llama_print_timings:       total time =    1528.73 ms /   997 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     150.70 ms /    21 runs   (    7.18 ms per token,   139.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.46 ms /   305 tokens (    0.69 ms per token,  1456.13 tokens per second)\n",
      "llama_print_timings:        eval time =     370.19 ms /    20 runs   (   18.51 ms per token,    54.03 tokens per second)\n",
      "llama_print_timings:       total time =     793.15 ms /   325 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     184.86 ms /    21 runs   (    8.80 ms per token,   113.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     208.97 ms /   304 tokens (    0.69 ms per token,  1454.73 tokens per second)\n",
      "llama_print_timings:        eval time =     371.55 ms /    20 runs   (   18.58 ms per token,    53.83 tokens per second)\n",
      "llama_print_timings:       total time =     839.22 ms /   324 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     241.91 ms /    31 runs   (    7.80 ms per token,   128.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     696.79 ms /  1130 tokens (    0.62 ms per token,  1621.72 tokens per second)\n",
      "llama_print_timings:        eval time =     592.12 ms /    30 runs   (   19.74 ms per token,    50.67 tokens per second)\n",
      "llama_print_timings:       total time =    1642.69 ms /  1160 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     315.96 ms /    42 runs   (    7.52 ms per token,   132.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     697.06 ms /  1131 tokens (    0.62 ms per token,  1622.53 tokens per second)\n",
      "llama_print_timings:        eval time =     809.93 ms /    41 runs   (   19.75 ms per token,    50.62 tokens per second)\n",
      "llama_print_timings:       total time =    1967.34 ms /  1172 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     317.87 ms /    45 runs   (    7.06 ms per token,   141.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.36 ms /   477 tokens (    0.56 ms per token,  1797.53 tokens per second)\n",
      "llama_print_timings:        eval time =     824.53 ms /    44 runs   (   18.74 ms per token,    53.36 tokens per second)\n",
      "llama_print_timings:       total time =    1540.72 ms /   521 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     263.30 ms /    39 runs   (    6.75 ms per token,   148.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     695.55 ms /  1131 tokens (    0.61 ms per token,  1626.06 tokens per second)\n",
      "llama_print_timings:        eval time =     747.60 ms /    38 runs   (   19.67 ms per token,    50.83 tokens per second)\n",
      "llama_print_timings:       total time =    1822.98 ms /  1169 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     436.56 ms /    61 runs   (    7.16 ms per token,   139.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.32 ms /   479 tokens (    0.56 ms per token,  1798.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1127.12 ms /    60 runs   (   18.79 ms per token,    53.23 tokens per second)\n",
      "llama_print_timings:       total time =    2011.99 ms /   539 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     145.21 ms /    21 runs   (    6.91 ms per token,   144.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     696.25 ms /  1124 tokens (    0.62 ms per token,  1614.36 tokens per second)\n",
      "llama_print_timings:        eval time =     391.63 ms /    20 runs   (   19.58 ms per token,    51.07 tokens per second)\n",
      "llama_print_timings:       total time =    1297.09 ms /  1144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     138.10 ms /    20 runs   (    6.90 ms per token,   144.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     695.42 ms /  1123 tokens (    0.62 ms per token,  1614.85 tokens per second)\n",
      "llama_print_timings:        eval time =     371.80 ms /    19 runs   (   19.57 ms per token,    51.10 tokens per second)\n",
      "llama_print_timings:       total time =    1266.47 ms /  1142 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     150.28 ms /    22 runs   (    6.83 ms per token,   146.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     699.05 ms /  1136 tokens (    0.62 ms per token,  1625.06 tokens per second)\n",
      "llama_print_timings:        eval time =     413.39 ms /    21 runs   (   19.69 ms per token,    50.80 tokens per second)\n",
      "llama_print_timings:       total time =    1329.64 ms /  1157 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     136.01 ms /    20 runs   (    6.80 ms per token,   147.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     694.71 ms /  1119 tokens (    0.62 ms per token,  1610.75 tokens per second)\n",
      "llama_print_timings:        eval time =     372.00 ms /    19 runs   (   19.58 ms per token,    51.08 tokens per second)\n",
      "llama_print_timings:       total time =    1263.54 ms /  1138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     143.66 ms /    21 runs   (    6.84 ms per token,   146.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     696.36 ms /  1125 tokens (    0.62 ms per token,  1615.54 tokens per second)\n",
      "llama_print_timings:        eval time =     391.65 ms /    20 runs   (   19.58 ms per token,    51.07 tokens per second)\n",
      "llama_print_timings:       total time =    1295.21 ms /  1145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     138.35 ms /    20 runs   (    6.92 ms per token,   144.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     694.53 ms /  1119 tokens (    0.62 ms per token,  1611.15 tokens per second)\n",
      "llama_print_timings:        eval time =     372.37 ms /    19 runs   (   19.60 ms per token,    51.02 tokens per second)\n",
      "llama_print_timings:       total time =    1267.48 ms /  1138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     156.34 ms /    23 runs   (    6.80 ms per token,   147.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     553.05 ms /   972 tokens (    0.57 ms per token,  1757.53 tokens per second)\n",
      "llama_print_timings:        eval time =     427.10 ms /    22 runs   (   19.41 ms per token,    51.51 tokens per second)\n",
      "llama_print_timings:       total time =    1203.83 ms /   994 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     136.39 ms /    20 runs   (    6.82 ms per token,   146.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     696.16 ms /  1125 tokens (    0.62 ms per token,  1616.00 tokens per second)\n",
      "llama_print_timings:        eval time =     371.87 ms /    19 runs   (   19.57 ms per token,    51.09 tokens per second)\n",
      "llama_print_timings:       total time =    1264.88 ms /  1144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     143.40 ms /    21 runs   (    6.83 ms per token,   146.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     695.90 ms /  1124 tokens (    0.62 ms per token,  1615.18 tokens per second)\n",
      "llama_print_timings:        eval time =     391.47 ms /    20 runs   (   19.57 ms per token,    51.09 tokens per second)\n",
      "llama_print_timings:       total time =    1294.14 ms /  1144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     157.39 ms /    21 runs   (    7.49 ms per token,   133.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     696.69 ms /  1126 tokens (    0.62 ms per token,  1616.20 tokens per second)\n",
      "llama_print_timings:        eval time =     393.59 ms /    20 runs   (   19.68 ms per token,    50.81 tokens per second)\n",
      "llama_print_timings:       total time =    1319.43 ms /  1146 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     363.75 ms /    53 runs   (    6.86 ms per token,   145.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     700.93 ms /  1140 tokens (    0.61 ms per token,  1626.42 tokens per second)\n",
      "llama_print_timings:        eval time =    1024.18 ms /    52 runs   (   19.70 ms per token,    50.77 tokens per second)\n",
      "llama_print_timings:       total time =    2246.66 ms /  1192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     152.72 ms /    22 runs   (    6.94 ms per token,   144.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.23 ms /   236 tokens (    0.68 ms per token,  1463.77 tokens per second)\n",
      "llama_print_timings:        eval time =     386.19 ms /    21 runs   (   18.39 ms per token,    54.38 tokens per second)\n",
      "llama_print_timings:       total time =     762.40 ms /   257 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     171.56 ms /    25 runs   (    6.86 ms per token,   145.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.72 ms /   229 tokens (    0.70 ms per token,  1433.76 tokens per second)\n",
      "llama_print_timings:        eval time =     440.99 ms /    24 runs   (   18.37 ms per token,    54.42 tokens per second)\n",
      "llama_print_timings:       total time =     841.68 ms /   253 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     213.79 ms /    31 runs   (    6.90 ms per token,   145.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.18 ms /   232 tokens (    0.69 ms per token,  1448.39 tokens per second)\n",
      "llama_print_timings:        eval time =     552.24 ms /    30 runs   (   18.41 ms per token,    54.32 tokens per second)\n",
      "llama_print_timings:       total time =    1013.79 ms /   262 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     208.26 ms /    30 runs   (    6.94 ms per token,   144.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     700.14 ms /  1138 tokens (    0.62 ms per token,  1625.38 tokens per second)\n",
      "llama_print_timings:        eval time =     571.25 ms /    29 runs   (   19.70 ms per token,    50.77 tokens per second)\n",
      "llama_print_timings:       total time =    1571.22 ms /  1167 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     238.96 ms /    35 runs   (    6.83 ms per token,   146.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     695.17 ms /  1120 tokens (    0.62 ms per token,  1611.11 tokens per second)\n",
      "llama_print_timings:        eval time =     668.26 ms /    34 runs   (   19.65 ms per token,    50.88 tokens per second)\n",
      "llama_print_timings:       total time =    1707.33 ms /  1154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     202.42 ms /    29 runs   (    6.98 ms per token,   143.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.35 ms /   222 tokens (    0.74 ms per token,  1342.62 tokens per second)\n",
      "llama_print_timings:        eval time =     513.92 ms /    28 runs   (   18.35 ms per token,    54.48 tokens per second)\n",
      "llama_print_timings:       total time =     964.13 ms /   250 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     152.82 ms /    22 runs   (    6.95 ms per token,   143.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     164.35 ms /   217 tokens (    0.76 ms per token,  1320.37 tokens per second)\n",
      "llama_print_timings:        eval time =     384.73 ms /    21 runs   (   18.32 ms per token,    54.58 tokens per second)\n",
      "llama_print_timings:       total time =     764.17 ms /   238 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     206.00 ms /    30 runs   (    6.87 ms per token,   145.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     164.95 ms /   220 tokens (    0.75 ms per token,  1333.71 tokens per second)\n",
      "llama_print_timings:        eval time =     532.01 ms /    29 runs   (   18.35 ms per token,    54.51 tokens per second)\n",
      "llama_print_timings:       total time =     985.97 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     322.61 ms /    48 runs   (    6.72 ms per token,   148.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.16 ms /   261 tokens (    0.76 ms per token,  1310.48 tokens per second)\n",
      "llama_print_timings:        eval time =     868.40 ms /    47 runs   (   18.48 ms per token,    54.12 tokens per second)\n",
      "llama_print_timings:       total time =    1522.99 ms /   308 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     199.86 ms /    29 runs   (    6.89 ms per token,   145.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.96 ms /   254 tokens (    0.65 ms per token,  1530.48 tokens per second)\n",
      "llama_print_timings:        eval time =     516.75 ms /    28 runs   (   18.46 ms per token,    54.19 tokens per second)\n",
      "llama_print_timings:       total time =     962.72 ms /   282 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     210.44 ms /    28 runs   (    7.52 ms per token,   133.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     212.37 ms /   314 tokens (    0.68 ms per token,  1478.57 tokens per second)\n",
      "llama_print_timings:        eval time =     501.48 ms /    27 runs   (   18.57 ms per token,    53.84 tokens per second)\n",
      "llama_print_timings:       total time =    1007.68 ms /   341 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     146.10 ms /    21 runs   (    6.96 ms per token,   143.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     718.21 ms /  1153 tokens (    0.62 ms per token,  1605.38 tokens per second)\n",
      "llama_print_timings:        eval time =     394.63 ms /    20 runs   (   19.73 ms per token,    50.68 tokens per second)\n",
      "llama_print_timings:       total time =    1324.69 ms /  1173 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     205.29 ms /    29 runs   (    7.08 ms per token,   141.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     721.21 ms /  1163 tokens (    0.62 ms per token,  1612.57 tokens per second)\n",
      "llama_print_timings:        eval time =     552.85 ms /    28 runs   (   19.74 ms per token,    50.65 tokens per second)\n",
      "llama_print_timings:       total time =    1568.49 ms /  1191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     172.39 ms /    25 runs   (    6.90 ms per token,   145.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.69 ms /   234 tokens (    0.69 ms per token,  1456.17 tokens per second)\n",
      "llama_print_timings:        eval time =     441.65 ms /    24 runs   (   18.40 ms per token,    54.34 tokens per second)\n",
      "llama_print_timings:       total time =     844.75 ms /   258 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     149.12 ms /    22 runs   (    6.78 ms per token,   147.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     210.94 ms /   309 tokens (    0.68 ms per token,  1464.88 tokens per second)\n",
      "llama_print_timings:        eval time =     388.55 ms /    21 runs   (   18.50 ms per token,    54.05 tokens per second)\n",
      "llama_print_timings:       total time =     810.26 ms /   330 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     156.89 ms /    23 runs   (    6.82 ms per token,   146.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.05 ms /   235 tokens (    0.69 ms per token,  1459.15 tokens per second)\n",
      "llama_print_timings:        eval time =     404.75 ms /    22 runs   (   18.40 ms per token,    54.35 tokens per second)\n",
      "llama_print_timings:       total time =     787.28 ms /   257 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     256.97 ms /    37 runs   (    6.95 ms per token,   143.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     718.97 ms /  1155 tokens (    0.62 ms per token,  1606.46 tokens per second)\n",
      "llama_print_timings:        eval time =     713.13 ms /    36 runs   (   19.81 ms per token,    50.48 tokens per second)\n",
      "llama_print_timings:       total time =    1802.40 ms /  1191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     200.30 ms /    29 runs   (    6.91 ms per token,   144.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     720.39 ms /  1162 tokens (    0.62 ms per token,  1613.02 tokens per second)\n",
      "llama_print_timings:        eval time =     553.45 ms /    28 runs   (   19.77 ms per token,    50.59 tokens per second)\n",
      "llama_print_timings:       total time =    1562.66 ms /  1190 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     192.09 ms /    27 runs   (    7.11 ms per token,   140.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     543.77 ms /   943 tokens (    0.58 ms per token,  1734.18 tokens per second)\n",
      "llama_print_timings:        eval time =     506.88 ms /    26 runs   (   19.50 ms per token,    51.29 tokens per second)\n",
      "llama_print_timings:       total time =    1324.16 ms /   969 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     208.06 ms /    30 runs   (    6.94 ms per token,   144.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.28 ms /   130 tokens (    1.07 ms per token,   933.34 tokens per second)\n",
      "llama_print_timings:        eval time =     531.46 ms /    29 runs   (   18.33 ms per token,    54.57 tokens per second)\n",
      "llama_print_timings:       total time =     962.61 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     277.40 ms /    41 runs   (    6.77 ms per token,   147.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.29 ms /   329 tokens (    0.66 ms per token,  1521.13 tokens per second)\n",
      "llama_print_timings:        eval time =     742.64 ms /    40 runs   (   18.57 ms per token,    53.86 tokens per second)\n",
      "llama_print_timings:       total time =    1352.49 ms /   369 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     297.50 ms /    44 runs   (    6.76 ms per token,   147.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.10 ms /   330 tokens (    0.65 ms per token,  1527.08 tokens per second)\n",
      "llama_print_timings:        eval time =     798.79 ms /    43 runs   (   18.58 ms per token,    53.83 tokens per second)\n",
      "llama_print_timings:       total time =    1435.79 ms /   373 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     204.23 ms /    29 runs   (    7.04 ms per token,   142.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.03 ms /   129 tokens (    1.08 ms per token,   927.84 tokens per second)\n",
      "llama_print_timings:        eval time =     512.40 ms /    28 runs   (   18.30 ms per token,    54.64 tokens per second)\n",
      "llama_print_timings:       total time =     937.96 ms /   157 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     311.14 ms /    46 runs   (    6.76 ms per token,   147.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     216.64 ms /   333 tokens (    0.65 ms per token,  1537.11 tokens per second)\n",
      "llama_print_timings:        eval time =     834.81 ms /    45 runs   (   18.55 ms per token,    53.90 tokens per second)\n",
      "llama_print_timings:       total time =    1492.21 ms /   378 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     210.76 ms /    30 runs   (    7.03 ms per token,   142.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.01 ms /   129 tokens (    1.08 ms per token,   927.98 tokens per second)\n",
      "llama_print_timings:        eval time =     530.24 ms /    29 runs   (   18.28 ms per token,    54.69 tokens per second)\n",
      "llama_print_timings:       total time =     967.63 ms /   158 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     176.37 ms /    26 runs   (    6.78 ms per token,   147.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.57 ms /   245 tokens (    0.67 ms per token,  1497.84 tokens per second)\n",
      "llama_print_timings:        eval time =     461.29 ms /    25 runs   (   18.45 ms per token,    54.20 tokens per second)\n",
      "llama_print_timings:       total time =     873.84 ms /   270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     189.85 ms /    28 runs   (    6.78 ms per token,   147.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     208.73 ms /   297 tokens (    0.70 ms per token,  1422.86 tokens per second)\n",
      "llama_print_timings:        eval time =     499.31 ms /    27 runs   (   18.49 ms per token,    54.08 tokens per second)\n",
      "llama_print_timings:       total time =     976.32 ms /   324 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     146.22 ms /    20 runs   (    7.31 ms per token,   136.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     123.61 ms /   115 tokens (    1.07 ms per token,   930.35 tokens per second)\n",
      "llama_print_timings:        eval time =     348.05 ms /    19 runs   (   18.32 ms per token,    54.59 tokens per second)\n",
      "llama_print_timings:       total time =     678.40 ms /   134 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     175.53 ms /    23 runs   (    7.63 ms per token,   131.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.64 ms /   155 tokens (    0.94 ms per token,  1064.25 tokens per second)\n",
      "llama_print_timings:        eval time =     404.37 ms /    22 runs   (   18.38 ms per token,    54.41 tokens per second)\n",
      "llama_print_timings:       total time =     803.87 ms /   177 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     176.68 ms /    23 runs   (    7.68 ms per token,   130.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     146.53 ms /   157 tokens (    0.93 ms per token,  1071.47 tokens per second)\n",
      "llama_print_timings:        eval time =     404.37 ms /    22 runs   (   18.38 ms per token,    54.41 tokens per second)\n",
      "llama_print_timings:       total time =     804.49 ms /   179 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     176.87 ms /    23 runs   (    7.69 ms per token,   130.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.81 ms /   155 tokens (    0.94 ms per token,  1063.04 tokens per second)\n",
      "llama_print_timings:        eval time =     404.33 ms /    22 runs   (   18.38 ms per token,    54.41 tokens per second)\n",
      "llama_print_timings:       total time =     805.22 ms /   177 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     243.78 ms /    34 runs   (    7.17 ms per token,   139.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     701.58 ms /  1134 tokens (    0.62 ms per token,  1616.34 tokens per second)\n",
      "llama_print_timings:        eval time =     649.62 ms /    33 runs   (   19.69 ms per token,    50.80 tokens per second)\n",
      "llama_print_timings:       total time =    1699.57 ms /  1167 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     169.92 ms /    24 runs   (    7.08 ms per token,   141.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     208.95 ms /   300 tokens (    0.70 ms per token,  1435.74 tokens per second)\n",
      "llama_print_timings:        eval time =     425.87 ms /    23 runs   (   18.52 ms per token,    54.01 tokens per second)\n",
      "llama_print_timings:       total time =     873.25 ms /   323 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     140.71 ms /    20 runs   (    7.04 ms per token,   142.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     145.08 ms /   153 tokens (    0.95 ms per token,  1054.59 tokens per second)\n",
      "llama_print_timings:        eval time =     347.90 ms /    19 runs   (   18.31 ms per token,    54.61 tokens per second)\n",
      "llama_print_timings:       total time =     691.95 ms /   172 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     200.42 ms /    29 runs   (    6.91 ms per token,   144.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     206.85 ms /   290 tokens (    0.71 ms per token,  1401.96 tokens per second)\n",
      "llama_print_timings:        eval time =     517.66 ms /    28 runs   (   18.49 ms per token,    54.09 tokens per second)\n",
      "llama_print_timings:       total time =    1007.48 ms /   318 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     152.08 ms /    22 runs   (    6.91 ms per token,   144.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     206.81 ms /   291 tokens (    0.71 ms per token,  1407.06 tokens per second)\n",
      "llama_print_timings:        eval time =     388.17 ms /    21 runs   (   18.48 ms per token,    54.10 tokens per second)\n",
      "llama_print_timings:       total time =     809.47 ms /   312 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     189.70 ms /    27 runs   (    7.03 ms per token,   142.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     206.63 ms /   289 tokens (    0.71 ms per token,  1398.64 tokens per second)\n",
      "llama_print_timings:        eval time =     480.57 ms /    26 runs   (   18.48 ms per token,    54.10 tokens per second)\n",
      "llama_print_timings:       total time =     953.82 ms /   315 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     144.98 ms /    21 runs   (    6.90 ms per token,   144.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     542.94 ms /   937 tokens (    0.58 ms per token,  1725.79 tokens per second)\n",
      "llama_print_timings:        eval time =     388.60 ms /    20 runs   (   19.43 ms per token,    51.47 tokens per second)\n",
      "llama_print_timings:       total time =    1140.30 ms /   957 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     143.68 ms /    21 runs   (    6.84 ms per token,   146.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     542.01 ms /   933 tokens (    0.58 ms per token,  1721.37 tokens per second)\n",
      "llama_print_timings:        eval time =     388.42 ms /    20 runs   (   19.42 ms per token,    51.49 tokens per second)\n",
      "llama_print_timings:       total time =    1135.74 ms /   953 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     220.42 ms /    32 runs   (    6.89 ms per token,   145.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     543.30 ms /   939 tokens (    0.58 ms per token,  1728.34 tokens per second)\n",
      "llama_print_timings:        eval time =     601.93 ms /    31 runs   (   19.42 ms per token,    51.50 tokens per second)\n",
      "llama_print_timings:       total time =    1459.26 ms /   970 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     231.71 ms /    31 runs   (    7.47 ms per token,   133.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     541.76 ms /   932 tokens (    0.58 ms per token,  1720.33 tokens per second)\n",
      "llama_print_timings:        eval time =     582.89 ms /    30 runs   (   19.43 ms per token,    51.47 tokens per second)\n",
      "llama_print_timings:       total time =    1450.63 ms /   962 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     366.46 ms /    55 runs   (    6.66 ms per token,   150.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     739.51 ms /  1216 tokens (    0.61 ms per token,  1644.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1068.20 ms /    54 runs   (   19.78 ms per token,    50.55 tokens per second)\n",
      "llama_print_timings:       total time =    2337.26 ms /  1270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     431.25 ms /    64 runs   (    6.74 ms per token,   148.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     736.30 ms /  1207 tokens (    0.61 ms per token,  1639.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1247.09 ms /    63 runs   (   19.80 ms per token,    50.52 tokens per second)\n",
      "llama_print_timings:       total time =    2606.06 ms /  1270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     152.90 ms /    22 runs   (    6.95 ms per token,   143.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     202.31 ms /   273 tokens (    0.74 ms per token,  1349.42 tokens per second)\n",
      "llama_print_timings:        eval time =     388.28 ms /    21 runs   (   18.49 ms per token,    54.08 tokens per second)\n",
      "llama_print_timings:       total time =     807.47 ms /   294 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     192.87 ms /    28 runs   (    6.89 ms per token,   145.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     202.73 ms /   273 tokens (    0.74 ms per token,  1346.64 tokens per second)\n",
      "llama_print_timings:        eval time =     498.97 ms /    27 runs   (   18.48 ms per token,    54.11 tokens per second)\n",
      "llama_print_timings:       total time =     973.08 ms /   300 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     150.07 ms /    22 runs   (    6.82 ms per token,   146.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.91 ms /   272 tokens (    0.74 ms per token,  1347.11 tokens per second)\n",
      "llama_print_timings:        eval time =     387.83 ms /    21 runs   (   18.47 ms per token,    54.15 tokens per second)\n",
      "llama_print_timings:       total time =     802.33 ms /   293 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     150.85 ms /    22 runs   (    6.86 ms per token,   145.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.78 ms /   270 tokens (    0.75 ms per token,  1338.08 tokens per second)\n",
      "llama_print_timings:        eval time =     387.98 ms /    21 runs   (   18.48 ms per token,    54.13 tokens per second)\n",
      "llama_print_timings:       total time =     803.15 ms /   291 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     193.34 ms /    28 runs   (    6.90 ms per token,   144.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     203.78 ms /   278 tokens (    0.73 ms per token,  1364.21 tokens per second)\n",
      "llama_print_timings:        eval time =     499.09 ms /    27 runs   (   18.48 ms per token,    54.10 tokens per second)\n",
      "llama_print_timings:       total time =     974.67 ms /   305 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     185.10 ms /    27 runs   (    6.86 ms per token,   145.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     203.43 ms /   276 tokens (    0.74 ms per token,  1356.72 tokens per second)\n",
      "llama_print_timings:        eval time =     480.61 ms /    26 runs   (   18.48 ms per token,    54.10 tokens per second)\n",
      "llama_print_timings:       total time =     945.54 ms /   302 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     157.77 ms /    23 runs   (    6.86 ms per token,   145.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.45 ms /   199 tokens (    0.81 ms per token,  1240.26 tokens per second)\n",
      "llama_print_timings:        eval time =     403.46 ms /    22 runs   (   18.34 ms per token,    54.53 tokens per second)\n",
      "llama_print_timings:       total time =     787.89 ms /   221 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     159.75 ms /    23 runs   (    6.95 ms per token,   143.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.84 ms /   205 tokens (    0.79 ms per token,  1266.68 tokens per second)\n",
      "llama_print_timings:        eval time =     403.78 ms /    22 runs   (   18.35 ms per token,    54.49 tokens per second)\n",
      "llama_print_timings:       total time =     792.44 ms /   227 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     159.45 ms /    23 runs   (    6.93 ms per token,   144.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.32 ms /   211 tokens (    0.77 ms per token,  1291.97 tokens per second)\n",
      "llama_print_timings:        eval time =     403.71 ms /    22 runs   (   18.35 ms per token,    54.49 tokens per second)\n",
      "llama_print_timings:       total time =     793.61 ms /   233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     148.23 ms /    21 runs   (    7.06 ms per token,   141.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.02 ms /    98 tokens (    1.22 ms per token,   816.54 tokens per second)\n",
      "llama_print_timings:        eval time =     364.37 ms /    20 runs   (   18.22 ms per token,    54.89 tokens per second)\n",
      "llama_print_timings:       total time =     693.36 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     148.24 ms /    21 runs   (    7.06 ms per token,   141.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.11 ms /    99 tokens (    1.21 ms per token,   824.22 tokens per second)\n",
      "llama_print_timings:        eval time =     364.61 ms /    20 runs   (   18.23 ms per token,    54.85 tokens per second)\n",
      "llama_print_timings:       total time =     693.89 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     147.94 ms /    21 runs   (    7.04 ms per token,   141.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.72 ms /   102 tokens (    1.18 ms per token,   844.96 tokens per second)\n",
      "llama_print_timings:        eval time =     365.00 ms /    20 runs   (   18.25 ms per token,    54.79 tokens per second)\n",
      "llama_print_timings:       total time =     695.14 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     158.32 ms /    23 runs   (    6.88 ms per token,   145.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     303.20 ms /   516 tokens (    0.59 ms per token,  1701.86 tokens per second)\n",
      "llama_print_timings:        eval time =     414.29 ms /    22 runs   (   18.83 ms per token,    53.10 tokens per second)\n",
      "llama_print_timings:       total time =     943.59 ms /   538 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     159.78 ms /    23 runs   (    6.95 ms per token,   143.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     306.11 ms /   517 tokens (    0.59 ms per token,  1688.91 tokens per second)\n",
      "llama_print_timings:        eval time =     413.81 ms /    22 runs   (   18.81 ms per token,    53.17 tokens per second)\n",
      "llama_print_timings:       total time =     948.07 ms /   539 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     158.83 ms /    23 runs   (    6.91 ms per token,   144.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     309.41 ms /   518 tokens (    0.60 ms per token,  1674.14 tokens per second)\n",
      "llama_print_timings:        eval time =     413.75 ms /    22 runs   (   18.81 ms per token,    53.17 tokens per second)\n",
      "llama_print_timings:       total time =     949.75 ms /   540 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     143.45 ms /    21 runs   (    6.83 ms per token,   146.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     696.72 ms /  1118 tokens (    0.62 ms per token,  1604.66 tokens per second)\n",
      "llama_print_timings:        eval time =     391.71 ms /    20 runs   (   19.59 ms per token,    51.06 tokens per second)\n",
      "llama_print_timings:       total time =    1296.95 ms /  1138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     143.70 ms /    21 runs   (    6.84 ms per token,   146.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     696.46 ms /  1118 tokens (    0.62 ms per token,  1605.26 tokens per second)\n",
      "llama_print_timings:        eval time =     391.57 ms /    20 runs   (   19.58 ms per token,    51.08 tokens per second)\n",
      "llama_print_timings:       total time =    1296.80 ms /  1138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     146.24 ms /    21 runs   (    6.96 ms per token,   143.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.32 ms /    19 tokens (    7.07 ms per token,   141.45 tokens per second)\n",
      "llama_print_timings:        eval time =     392.33 ms /    20 runs   (   19.62 ms per token,    50.98 tokens per second)\n",
      "llama_print_timings:       total time =     736.56 ms /    39 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     183.38 ms /    27 runs   (    6.79 ms per token,   147.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     698.28 ms /  1123 tokens (    0.62 ms per token,  1608.24 tokens per second)\n",
      "llama_print_timings:        eval time =     511.09 ms /    26 runs   (   19.66 ms per token,    50.87 tokens per second)\n",
      "llama_print_timings:       total time =    1474.79 ms /  1149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     144.34 ms /    21 runs   (    6.87 ms per token,   145.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     694.86 ms /  1111 tokens (    0.63 ms per token,  1598.88 tokens per second)\n",
      "llama_print_timings:        eval time =     391.89 ms /    20 runs   (   19.59 ms per token,    51.04 tokens per second)\n",
      "llama_print_timings:       total time =    1296.18 ms /  1131 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     143.86 ms /    21 runs   (    6.85 ms per token,   145.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     697.64 ms /  1120 tokens (    0.62 ms per token,  1605.41 tokens per second)\n",
      "llama_print_timings:        eval time =     391.53 ms /    20 runs   (   19.58 ms per token,    51.08 tokens per second)\n",
      "llama_print_timings:       total time =    1298.98 ms /  1140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     145.28 ms /    21 runs   (    6.92 ms per token,   144.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     695.23 ms /  1113 tokens (    0.62 ms per token,  1600.92 tokens per second)\n",
      "llama_print_timings:        eval time =     392.47 ms /    20 runs   (   19.62 ms per token,    50.96 tokens per second)\n",
      "llama_print_timings:       total time =    1298.35 ms /  1133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     144.34 ms /    21 runs   (    6.87 ms per token,   145.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     697.33 ms /  1119 tokens (    0.62 ms per token,  1604.70 tokens per second)\n",
      "llama_print_timings:        eval time =     391.91 ms /    20 runs   (   19.60 ms per token,    51.03 tokens per second)\n",
      "llama_print_timings:       total time =    1298.95 ms /  1139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     218.02 ms /    31 runs   (    7.03 ms per token,   142.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.01 ms /   315 tokens (    0.68 ms per token,  1478.79 tokens per second)\n",
      "llama_print_timings:        eval time =     555.99 ms /    30 runs   (   18.53 ms per token,    53.96 tokens per second)\n",
      "llama_print_timings:       total time =    1078.30 ms /   345 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     208.22 ms /    30 runs   (    6.94 ms per token,   144.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     212.82 ms /   314 tokens (    0.68 ms per token,  1475.44 tokens per second)\n",
      "llama_print_timings:        eval time =     537.09 ms /    29 runs   (   18.52 ms per token,    53.99 tokens per second)\n",
      "llama_print_timings:       total time =    1043.94 ms /   343 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     257.97 ms /    35 runs   (    7.37 ms per token,   135.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     735.00 ms /  1200 tokens (    0.61 ms per token,  1632.65 tokens per second)\n",
      "llama_print_timings:        eval time =     672.08 ms /    34 runs   (   19.77 ms per token,    50.59 tokens per second)\n",
      "llama_print_timings:       total time =    1779.03 ms /  1234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     164.38 ms /    24 runs   (    6.85 ms per token,   146.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.69 ms /   209 tokens (    0.78 ms per token,  1284.69 tokens per second)\n",
      "llama_print_timings:        eval time =     421.72 ms /    23 runs   (   18.34 ms per token,    54.54 tokens per second)\n",
      "llama_print_timings:       total time =     816.73 ms /   232 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     207.08 ms /    30 runs   (    6.90 ms per token,   144.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     164.87 ms /   217 tokens (    0.76 ms per token,  1316.18 tokens per second)\n",
      "llama_print_timings:        eval time =     531.87 ms /    29 runs   (   18.34 ms per token,    54.52 tokens per second)\n",
      "llama_print_timings:       total time =     989.46 ms /   246 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     153.25 ms /    22 runs   (    6.97 ms per token,   143.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.14 ms /   218 tokens (    0.76 ms per token,  1320.11 tokens per second)\n",
      "llama_print_timings:        eval time =     385.40 ms /    21 runs   (   18.35 ms per token,    54.49 tokens per second)\n",
      "llama_print_timings:       total time =     767.38 ms /   239 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     177.14 ms /    26 runs   (    6.81 ms per token,   146.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     165.08 ms /   248 tokens (    0.67 ms per token,  1502.27 tokens per second)\n",
      "llama_print_timings:        eval time =     461.71 ms /    25 runs   (   18.47 ms per token,    54.15 tokens per second)\n",
      "llama_print_timings:       total time =     877.20 ms /   273 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     272.89 ms /    41 runs   (    6.66 ms per token,   150.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     702.07 ms /  1135 tokens (    0.62 ms per token,  1616.64 tokens per second)\n",
      "llama_print_timings:        eval time =     788.80 ms /    40 runs   (   19.72 ms per token,    50.71 tokens per second)\n",
      "llama_print_timings:       total time =    1887.94 ms /  1175 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     212.50 ms /    31 runs   (    6.85 ms per token,   145.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     702.39 ms /  1138 tokens (    0.62 ms per token,  1620.19 tokens per second)\n",
      "llama_print_timings:        eval time =     591.06 ms /    30 runs   (   19.70 ms per token,    50.76 tokens per second)\n",
      "llama_print_timings:       total time =    1599.57 ms /  1168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     200.71 ms /    29 runs   (    6.92 ms per token,   144.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.31 ms /   257 tokens (    0.78 ms per token,  1289.48 tokens per second)\n",
      "llama_print_timings:        eval time =     517.25 ms /    28 runs   (   18.47 ms per token,    54.13 tokens per second)\n",
      "llama_print_timings:       total time =    1000.10 ms /   285 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     277.28 ms /    40 runs   (    6.93 ms per token,   144.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     199.85 ms /   259 tokens (    0.77 ms per token,  1295.97 tokens per second)\n",
      "llama_print_timings:        eval time =     720.45 ms /    39 runs   (   18.47 ms per token,    54.13 tokens per second)\n",
      "llama_print_timings:       total time =    1312.03 ms /   298 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     206.57 ms /    30 runs   (    6.89 ms per token,   145.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.90 ms /   213 tokens (    0.77 ms per token,  1299.55 tokens per second)\n",
      "llama_print_timings:        eval time =     535.16 ms /    29 runs   (   18.45 ms per token,    54.19 tokens per second)\n",
      "llama_print_timings:       total time =     991.53 ms /   242 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     221.39 ms /    32 runs   (    6.92 ms per token,   144.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     703.10 ms /  1140 tokens (    0.62 ms per token,  1621.39 tokens per second)\n",
      "llama_print_timings:        eval time =     612.29 ms /    31 runs   (   19.75 ms per token,    50.63 tokens per second)\n",
      "llama_print_timings:       total time =    1634.21 ms /  1171 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     218.75 ms /    33 runs   (    6.63 ms per token,   150.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     729.80 ms /  1176 tokens (    0.62 ms per token,  1611.40 tokens per second)\n",
      "llama_print_timings:        eval time =     632.84 ms /    32 runs   (   19.78 ms per token,    50.57 tokens per second)\n",
      "llama_print_timings:       total time =    1682.05 ms /  1208 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     150.35 ms /    22 runs   (    6.83 ms per token,   146.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     546.11 ms /   942 tokens (    0.58 ms per token,  1724.93 tokens per second)\n",
      "llama_print_timings:        eval time =     408.48 ms /    21 runs   (   19.45 ms per token,    51.41 tokens per second)\n",
      "llama_print_timings:       total time =    1171.64 ms /   963 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     149.34 ms /    22 runs   (    6.79 ms per token,   147.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     545.75 ms /   940 tokens (    0.58 ms per token,  1722.42 tokens per second)\n",
      "llama_print_timings:        eval time =     408.55 ms /    21 runs   (   19.45 ms per token,    51.40 tokens per second)\n",
      "llama_print_timings:       total time =    1170.09 ms /   961 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     144.07 ms /    21 runs   (    6.86 ms per token,   145.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     546.15 ms /   943 tokens (    0.58 ms per token,  1726.64 tokens per second)\n",
      "llama_print_timings:        eval time =     389.08 ms /    20 runs   (   19.45 ms per token,    51.40 tokens per second)\n",
      "llama_print_timings:       total time =    1142.30 ms /   963 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     144.15 ms /    21 runs   (    6.86 ms per token,   145.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     544.49 ms /   936 tokens (    0.58 ms per token,  1719.04 tokens per second)\n",
      "llama_print_timings:        eval time =     389.07 ms /    20 runs   (   19.45 ms per token,    51.40 tokens per second)\n",
      "llama_print_timings:       total time =    1140.96 ms /   956 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     144.04 ms /    21 runs   (    6.86 ms per token,   145.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     545.53 ms /   939 tokens (    0.58 ms per token,  1721.26 tokens per second)\n",
      "llama_print_timings:        eval time =     389.51 ms /    20 runs   (   19.48 ms per token,    51.35 tokens per second)\n",
      "llama_print_timings:       total time =    1142.05 ms /   959 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     143.74 ms /    21 runs   (    6.84 ms per token,   146.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     544.91 ms /   937 tokens (    0.58 ms per token,  1719.56 tokens per second)\n",
      "llama_print_timings:        eval time =     389.10 ms /    20 runs   (   19.45 ms per token,    51.40 tokens per second)\n",
      "llama_print_timings:       total time =    1140.82 ms /   957 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     144.52 ms /    21 runs   (    6.88 ms per token,   145.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     545.86 ms /   937 tokens (    0.58 ms per token,  1716.54 tokens per second)\n",
      "llama_print_timings:        eval time =     389.48 ms /    20 runs   (   19.47 ms per token,    51.35 tokens per second)\n",
      "llama_print_timings:       total time =    1143.70 ms /   957 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     145.64 ms /    21 runs   (    6.94 ms per token,   144.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     547.21 ms /   945 tokens (    0.58 ms per token,  1726.95 tokens per second)\n",
      "llama_print_timings:        eval time =     389.31 ms /    20 runs   (   19.47 ms per token,    51.37 tokens per second)\n",
      "llama_print_timings:       total time =    1146.68 ms /   965 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     151.12 ms /    22 runs   (    6.87 ms per token,   145.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     703.63 ms /  1136 tokens (    0.62 ms per token,  1614.49 tokens per second)\n",
      "llama_print_timings:        eval time =     415.30 ms /    21 runs   (   19.78 ms per token,    50.57 tokens per second)\n",
      "llama_print_timings:       total time =    1339.02 ms /  1157 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     156.25 ms /    23 runs   (    6.79 ms per token,   147.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.33 ms /  1267 tokens (    0.59 ms per token,  1681.87 tokens per second)\n",
      "llama_print_timings:        eval time =     438.76 ms /    22 runs   (   19.94 ms per token,    50.14 tokens per second)\n",
      "llama_print_timings:       total time =    1420.01 ms /  1289 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     208.53 ms /    30 runs   (    6.95 ms per token,   143.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     706.25 ms /  1144 tokens (    0.62 ms per token,  1619.83 tokens per second)\n",
      "llama_print_timings:        eval time =     573.33 ms /    29 runs   (   19.77 ms per token,    50.58 tokens per second)\n",
      "llama_print_timings:       total time =    1582.26 ms /  1173 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     205.51 ms /    30 runs   (    6.85 ms per token,   145.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     705.87 ms /  1143 tokens (    0.62 ms per token,  1619.29 tokens per second)\n",
      "llama_print_timings:        eval time =     572.88 ms /    29 runs   (   19.75 ms per token,    50.62 tokens per second)\n",
      "llama_print_timings:       total time =    1576.13 ms /  1172 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     199.92 ms /    29 runs   (    6.89 ms per token,   145.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     705.01 ms /  1141 tokens (    0.62 ms per token,  1618.42 tokens per second)\n",
      "llama_print_timings:        eval time =     553.64 ms /    28 runs   (   19.77 ms per token,    50.57 tokens per second)\n",
      "llama_print_timings:       total time =    1547.75 ms /  1169 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     150.87 ms /    22 runs   (    6.86 ms per token,   145.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.45 ms /  1239 tokens (    0.61 ms per token,  1644.43 tokens per second)\n",
      "llama_print_timings:        eval time =     418.27 ms /    21 runs   (   19.92 ms per token,    50.21 tokens per second)\n",
      "llama_print_timings:       total time =    1391.18 ms /  1260 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     150.15 ms /    22 runs   (    6.83 ms per token,   146.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     703.50 ms /  1135 tokens (    0.62 ms per token,  1613.35 tokens per second)\n",
      "llama_print_timings:        eval time =     415.32 ms /    21 runs   (   19.78 ms per token,    50.56 tokens per second)\n",
      "llama_print_timings:       total time =    1338.13 ms /  1156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     200.30 ms /    29 runs   (    6.91 ms per token,   144.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     706.27 ms /  1144 tokens (    0.62 ms per token,  1619.79 tokens per second)\n",
      "llama_print_timings:        eval time =     554.23 ms /    28 runs   (   19.79 ms per token,    50.52 tokens per second)\n",
      "llama_print_timings:       total time =    1548.90 ms /  1172 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     225.85 ms /    33 runs   (    6.84 ms per token,   146.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.10 ms /   195 tokens (    0.82 ms per token,  1217.95 tokens per second)\n",
      "llama_print_timings:        eval time =     587.01 ms /    32 runs   (   18.34 ms per token,    54.51 tokens per second)\n",
      "llama_print_timings:       total time =    1066.85 ms /   227 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     157.31 ms /    23 runs   (    6.84 ms per token,   146.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.26 ms /   104 tokens (    1.17 ms per token,   857.67 tokens per second)\n",
      "llama_print_timings:        eval time =     401.63 ms /    22 runs   (   18.26 ms per token,    54.78 tokens per second)\n",
      "llama_print_timings:       total time =     745.96 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     156.98 ms /    23 runs   (    6.83 ms per token,   146.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.61 ms /    92 tokens (    1.29 ms per token,   775.64 tokens per second)\n",
      "llama_print_timings:        eval time =     401.29 ms /    22 runs   (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:       total time =     742.41 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     158.66 ms /    23 runs   (    6.90 ms per token,   144.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     121.27 ms /   104 tokens (    1.17 ms per token,   857.57 tokens per second)\n",
      "llama_print_timings:        eval time =     401.83 ms /    22 runs   (   18.27 ms per token,    54.75 tokens per second)\n",
      "llama_print_timings:       total time =     747.62 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     152.40 ms /    22 runs   (    6.93 ms per token,   144.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.60 ms /   197 tokens (    0.82 ms per token,  1226.61 tokens per second)\n",
      "llama_print_timings:        eval time =     385.37 ms /    21 runs   (   18.35 ms per token,    54.49 tokens per second)\n",
      "llama_print_timings:       total time =     761.27 ms /   218 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     152.24 ms /    22 runs   (    6.92 ms per token,   144.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.21 ms /   196 tokens (    0.82 ms per token,  1223.37 tokens per second)\n",
      "llama_print_timings:        eval time =     385.28 ms /    21 runs   (   18.35 ms per token,    54.51 tokens per second)\n",
      "llama_print_timings:       total time =     760.41 ms /   217 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     204.38 ms /    30 runs   (    6.81 ms per token,   146.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.53 ms /    95 tokens (    1.26 ms per token,   794.79 tokens per second)\n",
      "llama_print_timings:        eval time =     529.47 ms /    29 runs   (   18.26 ms per token,    54.77 tokens per second)\n",
      "llama_print_timings:       total time =     938.35 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     170.80 ms /    25 runs   (    6.83 ms per token,   146.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.97 ms /    97 tokens (    1.24 ms per token,   808.51 tokens per second)\n",
      "llama_print_timings:        eval time =     437.54 ms /    24 runs   (   18.23 ms per token,    54.85 tokens per second)\n",
      "llama_print_timings:       total time =     799.23 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     226.30 ms /    33 runs   (    6.86 ms per token,   145.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.19 ms /    94 tokens (    1.27 ms per token,   788.66 tokens per second)\n",
      "llama_print_timings:        eval time =     583.98 ms /    32 runs   (   18.25 ms per token,    54.80 tokens per second)\n",
      "llama_print_timings:       total time =    1023.65 ms /   126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     152.30 ms /    22 runs   (    6.92 ms per token,   144.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     159.78 ms /   193 tokens (    0.83 ms per token,  1207.95 tokens per second)\n",
      "llama_print_timings:        eval time =     385.61 ms /    21 runs   (   18.36 ms per token,    54.46 tokens per second)\n",
      "llama_print_timings:       total time =     760.90 ms /   214 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     157.30 ms /    23 runs   (    6.84 ms per token,   146.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.09 ms /    94 tokens (    1.27 ms per token,   789.32 tokens per second)\n",
      "llama_print_timings:        eval time =     401.38 ms /    22 runs   (   18.24 ms per token,    54.81 tokens per second)\n",
      "llama_print_timings:       total time =     743.91 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     191.80 ms /    28 runs   (    6.85 ms per token,   145.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.08 ms /    94 tokens (    1.27 ms per token,   789.39 tokens per second)\n",
      "llama_print_timings:        eval time =     492.70 ms /    27 runs   (   18.25 ms per token,    54.80 tokens per second)\n",
      "llama_print_timings:       total time =     882.85 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =      87.26 ms /    13 runs   (    6.71 ms per token,   148.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.95 ms /    98 tokens (    1.22 ms per token,   817.03 tokens per second)\n",
      "llama_print_timings:        eval time =     218.81 ms /    12 runs   (   18.23 ms per token,    54.84 tokens per second)\n",
      "llama_print_timings:       total time =     463.75 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     157.58 ms /    23 runs   (    6.85 ms per token,   145.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.67 ms /    92 tokens (    1.29 ms per token,   775.26 tokens per second)\n",
      "llama_print_timings:        eval time =     401.61 ms /    22 runs   (   18.25 ms per token,    54.78 tokens per second)\n",
      "llama_print_timings:       total time =     743.44 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     151.83 ms /    22 runs   (    6.90 ms per token,   144.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.06 ms /    93 tokens (    1.28 ms per token,   781.11 tokens per second)\n",
      "llama_print_timings:        eval time =     383.07 ms /    21 runs   (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:       total time =     717.27 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     152.18 ms /    22 runs   (    6.92 ms per token,   144.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.02 ms /    89 tokens (    1.33 ms per token,   754.14 tokens per second)\n",
      "llama_print_timings:        eval time =     382.79 ms /    21 runs   (   18.23 ms per token,    54.86 tokens per second)\n",
      "llama_print_timings:       total time =     716.49 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     152.69 ms /    22 runs   (    6.94 ms per token,   144.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.18 ms /   182 tokens (    0.84 ms per token,  1195.98 tokens per second)\n",
      "llama_print_timings:        eval time =     386.03 ms /    21 runs   (   18.38 ms per token,    54.40 tokens per second)\n",
      "llama_print_timings:       total time =     755.35 ms /   203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     168.43 ms /    22 runs   (    7.66 ms per token,   130.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     152.27 ms /   182 tokens (    0.84 ms per token,  1195.27 tokens per second)\n",
      "llama_print_timings:        eval time =     387.23 ms /    21 runs   (   18.44 ms per token,    54.23 tokens per second)\n",
      "llama_print_timings:       total time =     782.14 ms /   203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     173.69 ms /    23 runs   (    7.55 ms per token,   132.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     151.77 ms /   180 tokens (    0.84 ms per token,  1185.99 tokens per second)\n",
      "llama_print_timings:        eval time =     405.96 ms /    22 runs   (   18.45 ms per token,    54.19 tokens per second)\n",
      "llama_print_timings:       total time =     809.36 ms /   202 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     163.12 ms /    22 runs   (    7.41 ms per token,   134.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.53 ms /   200 tokens (    0.81 ms per token,  1238.20 tokens per second)\n",
      "llama_print_timings:        eval time =     387.79 ms /    21 runs   (   18.47 ms per token,    54.15 tokens per second)\n",
      "llama_print_timings:       total time =     784.09 ms /   221 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     242.11 ms /    35 runs   (    6.92 ms per token,   144.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.92 ms /   197 tokens (    0.82 ms per token,  1224.23 tokens per second)\n",
      "llama_print_timings:        eval time =     624.33 ms /    34 runs   (   18.36 ms per token,    54.46 tokens per second)\n",
      "llama_print_timings:       total time =    1128.78 ms /   231 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     158.39 ms /    23 runs   (    6.89 ms per token,   145.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.53 ms /    96 tokens (    1.25 ms per token,   803.14 tokens per second)\n",
      "llama_print_timings:        eval time =     401.40 ms /    22 runs   (   18.25 ms per token,    54.81 tokens per second)\n",
      "llama_print_timings:       total time =     746.03 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     160.18 ms /    23 runs   (    6.96 ms per token,   143.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     118.51 ms /    91 tokens (    1.30 ms per token,   767.89 tokens per second)\n",
      "llama_print_timings:        eval time =     401.91 ms /    22 runs   (   18.27 ms per token,    54.74 tokens per second)\n",
      "llama_print_timings:       total time =     748.08 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     234.71 ms /    34 runs   (    6.90 ms per token,   144.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.31 ms /   196 tokens (    0.82 ms per token,  1222.61 tokens per second)\n",
      "llama_print_timings:        eval time =     606.16 ms /    33 runs   (   18.37 ms per token,    54.44 tokens per second)\n",
      "llama_print_timings:       total time =    1099.42 ms /   229 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     256.06 ms /    37 runs   (    6.92 ms per token,   144.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.77 ms /   315 tokens (    0.68 ms per token,  1473.52 tokens per second)\n",
      "llama_print_timings:        eval time =     668.54 ms /    36 runs   (   18.57 ms per token,    53.85 tokens per second)\n",
      "llama_print_timings:       total time =    1245.97 ms /   351 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     247.75 ms /    36 runs   (    6.88 ms per token,   145.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.63 ms /   313 tokens (    0.68 ms per token,  1465.12 tokens per second)\n",
      "llama_print_timings:        eval time =     650.51 ms /    35 runs   (   18.59 ms per token,    53.80 tokens per second)\n",
      "llama_print_timings:       total time =    1217.75 ms /   348 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     256.53 ms /    37 runs   (    6.93 ms per token,   144.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.29 ms /   319 tokens (    0.67 ms per token,  1481.74 tokens per second)\n",
      "llama_print_timings:        eval time =     668.60 ms /    36 runs   (   18.57 ms per token,    53.84 tokens per second)\n",
      "llama_print_timings:       total time =    1247.45 ms /   355 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     228.81 ms /    33 runs   (    6.93 ms per token,   144.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     213.56 ms /   314 tokens (    0.68 ms per token,  1470.35 tokens per second)\n",
      "llama_print_timings:        eval time =     594.71 ms /    32 runs   (   18.58 ms per token,    53.81 tokens per second)\n",
      "llama_print_timings:       total time =    1132.17 ms /   346 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     256.85 ms /    37 runs   (    6.94 ms per token,   144.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.66 ms /   318 tokens (    0.68 ms per token,  1481.44 tokens per second)\n",
      "llama_print_timings:        eval time =     669.31 ms /    36 runs   (   18.59 ms per token,    53.79 tokens per second)\n",
      "llama_print_timings:       total time =    1249.05 ms /   354 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     228.81 ms /    33 runs   (    6.93 ms per token,   144.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.78 ms /   316 tokens (    0.68 ms per token,  1471.27 tokens per second)\n",
      "llama_print_timings:        eval time =     594.78 ms /    32 runs   (   18.59 ms per token,    53.80 tokens per second)\n",
      "llama_print_timings:       total time =    1134.26 ms /   348 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     250.65 ms /    36 runs   (    6.96 ms per token,   143.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.50 ms /   317 tokens (    0.68 ms per token,  1477.85 tokens per second)\n",
      "llama_print_timings:        eval time =     650.16 ms /    35 runs   (   18.58 ms per token,    53.83 tokens per second)\n",
      "llama_print_timings:       total time =    1219.90 ms /   352 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     231.09 ms /    33 runs   (    7.00 ms per token,   142.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     214.57 ms /   317 tokens (    0.68 ms per token,  1477.36 tokens per second)\n",
      "llama_print_timings:        eval time =     594.41 ms /    32 runs   (   18.58 ms per token,    53.83 tokens per second)\n",
      "llama_print_timings:       total time =    1136.67 ms /   349 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     161.79 ms /    23 runs   (    7.03 ms per token,   142.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.88 ms /   299 tokens (    0.70 ms per token,  1424.59 tokens per second)\n",
      "llama_print_timings:        eval time =     410.44 ms /    22 runs   (   18.66 ms per token,    53.60 tokens per second)\n",
      "llama_print_timings:       total time =     856.07 ms /   321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     161.75 ms /    23 runs   (    7.03 ms per token,   142.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     210.42 ms /   301 tokens (    0.70 ms per token,  1430.48 tokens per second)\n",
      "llama_print_timings:        eval time =     408.89 ms /    22 runs   (   18.59 ms per token,    53.80 tokens per second)\n",
      "llama_print_timings:       total time =     849.32 ms /   323 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     190.73 ms /    27 runs   (    7.06 ms per token,   141.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.48 ms /   128 tokens (    0.99 ms per token,  1012.02 tokens per second)\n",
      "llama_print_timings:        eval time =     476.94 ms /    26 runs   (   18.34 ms per token,    54.51 tokens per second)\n",
      "llama_print_timings:       total time =     875.80 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     162.28 ms /    23 runs   (    7.06 ms per token,   141.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     210.03 ms /   299 tokens (    0.70 ms per token,  1423.59 tokens per second)\n",
      "llama_print_timings:        eval time =     408.85 ms /    22 runs   (   18.58 ms per token,    53.81 tokens per second)\n",
      "llama_print_timings:       total time =     850.93 ms /   321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     185.94 ms /    27 runs   (    6.89 ms per token,   145.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.24 ms /   123 tokens (    1.02 ms per token,   982.11 tokens per second)\n",
      "llama_print_timings:        eval time =     476.37 ms /    26 runs   (   18.32 ms per token,    54.58 tokens per second)\n",
      "llama_print_timings:       total time =     865.71 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     186.21 ms /    27 runs   (    6.90 ms per token,   145.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.67 ms /   124 tokens (    1.01 ms per token,   986.69 tokens per second)\n",
      "llama_print_timings:        eval time =     475.98 ms /    26 runs   (   18.31 ms per token,    54.62 tokens per second)\n",
      "llama_print_timings:       total time =     865.78 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     186.96 ms /    27 runs   (    6.92 ms per token,   144.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.24 ms /   123 tokens (    1.02 ms per token,   982.10 tokens per second)\n",
      "llama_print_timings:        eval time =     476.34 ms /    26 runs   (   18.32 ms per token,    54.58 tokens per second)\n",
      "llama_print_timings:       total time =     867.62 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     160.05 ms /    23 runs   (    6.96 ms per token,   143.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.57 ms /   234 tokens (    0.69 ms per token,  1448.30 tokens per second)\n",
      "llama_print_timings:        eval time =     405.66 ms /    22 runs   (   18.44 ms per token,    54.23 tokens per second)\n",
      "llama_print_timings:       total time =     794.76 ms /   256 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     159.64 ms /    23 runs   (    6.94 ms per token,   144.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.09 ms /   296 tokens (    0.71 ms per token,  1415.66 tokens per second)\n",
      "llama_print_timings:        eval time =     407.54 ms /    22 runs   (   18.52 ms per token,    53.98 tokens per second)\n",
      "llama_print_timings:       total time =     843.08 ms /   318 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     164.30 ms /    24 runs   (    6.85 ms per token,   146.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     212.09 ms /   310 tokens (    0.68 ms per token,  1461.62 tokens per second)\n",
      "llama_print_timings:        eval time =     426.72 ms /    23 runs   (   18.55 ms per token,    53.90 tokens per second)\n",
      "llama_print_timings:       total time =     873.13 ms /   333 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     158.61 ms /    23 runs   (    6.90 ms per token,   145.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     209.16 ms /   296 tokens (    0.71 ms per token,  1415.20 tokens per second)\n",
      "llama_print_timings:        eval time =     407.73 ms /    22 runs   (   18.53 ms per token,    53.96 tokens per second)\n",
      "llama_print_timings:       total time =     842.85 ms /   318 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     161.50 ms /    23 runs   (    7.02 ms per token,   142.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     162.24 ms /   236 tokens (    0.69 ms per token,  1454.64 tokens per second)\n",
      "llama_print_timings:        eval time =     405.75 ms /    22 runs   (   18.44 ms per token,    54.22 tokens per second)\n",
      "llama_print_timings:       total time =     797.59 ms /   258 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     186.65 ms /    27 runs   (    6.91 ms per token,   144.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.95 ms /   126 tokens (    1.00 ms per token,  1000.36 tokens per second)\n",
      "llama_print_timings:        eval time =     475.96 ms /    26 runs   (   18.31 ms per token,    54.63 tokens per second)\n",
      "llama_print_timings:       total time =     867.77 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     185.67 ms /    27 runs   (    6.88 ms per token,   145.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.01 ms /   126 tokens (    1.00 ms per token,   999.89 tokens per second)\n",
      "llama_print_timings:        eval time =     475.81 ms /    26 runs   (   18.30 ms per token,    54.64 tokens per second)\n",
      "llama_print_timings:       total time =     865.13 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     159.61 ms /    23 runs   (    6.94 ms per token,   144.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     160.29 ms /   228 tokens (    0.70 ms per token,  1422.46 tokens per second)\n",
      "llama_print_timings:        eval time =     405.54 ms /    22 runs   (   18.43 ms per token,    54.25 tokens per second)\n",
      "llama_print_timings:       total time =     792.31 ms /   250 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     186.35 ms /    27 runs   (    6.90 ms per token,   144.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.52 ms /   124 tokens (    1.01 ms per token,   987.87 tokens per second)\n",
      "llama_print_timings:        eval time =     475.89 ms /    26 runs   (   18.30 ms per token,    54.63 tokens per second)\n",
      "llama_print_timings:       total time =     867.74 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     209.42 ms /    30 runs   (    6.98 ms per token,   143.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.74 ms /   125 tokens (    1.01 ms per token,   994.09 tokens per second)\n",
      "llama_print_timings:        eval time =     530.65 ms /    29 runs   (   18.30 ms per token,    54.65 tokens per second)\n",
      "llama_print_timings:       total time =     953.07 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     208.98 ms /    30 runs   (    6.97 ms per token,   143.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.86 ms /   130 tokens (    1.08 ms per token,   929.52 tokens per second)\n",
      "llama_print_timings:        eval time =     530.80 ms /    29 runs   (   18.30 ms per token,    54.63 tokens per second)\n",
      "llama_print_timings:       total time =     966.26 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     209.05 ms /    30 runs   (    6.97 ms per token,   143.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.11 ms /   127 tokens (    0.99 ms per token,  1007.03 tokens per second)\n",
      "llama_print_timings:        eval time =     530.61 ms /    29 runs   (   18.30 ms per token,    54.65 tokens per second)\n",
      "llama_print_timings:       total time =     952.50 ms /   156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     230.25 ms /    33 runs   (    6.98 ms per token,   143.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     125.19 ms /   122 tokens (    1.03 ms per token,   974.54 tokens per second)\n",
      "llama_print_timings:        eval time =     586.10 ms /    32 runs   (   18.32 ms per token,    54.60 tokens per second)\n",
      "llama_print_timings:       total time =    1038.43 ms /   154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     228.95 ms /    33 runs   (    6.94 ms per token,   144.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     126.38 ms /   128 tokens (    0.99 ms per token,  1012.81 tokens per second)\n",
      "llama_print_timings:        eval time =     585.89 ms /    32 runs   (   18.31 ms per token,    54.62 tokens per second)\n",
      "llama_print_timings:       total time =    1037.59 ms /   160 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     248.41 ms\n",
      "llama_print_timings:      sample time =     229.43 ms /    33 runs   (    6.95 ms per token,   143.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.77 ms /   133 tokens (    1.06 ms per token,   944.79 tokens per second)\n",
      "llama_print_timings:        eval time =     586.35 ms /    32 runs   (   18.32 ms per token,    54.58 tokens per second)\n",
      "llama_print_timings:       total time =    1052.14 ms /   165 tokens\n"
     ]
    }
   ],
   "source": [
    "limit_rows = -1 # 20\n",
    "selected_devices = None # ['Smoke', 'Humidity']\n",
    "\n",
    "df = pd.read_csv('data/datasets/dataset_v2.csv')\n",
    "devices = list(df['device'].unique())\n",
    "if selected_devices:\n",
    "    df = df[df['device'].isin(selected_devices)].sort_index()\n",
    "    # print(df)\n",
    "\n",
    "output_df = pd.DataFrame(columns=['device', 'user_cmd', 'mtd', 'json_cmd'])\n",
    "for i, row in df.iterrows():\n",
    "    # if i < 8 or i > 16:\n",
    "    #     continue\n",
    "    user_cmd = row['user_cmd']\n",
    "\n",
    "    device = row['device']\n",
    "    sample_devices = devices.copy()\n",
    "    sample_devices.remove(device)\n",
    "    sample_devices = random.sample(sample_devices, k=2)\n",
    "    env = f'{sample_devices[0]} id=1, {sample_devices[1]} id=2, {device} id=444'\n",
    "\n",
    "    retrieved_nodes = retriever.retrieve(user_cmd)\n",
    "    methods_description = f'API method 1:\\n{retrieved_nodes[0].text}'\n",
    "    method = retrieved_nodes[0].metadata['file_name'].replace('.md', '')\n",
    "\n",
    "    user_prompt = user_prompt_template.format(**{'env': env, \n",
    "                                                 'methods_description': methods_description, \n",
    "                                                 'user_cmd': user_cmd})\n",
    "    query = base_prompt + '\\n\\n' + user_prompt\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': query}\n",
    "        ],\n",
    "        grammar=llama_grammar\n",
    "    )\n",
    "    response_text = response['choices'][0]['message']['content']\n",
    "    response_text = response_text.replace('\\_', '_')\n",
    "    json_cmd = json.dumps(json.loads(response_text))\n",
    "\n",
    "    output_df.loc[len(output_df)] = pd.Series({'device': row['device'], 'user_cmd': user_cmd, 'mtd': method, 'json_cmd': json_cmd})\n",
    "\n",
    "    # print(user_cmd)\n",
    "    # print(response_text)\n",
    "\n",
    "    if limit_rows > 0 and i == limit_rows - 1:\n",
    "        break\n",
    "\n",
    "output_df.to_csv('output/output.csv', index=False, header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device</th>\n",
       "      <th>user_cmd</th>\n",
       "      <th>mtd</th>\n",
       "      <th>json_cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the dining room switch to flip mode.</td>\n",
       "      <td>Light.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetMode\", \"params\": {\"id\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the living room switch to cycle mode.</td>\n",
       "      <td>Switch.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Enable Automatic OFF for the kitchen switch wi...</td>\n",
       "      <td>Switch.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the bedroom switch name to 'Bed Lamp'.</td>\n",
       "      <td>Light.SetConfig</td>\n",
       "      <td>{\"method\": \"Light.SetConfig\", \"params\": {\"id\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the hallway switch power limit to 50 Watts.</td>\n",
       "      <td>Cover.GetConfig</td>\n",
       "      <td>{\"method\": \"Cover.Configure\", \"params\": {\"id\":...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   device                                           user_cmd  \\\n",
       "0  Switch           Set the dining room switch to flip mode.   \n",
       "1  Switch          Set the living room switch to cycle mode.   \n",
       "2  Switch  Enable Automatic OFF for the kitchen switch wi...   \n",
       "3  Switch         Set the bedroom switch name to 'Bed Lamp'.   \n",
       "4  Switch    Set the hallway switch power limit to 50 Watts.   \n",
       "\n",
       "                mtd                                           json_cmd  \n",
       "0   Light.SetConfig  {\"method\": \"Switch.SetMode\", \"params\": {\"id\": ...  \n",
       "1  Switch.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "2  Switch.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "3   Light.SetConfig  {\"method\": \"Light.SetConfig\", \"params\": {\"id\":...  \n",
       "4   Cover.GetConfig  {\"method\": \"Cover.Configure\", \"params\": {\"id\":...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df = pd.read_csv('data/datasets/dataset_v2.csv')\n",
    "output_df = pd.read_csv('output/output.csv')\n",
    "incorrect_output_df = pd.DataFrame(columns=['device', 'user_cmd', 'gt_mtd', 'output_mtd', 'gt_json_cmd', 'output_json_cmd'])\n",
    "correct_output_df = pd.DataFrame(columns=['device', 'user_cmd', 'gt_mtd', 'output_mtd', 'gt_json_cmd', 'output_json_cmd'])\n",
    "\n",
    "correct_methods = 0\n",
    "correct_json_cmds = 0\n",
    "for i in range(len(output_df)):\n",
    "    gt = df.iloc[i]\n",
    "    output = output_df.iloc[i]\n",
    "\n",
    "    correct_ouput = True\n",
    "    if gt['mtd'] == output['mtd']:\n",
    "        correct_methods += 1\n",
    "    else:\n",
    "        correct_ouput = False\n",
    "    try:\n",
    "        if json.loads(gt['json_cmd']) == json.loads(output['json_cmd']):\n",
    "            correct_json_cmds += 1\n",
    "        else:\n",
    "            correct_ouput = False\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(gt['json_cmd'])\n",
    "        print(i, output['json_cmd'])\n",
    "\n",
    "    if not correct_ouput:\n",
    "        incorrect_output_df.loc[len(incorrect_output_df)] = pd.Series({'device': gt['device'], 'user_cmd': gt['user_cmd'], 'gt_mtd': gt['mtd'],\n",
    "                                                              'output_mtd': output['mtd'], 'gt_json_cmd': gt['json_cmd'], 'output_json_cmd': output['json_cmd']})\n",
    "    else:\n",
    "        correct_output_df.loc[len(correct_output_df)] = pd.Series({'device': gt['device'], 'user_cmd': gt['user_cmd'], 'gt_mtd': gt['mtd'],\n",
    "                                                              'output_mtd': output['mtd'], 'gt_json_cmd': gt['json_cmd'], 'output_json_cmd': output['json_cmd']})\n",
    "acc_methods = round(correct_methods / len(output_df), 2)\n",
    "acc_json_cmds = round(correct_json_cmds / len(output_df), 2)\n",
    "\n",
    "incorrect_output_df.to_csv('output/incorrect_output.csv', index=False)\n",
    "correct_output_df.to_csv('output/correct_output.csv', index=False)\n",
    "\n",
    "with open('output/results.txt', 'a') as f:\n",
    "    f.write(f'Acc of methods: {acc_methods}\\n'\n",
    "            f'Acc of json cmds: {acc_json_cmds}\\n\\n'\n",
    "            f'-----------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
