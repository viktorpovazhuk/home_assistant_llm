{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.embeddings.utils import resolve_embed_model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import Settings\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = SimpleDirectoryReader(\"data/docs/methods/\").load_data()\n",
    "\n",
    "# embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, show_progress=True, service_context=service_context\n",
    "# )\n",
    "\n",
    "# index.storage_context.persist(persist_dir=\"data/persist_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080, compute capability 7.5, VMM: yes\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    85.94 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4807.05 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# llm = Ollama(model=\"mistral\", request_timeout=180.0) # , base_url=\"http://62.80.172.138:11434\"\n",
    "llm = Llama('models/mistral-7b-instruct-v0.2.Q5_K_M.gguf', n_ctx=2048, verbose=True, n_gpu_layers=-1) # mistral-7b-instruct-v0.2.Q4_0.gguf mistral-7b-instruct-v0.2.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     124.81 ms\n",
      "llama_print_timings:      sample time =      38.93 ms /   100 runs   (    0.39 ms per token,  2568.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     124.69 ms /    14 tokens (    8.91 ms per token,   112.28 tokens per second)\n",
      "llama_print_timings:        eval time =    1505.38 ms /    99 runs   (   15.21 ms per token,    65.76 tokens per second)\n",
      "llama_print_timings:       total time =    1837.02 ms /   113 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-b3c5b341-cff1-44ea-9529-6843334ca973',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1710246469,\n",
       " 'model': 'models/mistral-7b-instruct-v0.2.Q5_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \" The color of the sky appears blue due to a process called Rayleigh scattering. When sunlight, which is made up of different colors, enters Earth's atmosphere, it interacts with molecules and particles in the air, such as nitrogen and oxygen. These molecules scatter the sunlight in all directions. Blue light is scattered more easily than other colors because it travels in smaller, shorter waves. As a result, when we look up at the sky, we primarily see the blue light that\"},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 14, 'completion_tokens': 100, 'total_tokens': 114}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(messages=[{'role': 'user', 'content': 'Why is sky blue?'}], max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  6 16:10:04 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 22%   25C    P2    51W / 215W |    918MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1411      G   /usr/bin/gnome-shell                3MiB |\n",
      "|    0   N/A  N/A   1476426      C   ...envs/assistant/bin/python      900MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"data/persist_dir/\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, show_progress=True) # , service_context=service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_scheme_prompt = {\n",
    "    \"method\": {\n",
    "        \"type\": \"string\"\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"type\": \"object\"\n",
    "    }\n",
    "}\n",
    "\n",
    "example_1_json = {\n",
    "  \"method\":\"Cover.Open\",\n",
    "  \"params\":\n",
    "  {\n",
    "    \"id\":2\n",
    "  }\n",
    "}\n",
    "\n",
    "example_2_json = {\n",
    "  \"method\":\"Cover.GetStatus\",\n",
    "  \"params\":\n",
    "  {\n",
    "    \"id\":0\n",
    "  }\n",
    "}\n",
    "\n",
    "with open('data/prompts/validation/instruction.md') as f:\n",
    "  instruction = f.read()\n",
    "\n",
    "variables = {\n",
    "    \"instruction\": instruction,\n",
    "    \"json_scheme\": \"The output JSON should follow the next scheme: \" + json.dumps(json_scheme_prompt),\n",
    "    \"devices\": \"\"\"Cover id=1\"\"\",\n",
    "    \"example_1\": \"\"\"Devices: Cover id=2\n",
    "Methods:\n",
    "API method 1: Cover.Open\n",
    "Description: Preconditions: Cover will not accept the command if: An overvoltage error is set at the time of the request. An undervoltage error is set at the time of the request. An overtemp error is set at the time of the request. An engaged safety_switch prohibits movement in the requested direction. Cover calibration is running at the time of the request. Properties: [{'name': 'id', 'type': 'number', 'description': 'The numeric ID of the Cover component instance'}, {'name': 'duration', 'type': 'number', 'description': 'If duration is not provided, Cover will fully open, unless it times out because of maxtime_open first. If duration (seconds) is provided, Cover will move in the open direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional'}] Response: null on success; error if the request can not be executed or failed \n",
    "Command: Open the cover.\n",
    "JSON: \"\"\" + json.dumps(example_1_json),\n",
    "    \"example_2\": \"\"\"Devices: Cover id=0\n",
    "Methods: \n",
    "API method 1: Cover.GetStatus\n",
    "Description: Properties: [{'name': 'id', 'type': 'number', 'description': 'The numeric ID of the Cover component instance'}] Find more about the status properties in status section \n",
    "Command: Get cover status.\n",
    "JSON: \"\"\" + json.dumps(example_2_json),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI Assistant that controls the devices in a house. For a given user command create a corresponding JSON object. Don't add properties with null value in output JSON object. Output must be strictly in JSON format.\n",
      "The output JSON should follow the next scheme: {\"method\": {\"type\": \"string\"}, \"params\": {\"type\": \"object\"}}\n",
      "\n",
      "Devices: Cover id=2\n",
      "Methods:\n",
      "API method 1: Cover.Open\n",
      "Description: Preconditions: Cover will not accept the command if: An overvoltage error is set at the time of the request. An undervoltage error is set at the time of the request. An overtemp error is set at the time of the request. An engaged safety_switch prohibits movement in the requested direction. Cover calibration is running at the time of the request. Properties: [{'name': 'id', 'type': 'number', 'description': 'The numeric ID of the Cover component instance'}, {'name': 'duration', 'type': 'number', 'description': 'If duration is not provided, Cover will fully open, unless it times out because of maxtime_open first. If duration (seconds) is provided, Cover will move in the open direction for the specified time. duration must be in the range [0.1..maxtime_open]Optional'}] Response: null on success; error if the request can not be executed or failed \n",
      "Command: Open the cover.\n",
      "JSON: {\"method\": \"Cover.Open\", \"params\": {\"id\": 2}}\n",
      "\n",
      "Devices: Cover id=0\n",
      "Methods: \n",
      "API method 1: Cover.GetStatus\n",
      "Description: Properties: [{'name': 'id', 'type': 'number', 'description': 'The numeric ID of the Cover component instance'}] Find more about the status properties in status section \n",
      "Command: Get cover status.\n",
      "JSON: {\"method\": \"Cover.GetStatus\", \"params\": {\"id\": 0}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_prompt_template = \"\"\"\n",
    "{instruction}\n",
    "{json_scheme}\n",
    "\n",
    "{example_1}\n",
    "\n",
    "{example_2}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_template = \"\"\"Devices: {env}\n",
    "Methods:\n",
    "{methods_description}\n",
    "Command: {user_cmd}\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "base_prompt = base_prompt_template.format(**variables)\n",
    "\n",
    "print(base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "# logger.addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger Q4_K_M model generates more complex output with unnecessary parameters. Possible fix: parameters tuning. \\\n",
    "Whereas Q4_0 model tries to escape `_` character with `\\` inside object property. Possible fix: grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import LlamaGrammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_scheme_general = {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"method\": {\n",
    "#             \"type\": \"string\"\n",
    "#         },\n",
    "#         \"params\": {\n",
    "#             \"type\": \"object\",\n",
    "#             # doesn't work without properties defined.\n",
    "#             # however, json schema is valid even without 'properties' field\n",
    "#             # https://stackoverflow.com/questions/42977208/json-schema-without-properties-keyword\n",
    "#             # \"properties\": {\n",
    "#             #     \"config\": {\n",
    "#             #         \"type\": \"string\"\n",
    "#             #     }\n",
    "#             # }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# s = str(json_scheme_general).replace(\"'\", '\"')\n",
    "# print(s)\n",
    "# gr = LlamaGrammar.from_json_schema(s)\n",
    "# print(gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root   ::= object\n",
      "value  ::= object | array | string | number | (\"true\" | \"false\" | \"null\") ws\n",
      "\n",
      "object ::=\n",
      "  \"{\" ws (\n",
      "            string \":\" ws value\n",
      "    (\",\" ws string \":\" ws value)*\n",
      "  )? \"}\" ws\n",
      "\n",
      "array  ::=\n",
      "  \"[\" ws (\n",
      "            value\n",
      "    (\",\" ws value)*\n",
      "  )? \"]\" ws\n",
      "\n",
      "string ::=\n",
      "  \"\\\"\" (\n",
      "    [^\"\\\\\\x7F\\x00-\\x1F] |\n",
      "    \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n",
      "  )* \"\\\"\" ws\n",
      "\n",
      "number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n",
      "\n",
      "# Optional space: by convention, applied in this grammar after literal chars when allowed\n",
      "# Removed recursive whitespace\n",
      "ws ::= ([ \\t\\n])?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/grammars/json.gbnf') as f:\n",
    "    grammar_str = f.read()\n",
    "print(grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_cpp.llama_grammar.LlamaGrammar object at 0x7f403d2b3b20>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] \n",
      "ws_31 ::= ws_30 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grammar = LlamaGrammar.from_string(grammar_str)\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_rows = -1 # 20\n",
    "selected_devices = ['Smoke', 'Humidity']\n",
    "\n",
    "df = pd.read_csv('data/datasets/dataset_v1.csv')\n",
    "devices = list(df['device'].unique())\n",
    "output_df = pd.DataFrame(columns=['device', 'user_cmd', 'mtd', 'json_cmd'])\n",
    "if selected_devices:\n",
    "    df = df[df['device'].isin(selected_devices)].sort_index()\n",
    "    # print(df)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # if i < 8 or i > 16:\n",
    "    #     continue\n",
    "    user_cmd = row['user_cmd']\n",
    "\n",
    "    device = row['device']\n",
    "    sample_devices = devices.copy()\n",
    "    sample_devices.remove(device)\n",
    "    sample_devices = random.sample(sample_devices, k=2)\n",
    "    env = f'{sample_devices[0]} id=1, {sample_devices[1]} id=2, {device} id=444'\n",
    "\n",
    "    retrieved_nodes = retriever.retrieve(user_cmd)\n",
    "    methods_description = f'API method 1:\\n{retrieved_nodes[0].text}'\n",
    "    method = retrieved_nodes[0].metadata['file_name'].replace('.md', '')\n",
    "\n",
    "    user_prompt = user_prompt_template.format(**{'env': env, \n",
    "                                                 'methods_description': methods_description, \n",
    "                                                 'user_cmd': user_cmd})\n",
    "    query = base_prompt + '\\n\\n' + user_prompt\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': query}\n",
    "        ],\n",
    "        grammar=grammar\n",
    "        # response_format={\"type\": \"json_object\",\n",
    "        #                 \"schema\": json_scheme_general} # \n",
    "    )\n",
    "    response_text = response['choices'][0]['message']['content']\n",
    "    response_text = response_text.replace('\\_', '_')\n",
    "    json_cmd = json.dumps(json.loads(response_text))\n",
    "\n",
    "    output_df.loc[len(output_df)] = pd.Series({'device': row['device'], 'user_cmd': user_cmd, 'mtd': method, 'json_cmd': json_cmd})\n",
    "\n",
    "    # print(user_cmd)\n",
    "    # print(response_text)\n",
    "\n",
    "    if limit_rows > 0 and i == limit_rows - 1:\n",
    "        break\n",
    "\n",
    "output_df.to_csv('output/output.csv', index=False, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device</th>\n",
       "      <th>user_cmd</th>\n",
       "      <th>mtd</th>\n",
       "      <th>json_cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the dining room switch to flip mode.</td>\n",
       "      <td>Switch.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the living room switch to cycle mode.</td>\n",
       "      <td>Switch.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Enable Automatic OFF for the kitchen switch wi...</td>\n",
       "      <td>Switch.GetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the bedroom switch name to 'Bed Lamp'.</td>\n",
       "      <td>Light.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Switch</td>\n",
       "      <td>Set the hallway switch power limit to 50 Watts.</td>\n",
       "      <td>Switch.SetConfig</td>\n",
       "      <td>{\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   device                                           user_cmd  \\\n",
       "0  Switch           Set the dining room switch to flip mode.   \n",
       "1  Switch          Set the living room switch to cycle mode.   \n",
       "2  Switch  Enable Automatic OFF for the kitchen switch wi...   \n",
       "3  Switch         Set the bedroom switch name to 'Bed Lamp'.   \n",
       "4  Switch    Set the hallway switch power limit to 50 Watts.   \n",
       "\n",
       "                mtd                                           json_cmd  \n",
       "0  Switch.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "1  Switch.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "2  Switch.GetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "3   Light.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  \n",
       "4  Switch.SetConfig  {\"method\": \"Switch.SetConfig\", \"params\": {\"id\"...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df = pd.read_csv('data/datasets/dataset_v1.csv')\n",
    "output_df = pd.read_csv('output/output.csv')\n",
    "incorrect_output_df = pd.DataFrame(columns=['device', 'user_cmd', 'gt_mtd', 'output_mtd', 'gt_json_cmd', 'output_json_cmd'])\n",
    "correct_output_df = pd.DataFrame(columns=['device', 'user_cmd', 'gt_mtd', 'output_mtd', 'gt_json_cmd', 'output_json_cmd'])\n",
    "\n",
    "correct_methods = 0\n",
    "correct_json_cmds = 0\n",
    "for i in range(len(output_df)):\n",
    "    gt = df.iloc[i]\n",
    "    output = output_df.iloc[i]\n",
    "\n",
    "    correct_ouput = True\n",
    "    if gt['mtd'] == output['mtd']:\n",
    "        correct_methods += 1\n",
    "    else:\n",
    "        correct_ouput = False\n",
    "    try:\n",
    "        if ast.literal_eval(gt['json_cmd'].replace(\"'\", '\"')) == json.loads(output['json_cmd']):\n",
    "            correct_json_cmds += 1\n",
    "        else:\n",
    "            correct_ouput = False\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(gt['json_cmd'].replace(\"'\", '\"'))\n",
    "        print(i, output['json_cmd'])\n",
    "\n",
    "    if not correct_ouput:\n",
    "        incorrect_output_df.loc[len(incorrect_output_df)] = pd.Series({'device': gt['device'], 'user_cmd': gt['user_cmd'], 'gt_mtd': gt['mtd'],\n",
    "                                                              'output_mtd': output['mtd'], 'gt_json_cmd': gt['json_cmd'], 'output_json_cmd': output['json_cmd']})\n",
    "    else:\n",
    "        correct_output_df.loc[len(correct_output_df)] = pd.Series({'device': gt['device'], 'user_cmd': gt['user_cmd'], 'gt_mtd': gt['mtd'],\n",
    "                                                              'output_mtd': output['mtd'], 'gt_json_cmd': gt['json_cmd'], 'output_json_cmd': output['json_cmd']})\n",
    "acc_methods = round(correct_methods / len(output_df), 2)\n",
    "acc_json_cmds = round(correct_json_cmds / len(output_df), 2)\n",
    "\n",
    "incorrect_output_df.to_csv('output/incorrect_output.csv', index=False)\n",
    "correct_output_df.to_csv('output/correct_output.csv', index=False)\n",
    "\n",
    "with open('output/results.txt', 'a') as f:\n",
    "    f.write(f'Acc of methods: {acc_methods}\\n'\n",
    "            f'Acc of json cmds: {acc_json_cmds}\\n\\n'\n",
    "            f'-----------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
